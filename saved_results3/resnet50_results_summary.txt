ResNet-50 Finetuning

Using 5 Epochs, 3 linear (layers 300 neurons to 100 neurons to 1 neuron), 1D batch-normalization, dropout layers with p=0.3, and a 0.001 weight-decay (L2 regularization) ...

- ROC AUC reached 0.888
- Validation loss plateaued 


Notes:

- Accidentally counted half-epochs as full epochs, so this will need to be updated.
- For the ResNet50 fine-tuning, I played with hyper-parameters a couple more times (layer size went to 400 and 200, scheduler change, weight_decay i.e. 5e-5, and dropout probability increased to p = 0.4), but the original hyper-parameters demonstrated better validation results.