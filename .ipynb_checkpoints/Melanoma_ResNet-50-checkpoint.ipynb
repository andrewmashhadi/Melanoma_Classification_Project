{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c030ccd3",
   "metadata": {},
   "source": [
    "# Melanoma Detection with the ResNet-50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce87f3",
   "metadata": {},
   "source": [
    "This code was used in the Hoffman2 Linux Compute Cluster, making use of UCLA's high performance cloud computing resources like the Tesla P4 - GPU (6.1 Compute Capability, 2560 CUDA Cores, 8GB) with additional 32GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf460317",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad9aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1e190",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a1310",
   "metadata": {},
   "source": [
    "General histograms and bar charts for frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262eb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_df = pd.read_csv(os.path.join('train_data', 'train.csv'))\n",
    "gt = mel_df['target']\n",
    "isic_id = mel_df['image_name']\n",
    "\n",
    "# proportion of postives\n",
    "print(\"Proportion of positives:\", np.mean(gt))\n",
    "\n",
    "plt.hist(mel_df['age_approx'])\n",
    "plt.title('Histogram of Ages')\n",
    "plt.xlabel('Age')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x = mel_df['benign_malignant'],\n",
    "            y = mel_df['age_approx'])\n",
    "plt.title('Ages Grouped By Benign / Malignant Status')\n",
    "plt.xlabel('Benign / Malignant Status')\n",
    "plt.ylabel('Approximate Age')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.bar(mel_df.sex.value_counts().index,  mel_df.sex.value_counts().values)\n",
    "plt.title('Sex / Gender in Dataset')\n",
    "plt.show()\n",
    "\n",
    "plt.barh(mel_df.anatom_site_general_challenge.value_counts().index, mel_df.anatom_site_general_challenge.value_counts().values)\n",
    "plt.title('Lesion Locations in Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb11e5",
   "metadata": {},
   "source": [
    "Tests to find potential correlation between target variables and other categorical variables such as sex/gender or lesion location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101b246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* TARGET W/ SEX INDEPENDENCE TESTS *******************\n",
      "benign_malignant  benign  malignant\n",
      "sex                                \n",
      "female             11824        170\n",
      "male               12535        267\n",
      "Chi-Squared test of independence (P-value): 7.87631386486258e-05 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************* TARGET W/ LESION LOCATION INDEPENDENCE TESTS *******************\n",
      "anatom_site_general_challenge  head/neck  lower extremity  oral/genital  \\\n",
      "sex                                                                       \n",
      "female                               629             3363            33   \n",
      "male                                 767             2966            57   \n",
      "\n",
      "anatom_site_general_challenge  palms/soles  torso  upper extremity  \n",
      "sex                                                                 \n",
      "female                                 111   5683             2001  \n",
      "male                                   169   6926             1698  \n",
      "Chi-Squared test of independence (P-value): 3.917186815096256e-37 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"******************* TARGET W/ SEX INDEPENDENCE TESTS *******************\")\n",
    "\n",
    "data_crosstab = pd.crosstab(mel_df['sex'],\n",
    "                            mel_df['benign_malignant'], \n",
    "                            margins = False)\n",
    "print(data_crosstab)\n",
    "\n",
    "chi2, p, dof, ex = ss.chi2_contingency(data_crosstab)\n",
    "\n",
    "print(\"Chi-Squared test of independence (P-value):\", p, \"\\n\")\n",
    "\n",
    "g_df1 = mel_df.groupby(['sex']).mean()\n",
    "plt.bar(mel_df.sex.value_counts().index,  g_df1['target'].values)\n",
    "plt.title(\"Proportion of positives by Sex / Gender\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\n******************* TARGET W/ LESION LOCATION INDEPENDENCE TESTS *******************\")\n",
    "\n",
    "data_crosstab = pd.crosstab(mel_df['sex'],\n",
    "                            mel_df['anatom_site_general_challenge'], \n",
    "                            margins = False)\n",
    "print(data_crosstab)\n",
    "\n",
    "chi2, p, dof, ex = ss.chi2_contingency(data_crosstab)\n",
    "\n",
    "print(\"Chi-Squared test of independence (P-value):\", p, \"\\n\")\n",
    "\n",
    "g_df2 = mel_df.groupby(['anatom_site_general_challenge']).mean() \n",
    "plt.barh(mel_df.anatom_site_general_challenge.value_counts().index, g_df2['target'].values)\n",
    "plt.title(\"Proportion of positives by Lesion Location\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713af71",
   "metadata": {},
   "source": [
    "## ResNet-50 (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9efcaad",
   "metadata": {},
   "source": [
    "Set device as CPU, or GPU if available. Code will have to change if using multiple GPUs (cuda:0, cuda:1, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eba207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Number of devices: 36\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    n_workers = os.cpu_count()\n",
    "else:\n",
    "    n_workers = torch.cuda.device_count()\n",
    "\n",
    "# If on a CUDA machine, this should print a CUDA device:\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of devices:\", n_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cea86",
   "metadata": {},
   "source": [
    "We create a custom dataset loader class to use the ID and target information from the CSV to properly load our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1bef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset to load in with the benign \n",
    "# and malignant images in the same directory\n",
    "class ISICDatasetImages(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, patientfile, num_samples=100, start_ind=0, up_sample=False, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        mel_df = pd.read_csv(patientfile) \n",
    "        \n",
    "        if up_sample:\n",
    "            \n",
    "            # Separate majority and minority classes\n",
    "            df_benign = mel_df[mel_df['target']==0]\n",
    "            df_malignant = mel_df[mel_df['target']==1]\n",
    "            \n",
    "\n",
    "            # sample minority class\n",
    "            df_benign_sampled = resample(df_benign, \n",
    "                                         replace=True,     # sample with replacement\n",
    "                                         n_samples=num_samples//2)\n",
    "            \n",
    "\n",
    "            # Upsample minority class\n",
    "            df_malignant_upsampled = resample(df_malignant, \n",
    "                                              replace=True,     # sample with replacement\n",
    "                                              n_samples=num_samples//2)\n",
    "            \n",
    "            # Combine majority class with upsampled minority class\n",
    "            mel_df = pd.concat([df_benign_sampled, df_malignant_upsampled])\n",
    "            \n",
    "            # randomly mix them up (not necessary due to shuffling in dataloader)\n",
    "            mel_df = shuffle(mel_df)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.start_ind = start_ind\n",
    "            self.end_ind = start_ind+num_samples\n",
    "\n",
    "            if self.end_ind > len(mel_df):\n",
    "                self.end_ind = len(mel_df)\n",
    "        \n",
    "            mel_df = mel_df[self.start_ind:self.end_ind]\n",
    "            \n",
    "        self.gt = mel_df['target'].reset_index(drop=True)\n",
    "        self.isic_id = mel_df['image_name'].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.isic_id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, f\"{self.isic_id[idx]}.jpg\")\n",
    "        img = read_image(img_path).float()\n",
    "        class_id = torch.tensor([self.gt[idx]])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "    \n",
    "        \n",
    "        return img, class_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c73fa",
   "metadata": {},
   "source": [
    "We create a custom collate function to pad lower resolution images with zeros to maintain a constant high resolution of 3x4000x6000 for the CNN to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364d0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for padding images one at a time\n",
    "def pad2d_4k6k(_image):\n",
    "\n",
    "    rows = _image.shape[1]\n",
    "    cols = _image.shape[2]\n",
    "\n",
    "    top = np.ceil((4000 - rows)/2).astype('int')\n",
    "    bottom = np.floor((4000 - rows)/2).astype('int')\n",
    "    right = np.ceil((6000 - cols)/2).astype('int')\n",
    "    left = np.floor((6000 - cols)/2).astype('int')\n",
    "\n",
    "    pad_func = nn.ConstantPad2d((left, right, top, bottom), 0)\n",
    "    \n",
    "    return pad_func(_image)\n",
    "\n",
    "\n",
    "# recall that a CNN needs the inputs to be the same dimension so we \n",
    "# custom collate function to pad small res images with 0s if they are not 3x4000x6000\n",
    "def pad_collate2d(batch):\n",
    "    \n",
    "    # init lists\n",
    "    image_list, label_list = [], []\n",
    "   \n",
    "    for _image, _label in batch:\n",
    "    \n",
    "        pad_image = pad2d_4k6k(_image)\n",
    "        \n",
    "        image_list.append(torch.unsqueeze(pad_image, dim=0))\n",
    "        label_list.append(_label)\n",
    "        \n",
    "\n",
    "    image_out = torch.cat(image_list, dim=0) \n",
    "    label_out = torch.tensor(label_list, dtype=torch.int64)\n",
    "   \n",
    "    return image_out, label_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20c0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = False\n",
    "\n",
    "# set our batch size\n",
    "batch_size = 2\n",
    "\n",
    "tr_transf = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.3),\n",
    "     transforms.RandomVerticalFlip(p=0.3),\n",
    "     transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=(5, 7), sigma=(0.1, 2))]), p=0.2),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225]),\n",
    "     transforms.RandomErasing(scale=(0.02, 0.05), p=0.2)\n",
    "    ])\n",
    "\n",
    "val_transf = transforms.Compose(\n",
    "    [transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "train_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data\", \"train.csv\"), \n",
    "                            num_samples=2*24408, up_sample=True, start_ind=0, transform=tr_transf)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate2d, \n",
    "                          num_workers=n_workers)\n",
    "\n",
    "\n",
    "val_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data\", \"val.csv\"), \n",
    "                            num_samples=2*25, up_sample=True, start_ind=0, transform=val_transf)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=pad_collate2d, \n",
    "                        num_workers=n_workers)\n",
    "\n",
    "\n",
    "\n",
    "# test DataLoader with custom settings\n",
    "if testing:\n",
    "    for imgs, labels in train_loader:\n",
    "        print(\"Batch of images has shape: \",imgs.shape)\n",
    "        print(\"Batch of labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ef7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show the image\n",
    "def imshow(img):\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "    img = img * torch.tensor(std).view(3, 1, 1) + torch.tensor(mean).view(3, 1, 1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg.astype('int'), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "label_id = [\"Benign\", \"Malignant\"]\n",
    "\n",
    "if testing:\n",
    "    # get some random training images\n",
    "    trainiter = iter(train_loader)\n",
    "    images, labels = next(trainiter)\n",
    "    print(\"Size:\", images.shape)\n",
    "\n",
    "\n",
    "    # show images\n",
    "    imshow(images[0,])\n",
    "\n",
    "    # print labels\n",
    "    print(\"Label:\", label_id[labels[0,]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024e4f1",
   "metadata": {},
   "source": [
    "Sample and image from the data loader object to confirm it worked. Continue to run the cell for different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d401950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new ResNet-50 FC Layer weights.\n"
     ]
    }
   ],
   "source": [
    "load_weights = False\n",
    "create_new_weights = True\n",
    "PATH = './melanoma_ResNet50.pth'\n",
    "\n",
    "if load_weights:\n",
    "    print('Loading the pre-trained CNN weights.')\n",
    "    \n",
    "    # network weights load\n",
    "    net = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 100),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(100, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "    checkpoint = torch.load(PATH, map_location=device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # optimizer state load\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.fc.parameters(), weight_decay=0.01)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=2*24408, gamma=0.5)\n",
    "    lr_sched.load_state_dict(checkpoint['lr_sched'])\n",
    "    \n",
    "    # total mini_batch state load\n",
    "    mini_batch = checkpoint['mini_batch']\n",
    "    \n",
    "    print(\"CUDA Memory Allocated:\", torch.cuda.max_memory_allocated())\n",
    "    \n",
    "elif create_new_weights:\n",
    "    print('Creating new ResNet-50 FC Layer weights.')\n",
    "    \n",
    "    net = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 100),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(100, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "    \n",
    "    \n",
    "    mini_batch = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.fc.parameters(), weight_decay=0.01)\n",
    "    lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=2*24408, gamma=0.5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b13966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training CUDA Memory Allocation: 0\n",
      "CUDA Memory Allocated: 0\n",
      "[Epoch 0, Batch 1] Loss: 0.7072024345397949\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-490526b94c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;31m# calculate outputs by running images through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_weights = True\n",
    "\n",
    "print(\"Pre-Training CUDA Memory Allocation:\", torch.cuda.max_memory_allocated())\n",
    "\n",
    "if learn_weights:\n",
    "\n",
    "    # set start time for cnn training\n",
    "    start_time = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net.forward(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels.unsqueeze(-1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_sched.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # update mini-batch count\n",
    "        mini_batch += 1\n",
    "        epoch = mini_batch // 24408\n",
    "\n",
    "        # print every 25 mini-batch\n",
    "        if i % 25 == 0:\n",
    "            print(\"CUDA Memory Allocated:\", torch.cuda.max_memory_allocated())\n",
    "            print(f'[Epoch {epoch}, Batch {mini_batch % 24408}] Loss: {running_loss / (i+1)}\\n')\n",
    "\n",
    "        # save every 100 mini-batch\n",
    "        if i % 100 == 0:\n",
    "            print(\"*********** Saving network weights and optimizer state *********** \\n\\n\")\n",
    "            # save the weights and optimizer\n",
    "            torch.save({'mini_batch': mini_batch,\n",
    "                        'model_state_dict': net.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'lr_sched': lr_sched.state_dict()}, PATH)\n",
    "\n",
    "        # validate every 1000 mini-batch\n",
    "        if i % 1000 == 0:\n",
    "\n",
    "            print(\"******************************************************************\")\n",
    "            print(\"*********************** Performance Update ***********************\")\n",
    "            print(\"******************************************************************\\n\")\n",
    "            \n",
    "            ground_truths = []\n",
    "            probs = []\n",
    "\n",
    "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "            with torch.no_grad():\n",
    "                for j, valdata in enumerate(val_loader, 0):\n",
    "                    image, label = valdata\n",
    "                    image = image.to(device)\n",
    "\n",
    "                    # save for analysis\n",
    "                    ground_truths.append(label)\n",
    "\n",
    "                    # calculate outputs by running images through the network \n",
    "                    outputs = net(image)\n",
    "                    outputs = outputs.to(\"cpu\")\n",
    "\n",
    "                    # # save for analysis\n",
    "                    probs.append(outputs)\n",
    "\n",
    "            print(\"Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs))\n",
    "\n",
    "            precision, recall, thresholds = metrics.precision_recall_curve(ground_truths, probs)\n",
    "            f1scores = 2 * (precision * recall) / (precision + recall)\n",
    "            opt_thresh = thresholds[np.argmax(f1scores)]\n",
    "            opt_preds = probs > opt_thresh\n",
    "\n",
    "\n",
    "            print(\"Using max F1-Score threshold, the confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, opt_preds))\n",
    "            \n",
    "            \n",
    "            print(\"\\n******************************************************************\")\n",
    "            print(\"****************** Performance Update Complete! ******************\")\n",
    "            print(\"******************************************************************\\n\\n\")\n",
    "\n",
    "        # save unique set of weights and optimizer for validation later\n",
    "        if mini_batch % 12000 == 0:\n",
    "            uPATH = f'./melanoma_ResNet50_{epoch}e_{mini_batch % 24408}b.pth'\n",
    "            torch.save({'mini_batch': mini_batch,\n",
    "                        'model_state_dict': net.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'lr_sched': lr_sched.state_dict()}, uPATH)\n",
    "\n",
    "    print('*********** Finished Training this Epoch in', time.time() - start_time, 'seconds ***********')\n",
    "    \n",
    "    # save the weights and optimizer\n",
    "    torch.save({'mini_batch': mini_batch,\n",
    "                'model_state_dict': net.state_dict(), \n",
    "                'optimizer_state_dict': optimizer.state_dict(), \n",
    "                'lr_sched': lr_sched.state_dict()}, PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998de40",
   "metadata": {},
   "source": [
    "# Formally test performance on our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f63c0",
   "metadata": {},
   "source": [
    "First, let us see what the convolutional neural network thinks of a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b172eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "test_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data\", \"val.csv\"), \n",
    "                            num_samples=8281, up_sample=False, start_ind=0, transform=val_transf)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate2d, \n",
    "                         num_workers=n_workers)\n",
    "\n",
    "\n",
    "\n",
    "testiter = iter(test_loader)\n",
    "images, labels = next(testiter)\n",
    "\n",
    "# print images\n",
    "print('GroundTruth: ', ' '.join('%5s' % label_id[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072622c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_weights | create_new_weights:\n",
    "    \n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % label_id[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9271d8",
   "metadata": {},
   "source": [
    "Fortunately, we saved weights off at different epoch/batch values. Here is the list of saved weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc00fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./saved_weights3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_fnames = os.listdir('./saved_weights3/')\n",
    "weight_fnames.sort() # isnt perfectly sorted, but too lazy to add the code (not important)\n",
    "batch_sizes = []\n",
    "losses = []\n",
    "\n",
    "for fname in weight_fnames:\n",
    "    \n",
    "    print(f'Loading: {fname}\\n')\n",
    "\n",
    "    checkpoint = torch.load(f'./saved_weights2/{fname}', map_location=device)\n",
    "    \n",
    "    # network weights load\n",
    "    net = Net().to(device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # set start time for cnn training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ground_truths = []\n",
    "    probs = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i, testdata in enumerate(test_loader, 0):\n",
    "            \n",
    "            image, label = testdata\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = net(image)\n",
    "            \n",
    "            loss = criterion(outputs, label.unsqueeze(-1).float())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # everything saved should be on RAM\n",
    "            outputs = outputs.to(\"cpu\")\n",
    "            label = label.to(\"cpu\")\n",
    "            \n",
    "            # save for analysis\n",
    "            ground_truths.append(label)\n",
    "            \n",
    "            # # save for analysis\n",
    "            probs += outputs.squeeze(-1).tolist()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"\\t Processing Image #{2*i + 1} ... Running Time {time.time() - start_time}\")\n",
    "                print(f'\\t Current Testing Loss: {running_loss / (i+1)}\\n')\n",
    "                \n",
    "    print(f'******* Final Testing Loss: {running_loss / (i+1)} *******\\n')\n",
    "    \n",
    "    batch_sizes.append(checkpoint['mini_batch'])\n",
    "    losses.append(running_loss / (i+1))         \n",
    "                \n",
    "    # Save ground-truths and probability results\n",
    "    res = {}\n",
    "    res[\"ground_truths\"] = ground_truths\n",
    "    res[\"probs\"] = probs\n",
    "    res[\"num_batches\"] = checkpoint['mini_batch']\n",
    "    res[\"testing_loss\"] = running_loss / (i+1)\n",
    "\n",
    "    pkl_f_name = f'./saved_results2/results_cnn2_{checkpoint[\"mini_batch\"]}b.pkl'\n",
    "    with open(pkl_f_name, 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "\n",
    "        \n",
    "plt.plot(batch_sizes, losses, 'o')\n",
    "plt.title(\"Testing Loss Over Time\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Overall Testing Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f86c7e",
   "metadata": {},
   "source": [
    "## Choose the results from the best performing model (training size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "# in case pkl results were calculated in batch job\n",
    "# we may wont to visualize the test plot over time\n",
    "recalc_loss_plot = True\n",
    "\n",
    "if recalc_loss_plot:\n",
    "    \n",
    "    batch_sizes = []\n",
    "    losses = []\n",
    "    res_fnames = os.listdir('./saved_results2/')\n",
    "    \n",
    "    for fname in res_fnames:\n",
    "        with open(f'./saved_results2/{fname}', 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            batch_sizes.append(res[\"num_batches\"])\n",
    "            losses.append(res[\"testing_loss\"])    \n",
    "            \n",
    "    \n",
    "    plt.plot(batch_sizes, losses, 'o')\n",
    "    plt.title(\"Testing Loss Over Time\")\n",
    "    plt.xlabel(\"Batch Number\")\n",
    "    plt.ylabel(\"Overall Testing Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_batch_size = 156000 \n",
    "\n",
    "\n",
    "with open(f'./saved_results2/results_cnn2_{best_batch_size}b.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)\n",
    "    \n",
    "    \n",
    "gt = res[\"ground_truths\"]\n",
    "probs = np.array(res[\"probs\"])\n",
    "\n",
    "\n",
    "# match formats (shouldve done this before, forgot to check)\n",
    "ground_truths = []\n",
    "for i in range(len(gt)):\n",
    "    if gt[i].size() > torch.Size([1]):\n",
    "        ground_truths += gt[i].squeeze(-1).tolist()\n",
    "    else:\n",
    "        ground_truths.append(gt[i].squeeze(-1).tolist())\n",
    "        \n",
    "ground_truths = np.array(ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc478afa",
   "metadata": {},
   "source": [
    "## Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8fc4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(ground_truths, probs)\n",
    "recall = tpr\n",
    "\n",
    "# compute other metrics using the same thresholds\n",
    "specificity = np.zeros_like(tpr)\n",
    "precision = np.zeros_like(tpr)\n",
    "fbetascores = np.zeros_like(tpr)\n",
    "CKappas = np.zeros_like(tpr)\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    preds = probs > thresholds[i]\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(ground_truths, preds).ravel()\n",
    "    \n",
    "    specificity[i] = tn / (tn + fp)\n",
    "    precision[i] = tp / (tp + fp)\n",
    "    \n",
    "    # more attention put on recall, such as when false negatives are more important to\n",
    "    # minimize, but false positives are still important.\n",
    "    fbetascores[i] = metrics.fbeta_score(ground_truths, preds, beta = 2)\n",
    "    \n",
    "    CKappas[i] = metrics.cohen_kappa_score(ground_truths, preds,)\n",
    "    \n",
    "\n",
    "\n",
    "gmeans = np.sqrt(specificity * recall)\n",
    "\n",
    "\n",
    "print(\"Max F2-Score is:\", np.nanmax(fbetascores))\n",
    "print(\"Max G-Mean is:\", np.nanmax(gmeans))\n",
    "print(\"Max Cohen's Kappa is:\", np.nanmax(CKappas))\n",
    "\n",
    "\n",
    "print(\"Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs), \"\\n\")\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr[np.nanargmax(fbetascores)], tpr[np.nanargmax(fbetascores)], 'ro')\n",
    "plt.plot(fpr[np.nanargmax(gmeans)], tpr[np.nanargmax(gmeans)], 'go')\n",
    "plt.plot(fpr[np.nanargmax(CKappas)], tpr[np.nanargmax(CKappas)], 'yo')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(['ROC Curve', 'F2-Score Optimal Coordinates', 'G-Mean Optimal Coordinates', \n",
    "            \"Kappa's Optimal Coordinates\"], loc='lower right', prop={'size': 8}, \n",
    "           frameon=True, facecolor = 'white')\n",
    "plt.show()\n",
    "\n",
    "fb_opt_thresh = thresholds[np.nanargmax(fbetascores)]\n",
    "fb_opt_preds = probs > fb_opt_thresh\n",
    "\n",
    "print('\\n********************* USING F2-SCORE OPTIMAL THRESHOLD *************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, fb_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(fbetascores)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(fbetascores)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(fbetascores)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(fbetascores)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(fbetascores)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(fbetascores)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "gm_opt_thresh = thresholds[np.nanargmax(gmeans)]\n",
    "gm_opt_preds = probs > gm_opt_thresh\n",
    "\n",
    "print('\\n********************** USING G-MEAN OPTIMAL THRESHOLD **************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, gm_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(gmeans)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(gmeans)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(gmeans)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(gmeans)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(gmeans)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(gmeans)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "\n",
    "ck_opt_thresh = thresholds[np.nanargmax(CKappas)]\n",
    "ck_opt_preds = probs > ck_opt_thresh\n",
    "\n",
    "print('\\n********************** USING KAPPA OPTIMAL THRESHOLD ***************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, ck_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(CKappas)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(CKappas)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(CKappas)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(CKappas)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(CKappas)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(CKappas)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "\n",
    "accuracy_scores = []\n",
    "for thresh in thresholds:\n",
    "    accuracy_scores.append(metrics.accuracy_score(ground_truths, [m > thresh for m in probs]))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(thresholds, fbetascores, \"-r\")\n",
    "plt.plot(thresholds, gmeans, \"-g\")\n",
    "plt.plot(thresholds, CKappas, \"-y\")\n",
    "plt.title(\"F2-Score, G-Means, and Cohen's Kappa Curves\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Performance Metrics\")\n",
    "plt.legend(['F2-Score', 'G-Mean', \"Cohen's Kappa\"], loc='upper right',\n",
    "           frameon=True, facecolor = 'white')\n",
    "plt.show()    \n",
    "    \n",
    "\n",
    "plt.plot(thresholds, accuracy_scores)\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
