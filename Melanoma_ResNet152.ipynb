{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c030ccd3",
   "metadata": {},
   "source": [
    "# Melanoma Detection with the ResNet-50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce87f3",
   "metadata": {},
   "source": [
    "This code was used in the Hoffman2 Linux Compute Cluster, making use of UCLA's high performance cloud computing resources like the Tesla P4 - GPU (6.1 Compute Capability, 2560 CUDA Cores, 8GB) with additional 32GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf460317",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad9aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1e190",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a1310",
   "metadata": {},
   "source": [
    "General histograms and bar charts for frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262eb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of positives: 0.017589052123163616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mel_df = pd.read_csv(os.path.join('train_data', 'train.csv'))\n",
    "gt = mel_df['target']\n",
    "isic_id = mel_df['image_name']\n",
    "\n",
    "# proportion of postives\n",
    "print(\"Proportion of positives:\", np.mean(gt))\n",
    "\n",
    "plt.hist(mel_df['age_approx'])\n",
    "plt.title('Histogram of Ages')\n",
    "plt.xlabel('Age')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x = mel_df['benign_malignant'],\n",
    "            y = mel_df['age_approx'])\n",
    "plt.title('Ages Grouped By Benign / Malignant Status')\n",
    "plt.xlabel('Benign / Malignant Status')\n",
    "plt.ylabel('Approximate Age')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.bar(mel_df.sex.value_counts().index,  mel_df.sex.value_counts().values)\n",
    "plt.title('Sex / Gender in Dataset')\n",
    "plt.show()\n",
    "\n",
    "plt.barh(mel_df.anatom_site_general_challenge.value_counts().index, mel_df.anatom_site_general_challenge.value_counts().values)\n",
    "plt.title('Lesion Locations in Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb11e5",
   "metadata": {},
   "source": [
    "Tests to find potential correlation between target variables and other categorical variables such as sex/gender or lesion location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101b246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* TARGET W/ SEX INDEPENDENCE TESTS *******************\n",
      "benign_malignant  benign  malignant\n",
      "sex                                \n",
      "female             11824        170\n",
      "male               12535        267\n",
      "Chi-Squared test of independence (P-value): 7.87631386486258e-05 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************* TARGET W/ LESION LOCATION INDEPENDENCE TESTS *******************\n",
      "anatom_site_general_challenge  head/neck  lower extremity  oral/genital  \\\n",
      "sex                                                                       \n",
      "female                               629             3363            33   \n",
      "male                                 767             2966            57   \n",
      "\n",
      "anatom_site_general_challenge  palms/soles  torso  upper extremity  \n",
      "sex                                                                 \n",
      "female                                 111   5683             2001  \n",
      "male                                   169   6926             1698  \n",
      "Chi-Squared test of independence (P-value): 3.917186815096256e-37 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"******************* TARGET W/ SEX INDEPENDENCE TESTS *******************\")\n",
    "\n",
    "data_crosstab = pd.crosstab(mel_df['sex'],\n",
    "                            mel_df['benign_malignant'], \n",
    "                            margins = False)\n",
    "print(data_crosstab)\n",
    "\n",
    "chi2, p, dof, ex = ss.chi2_contingency(data_crosstab)\n",
    "\n",
    "print(\"Chi-Squared test of independence (P-value):\", p, \"\\n\")\n",
    "\n",
    "g_df1 = mel_df.groupby(['sex']).mean()\n",
    "plt.bar(mel_df.sex.value_counts().index,  g_df1['target'].values)\n",
    "plt.title(\"Proportion of positives by Sex / Gender\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\n******************* TARGET W/ LESION LOCATION INDEPENDENCE TESTS *******************\")\n",
    "\n",
    "data_crosstab = pd.crosstab(mel_df['sex'],\n",
    "                            mel_df['anatom_site_general_challenge'], \n",
    "                            margins = False)\n",
    "print(data_crosstab)\n",
    "\n",
    "chi2, p, dof, ex = ss.chi2_contingency(data_crosstab)\n",
    "\n",
    "print(\"Chi-Squared test of independence (P-value):\", p, \"\\n\")\n",
    "\n",
    "g_df2 = mel_df.groupby(['anatom_site_general_challenge']).mean() \n",
    "plt.barh(mel_df.anatom_site_general_challenge.value_counts().index, g_df2['target'].values)\n",
    "plt.title(\"Proportion of positives by Lesion Location\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713af71",
   "metadata": {},
   "source": [
    "## ResNet-50 (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9efcaad",
   "metadata": {},
   "source": [
    "Set device as CPU, or GPU if available. Code will have to change if using multiple GPUs (cuda:0, cuda:1, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eba207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    n_workers = os.cpu_count()\n",
    "else:\n",
    "    n_workers = torch.cuda.device_count()\n",
    "\n",
    "# If on a CUDA machine, this should print a CUDA device:\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of devices:\", n_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cea86",
   "metadata": {},
   "source": [
    "We create a custom dataset loader class to use the ID and target information from the CSV to properly load our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1bef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset to load in with the benign \n",
    "# and malignant images in the same directory\n",
    "class ISICDatasetImages(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, patientfile, num_samples=100, start_ind=0, up_sample=False, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        mel_df = pd.read_csv(patientfile) \n",
    "        \n",
    "        if up_sample:\n",
    "            \n",
    "            # Separate majority and minority classes\n",
    "            df_benign = mel_df[mel_df['target']==0]\n",
    "            df_malignant = mel_df[mel_df['target']==1]\n",
    "            \n",
    "\n",
    "            # sample minority class\n",
    "            df_benign_sampled = resample(df_benign, \n",
    "                                         replace=True,     # sample with replacement\n",
    "                                         n_samples=num_samples//2)\n",
    "            \n",
    "\n",
    "            # Upsample minority class\n",
    "            df_malignant_upsampled = resample(df_malignant, \n",
    "                                              replace=True,     # sample with replacement\n",
    "                                              n_samples=num_samples//2)\n",
    "            \n",
    "            # Combine majority class with upsampled minority class\n",
    "            mel_df = pd.concat([df_benign_sampled, df_malignant_upsampled])\n",
    "            \n",
    "            # randomly mix them up (not necessary due to shuffling in dataloader)\n",
    "            mel_df = shuffle(mel_df)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.start_ind = start_ind\n",
    "            self.end_ind = start_ind+num_samples\n",
    "\n",
    "            if self.end_ind > len(mel_df):\n",
    "                self.end_ind = len(mel_df)\n",
    "        \n",
    "            mel_df = mel_df[self.start_ind:self.end_ind]\n",
    "            \n",
    "        self.gt = mel_df['target'].reset_index(drop=True)\n",
    "        self.isic_id = mel_df['image_name'].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.isic_id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, f\"{self.isic_id[idx]}.jpg\")\n",
    "        img = read_image(img_path).float()\n",
    "        class_id = torch.tensor([self.gt[idx]])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "    \n",
    "        \n",
    "        return img, class_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c73fa",
   "metadata": {},
   "source": [
    "We create a custom collate function to pad lower resolution images with zeros to maintain a constant high resolution of 3x4000x6000 for the CNN to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364d0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that a CNN needs the inputs to be the same dimension so we \n",
    "# custom collate function to pad small res images with 0s if they are not 3x4000x6000\n",
    "def pad_collate2d(batch):\n",
    "    \n",
    "    # init lists\n",
    "    image_list, label_list = [], []\n",
    "   \n",
    "    for _image, _label in batch:\n",
    "        \n",
    "        image_list.append(torch.unsqueeze(_image, dim=0))\n",
    "        label_list.append(_label)\n",
    "        \n",
    "\n",
    "    image_out = torch.cat(image_list, dim=0) \n",
    "    label_out = torch.tensor(label_list, dtype=torch.int64)\n",
    "   \n",
    "    return image_out, label_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20c0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = False\n",
    "\n",
    "# set our batch size\n",
    "batch_size = 16\n",
    "\n",
    "tr_transf = transforms.Compose(\n",
    "    [transforms.Resize(224),\n",
    "     transforms.RandomHorizontalFlip(p=0.3),\n",
    "     transforms.RandomVerticalFlip(p=0.3),\n",
    "     transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=(5, 7), sigma=(0.1, 2))]), p=0.2),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225]),\n",
    "     transforms.RandomErasing(scale=(0.02, 0.05), p=0.2)\n",
    "    ])\n",
    "\n",
    "val_transf = transforms.Compose(\n",
    "    [transforms.Resize(224),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "train_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data256x256\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data256x256\", \"train.csv\"), \n",
    "                            num_samples=5*2*24408, up_sample=True, start_ind=0, transform=tr_transf)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=pad_collate2d, \n",
    "                          num_workers=n_workers)\n",
    "\n",
    "\n",
    "val_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data256x256\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data256x256\", \"val.csv\"), \n",
    "                            num_samples=2*100, up_sample=True, start_ind=0, transform=val_transf)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=pad_collate2d, \n",
    "                        num_workers=n_workers)\n",
    "\n",
    "\n",
    "\n",
    "# test DataLoader with custom settings\n",
    "if testing:\n",
    "    for imgs, labels in train_loader:\n",
    "        print(\"Batch of images has shape: \",imgs.shape)\n",
    "        print(\"Batch of labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ef7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show the image\n",
    "def imshow(img):\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "    img = img * torch.tensor(std).view(3, 1, 1) + torch.tensor(mean).view(3, 1, 1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg.astype('int'), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "label_id = [\"Benign\", \"Malignant\"]\n",
    "\n",
    "if testing:\n",
    "    # get some random training images\n",
    "    trainiter = iter(train_loader)\n",
    "    images, labels = next(trainiter)\n",
    "    print(\"Size:\", images.shape)\n",
    "\n",
    "\n",
    "    # show images\n",
    "    imshow(images[0,])\n",
    "\n",
    "    # print labels\n",
    "    print(\"Label:\", label_id[labels[0,]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024e4f1",
   "metadata": {},
   "source": [
    "Sample and image from the data loader object to confirm it worked. Continue to run the cell for different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d401950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pre-trained ResNet-152 weights.\n",
      "CUDA Memory Allocated: 484819456\n"
     ]
    }
   ],
   "source": [
    "load_weights = True\n",
    "create_new_weights = False\n",
    "PATH = './melanoma_ResNet152.pth'\n",
    "\n",
    "if load_weights:\n",
    "    print('Loading the pre-trained ResNet-152 weights.')\n",
    "    \n",
    "    # network weights load\n",
    "    net = torchvision.models.resnet152(weights='IMAGENET1K_V2').to(device)\n",
    "    \n",
    "    # for feature extraction\n",
    "    #for param in net.parameters():\n",
    "        #param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 300),\n",
    "               nn.BatchNorm1d(300),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(300, 100),\n",
    "               nn.BatchNorm1d(100),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(100, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "\n",
    "    checkpoint = torch.load(PATH, map_location=device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # optimizer state load\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.fc.parameters(), weight_decay=0.01)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "    lr_sched.load_state_dict(checkpoint['lr_sched'])\n",
    "    \n",
    "    # total mini_batch state load\n",
    "    mini_batch = checkpoint['mini_batch']\n",
    "    \n",
    "    print(\"CUDA Memory Allocated:\", torch.cuda.max_memory_allocated())\n",
    "    \n",
    "elif create_new_weights:\n",
    "    print('Creating new ResNet-152 FC Layer weights.')\n",
    "    \n",
    "    net = torchvision.models.resnet152(weights='IMAGENET1K_V2').to(device)\n",
    "    \n",
    "    # for feature extraction\n",
    "    #for param in net.parameters():\n",
    "        #param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 300),\n",
    "               nn.BatchNorm1d(300),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(300, 100),\n",
    "               nn.BatchNorm1d(100),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(100, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "    \n",
    "    \n",
    "    mini_batch = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.fc.parameters(), weight_decay=0.001)\n",
    "    lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b13966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training CUDA Memory Allocation: 484819456\n",
      "CUDA Memory Allocated: 3409839616\n",
      "[Epoch 10, Batch 6] Loss: 0.16947811841964722\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8971999999999999\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 7] Loss: 0.2616160213947296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 8] Loss: 0.26937886079152423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 9] Loss: 0.2816377133131027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 10] Loss: 0.27578909397125245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 11] Loss: 0.31384895245234173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 12] Loss: 0.2990246628011976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 13] Loss: 0.30036288872361183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 14] Loss: 0.30052684081925285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 15] Loss: 0.30854612588882446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 16] Loss: 0.34798177805813874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 17] Loss: 0.35011690855026245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 18] Loss: 0.3334507414927849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 19] Loss: 0.3435347484690802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 20] Loss: 0.3363412360350291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 21] Loss: 0.3241338087245822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 22] Loss: 0.3239457212826785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 23] Loss: 0.3232728350493643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 24] Loss: 0.3194485534178583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 25] Loss: 0.31742260083556173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 26] Loss: 0.31728583361421314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 27] Loss: 0.3110008273612369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 28] Loss: 0.30418544295041455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 29] Loss: 0.29941402189433575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 30] Loss: 0.2998622423410416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 31] Loss: 0.29752169377528703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 32] Loss: 0.31304566672554723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 33] Loss: 0.3169149985270841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 34] Loss: 0.3161418165626197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 35] Loss: 0.30957096790273986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 36] Loss: 0.3119642741737827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 37] Loss: 0.3201231367420405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 38] Loss: 0.3259140801700679\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 39] Loss: 0.33178189988521967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 40] Loss: 0.3294308847614697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 41] Loss: 0.3261831793934107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 42] Loss: 0.3274141737335437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 43] Loss: 0.32210753250278923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 44] Loss: 0.3212011007544322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 45] Loss: 0.3202931882813573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 46] Loss: 0.3218903368929537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 47] Loss: 0.32133559687506585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 48] Loss: 0.3184253273661746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 49] Loss: 0.31896337917582557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 50] Loss: 0.3196333956387308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 51] Loss: 0.3193017009483731\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 52] Loss: 0.31716488349310895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 53] Loss: 0.31821010091031593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 54] Loss: 0.3192218865971176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 55] Loss: 0.3244611556828022\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 56] Loss: 0.32187960442959096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 57] Loss: 0.3196813359570045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 58] Loss: 0.3162722484964245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 59] Loss: 0.31380874567009787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 60] Loss: 0.3103417105295441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 61] Loss: 0.3115083704303418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 62] Loss: 0.3093686685488935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 63] Loss: 0.3071577447498667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 64] Loss: 0.3068075398519888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 65] Loss: 0.30815469386676947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 66] Loss: 0.3114907591313612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 67] Loss: 0.31169043637571797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 68] Loss: 0.314411399028604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 69] Loss: 0.31700159108731896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 70] Loss: 0.31624924185184333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 71] Loss: 0.320221798550902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 72] Loss: 0.31844005400120323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 73] Loss: 0.31580272919553165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 74] Loss: 0.31488947278779483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 75] Loss: 0.3134270185870784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 76] Loss: 0.31301509609944383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 77] Loss: 0.31222336108071935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 78] Loss: 0.313999754823234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 79] Loss: 0.31316009395428607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 80] Loss: 0.31104479302962623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 81] Loss: 0.3092847657635024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 82] Loss: 0.3070782499460431\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 83] Loss: 0.3064927396674951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 84] Loss: 0.30592946830806855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 85] Loss: 0.3094153462909162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 86] Loss: 0.3080473331573569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 87] Loss: 0.306741056918371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 88] Loss: 0.3070868080459445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 89] Loss: 0.30744131716589135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 90] Loss: 0.31018735547276105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 91] Loss: 0.3098689736148646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 92] Loss: 0.3096353352583688\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 93] Loss: 0.3087988676164638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 94] Loss: 0.3093954241845045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 95] Loss: 0.30890580125980904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 96] Loss: 0.30898848184189953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 97] Loss: 0.3115772226744372\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 98] Loss: 0.31664357238238855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 99] Loss: 0.31862375433457657\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 100] Loss: 0.32170520244460354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 101] Loss: 0.3230522524099797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 102] Loss: 0.3214383559282293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 103] Loss: 0.3248486571318033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 104] Loss: 0.32589104478106357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 105] Loss: 0.32523452796041963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 106] Loss: 0.3237231662220294\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 107] Loss: 0.324481278000509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 108] Loss: 0.3270298548549124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 109] Loss: 0.3263185590935441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 110] Loss: 0.3265866329982167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 111] Loss: 0.3273784984254612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 112] Loss: 0.32667420393674174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 113] Loss: 0.32653092475676976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 114] Loss: 0.3272518681686953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 115] Loss: 0.3274029899049889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 116] Loss: 0.32709144680081187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 117] Loss: 0.3257787554258747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 118] Loss: 0.32570083445943565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 119] Loss: 0.3263431318888539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 120] Loss: 0.3250124177854994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 121] Loss: 0.3270136809683052\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 122] Loss: 0.3319519567820761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 123] Loss: 0.3314405953101182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 124] Loss: 0.33042074558364243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 125] Loss: 0.3291106451923648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 126] Loss: 0.3283978233283216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 127] Loss: 0.32855543842325446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 128] Loss: 0.3275373195850752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 129] Loss: 0.3290951242970844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 130] Loss: 0.32847456270456316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 131] Loss: 0.3274595738048591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 132] Loss: 0.3269456545785656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 133] Loss: 0.32657775847474113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 134] Loss: 0.3283685626563176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 135] Loss: 0.32687821061565325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 136] Loss: 0.325610583121995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 137] Loss: 0.3245965726109165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 138] Loss: 0.3242205646365209\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 139] Loss: 0.32635980761095656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 140] Loss: 0.32552391329297314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 141] Loss: 0.3246494055561283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 142] Loss: 0.3259425225810413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 143] Loss: 0.32497758445316466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 144] Loss: 0.32412149939391255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 145] Loss: 0.3232834804803133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 146] Loss: 0.32268794826793334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 147] Loss: 0.3225300239110497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 148] Loss: 0.32114992843016044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 149] Loss: 0.32007525907829404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 150] Loss: 0.32138492244070976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 151] Loss: 0.3214948777976918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 152] Loss: 0.3203028948838208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 153] Loss: 0.31922189201656226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 154] Loss: 0.31826113809475165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 155] Loss: 0.31813461576898894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 156] Loss: 0.31780615641383936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 157] Loss: 0.31704623649190916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 158] Loss: 0.31561795140013976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 159] Loss: 0.3158450425638781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 160] Loss: 0.3148260619371168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 161] Loss: 0.31538416703159994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 162] Loss: 0.314958724816134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 163] Loss: 0.31415487561799305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 164] Loss: 0.3141695216016949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 165] Loss: 0.31397666651755574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 166] Loss: 0.3125117217531856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 167] Loss: 0.3122369514571296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 168] Loss: 0.3114553944099169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 169] Loss: 0.31208816170692444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 170] Loss: 0.3113911719033212\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 171] Loss: 0.31090545546577636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 172] Loss: 0.31144680091720855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 173] Loss: 0.3112069606071427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 174] Loss: 0.31058606117434756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 175] Loss: 0.3108911800033906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 176] Loss: 0.3102189022720906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 177] Loss: 0.3101876470758471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 178] Loss: 0.30966820778874304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 179] Loss: 0.30956846098790225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 180] Loss: 0.31074281658445085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 181] Loss: 0.3100031571124088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 182] Loss: 0.30973889809207056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 183] Loss: 0.30927509178271456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 184] Loss: 0.3097164265436833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 185] Loss: 0.30900128160913787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 186] Loss: 0.3092878952375433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 187] Loss: 0.3101785367170533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 188] Loss: 0.3102126796551741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 189] Loss: 0.3119209840407838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 190] Loss: 0.31119531570254144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 191] Loss: 0.31089202082285317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 192] Loss: 0.3109044907564785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 193] Loss: 0.3108276832611003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 194] Loss: 0.31051971341567064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 195] Loss: 0.3098505239737661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 196] Loss: 0.3103380661984389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 197] Loss: 0.31154694842795533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 198] Loss: 0.31217233103174\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 199] Loss: 0.3119801421140887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 200] Loss: 0.3132894323422359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 201] Loss: 0.3125148532646043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 202] Loss: 0.3122930652296483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 203] Loss: 0.31162859981108193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 204] Loss: 0.31156279049327024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 205] Loss: 0.31152078941464423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 206] Loss: 0.3113017882873763\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 207] Loss: 0.3109386268228588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 208] Loss: 0.3101031438910902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 209] Loss: 0.31012418888071003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 210] Loss: 0.309562045626524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 211] Loss: 0.30864683943755417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 212] Loss: 0.30791003426203983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 213] Loss: 0.3077772148669912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 214] Loss: 0.30751864982849103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 215] Loss: 0.3072916864639237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 216] Loss: 0.3079785347973566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 217] Loss: 0.30710079689633174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 218] Loss: 0.3071043839756872\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 219] Loss: 0.3070905636125636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 220] Loss: 0.3066404892261638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 221] Loss: 0.3062382183831047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 222] Loss: 0.30682978466633826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 223] Loss: 0.3060226361139105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 224] Loss: 0.305330656570931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 225] Loss: 0.3048209296708757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 226] Loss: 0.30437462685874145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 227] Loss: 0.30378283553563795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 228] Loss: 0.30345222280431755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 229] Loss: 0.30284846647243413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 230] Loss: 0.30420061820083194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 231] Loss: 0.3036579445531938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 232] Loss: 0.30327568101462815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 233] Loss: 0.30366120095315735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 234] Loss: 0.30320042671074515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 235] Loss: 0.3032251370989758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 236] Loss: 0.30447550537266255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 237] Loss: 0.30380249479464416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 238] Loss: 0.30373994213061273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 239] Loss: 0.30386778400239783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 240] Loss: 0.30389417517692485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 241] Loss: 0.30326186991849186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 242] Loss: 0.30258384051453713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 243] Loss: 0.3021851350404635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 244] Loss: 0.3020066272888223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 245] Loss: 0.3018400719389319\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 246] Loss: 0.30320846001884255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 247] Loss: 0.3035350646977582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 248] Loss: 0.30364742071785555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 249] Loss: 0.302846567216711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 250] Loss: 0.3030051225606276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 251] Loss: 0.3030365060075996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 252] Loss: 0.30236219530284164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 253] Loss: 0.30222561501807743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 254] Loss: 0.30202431076142683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 255] Loss: 0.30181338360905646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 256] Loss: 0.3017026872570771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 257] Loss: 0.3024304975415506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 258] Loss: 0.30308954108491715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 259] Loss: 0.3029930517957436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 260] Loss: 0.3029707148086791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 261] Loss: 0.3040007247182075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 262] Loss: 0.30352658071058736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 263] Loss: 0.3037118498495845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 264] Loss: 0.3039366396064924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 265] Loss: 0.30414699995173855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 266] Loss: 0.30382719170658984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 267] Loss: 0.3039513360282392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 268] Loss: 0.30430665025806247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 269] Loss: 0.303878723135726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 270] Loss: 0.3036982128361486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 271] Loss: 0.30347187528596786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 272] Loss: 0.30315527958927974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 273] Loss: 0.303095234669189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 274] Loss: 0.302640212429722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 275] Loss: 0.3027277377192621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 276] Loss: 0.30251671431579275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 277] Loss: 0.30184406614588466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 278] Loss: 0.302714280183717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 279] Loss: 0.30320320123412314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 280] Loss: 0.30307349825447255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 281] Loss: 0.30263724358941335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 282] Loss: 0.30257413713833053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 283] Loss: 0.3024819518754379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 284] Loss: 0.3020495808412952\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 285] Loss: 0.30199426569576776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 286] Loss: 0.3024242061896256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 287] Loss: 0.30278835712489505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 288] Loss: 0.303468388903478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 289] Loss: 0.30321780893899186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 290] Loss: 0.30285499344269434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 291] Loss: 0.3036841641512367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 292] Loss: 0.30341463769665994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 293] Loss: 0.3031453121608744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 294] Loss: 0.3033222677540614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 295] Loss: 0.3033264876182737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 296] Loss: 0.3035890252883082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 297] Loss: 0.3032982558879542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 298] Loss: 0.30310482236814174\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 299] Loss: 0.3026579505681586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 300] Loss: 0.30226230351096495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 301] Loss: 0.3026525562729787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 302] Loss: 0.3022513053684122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 303] Loss: 0.30162382505884106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 304] Loss: 0.3016502297044199\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 305] Loss: 0.3010785666108131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 306] Loss: 0.30114431842617023\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 307] Loss: 0.30102184345785354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 308] Loss: 0.30244481730775863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 309] Loss: 0.3024856916775829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 310] Loss: 0.3017882433093962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 311] Loss: 0.30181070879783506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 312] Loss: 0.30129479362056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 313] Loss: 0.30157370149315177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 314] Loss: 0.30114129778829596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 315] Loss: 0.30149375128169215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 316] Loss: 0.30154127883949466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 317] Loss: 0.3009268611860581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 318] Loss: 0.3007136747574273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 319] Loss: 0.30110736281439\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 320] Loss: 0.30061713206389595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 321] Loss: 0.30038920662637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 322] Loss: 0.2999787678477892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 323] Loss: 0.2994587448595455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 324] Loss: 0.29905667106933354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 325] Loss: 0.2990536751225591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 326] Loss: 0.3000693984120806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 327] Loss: 0.2997491368040535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 328] Loss: 0.29917532431636434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 329] Loss: 0.29913740140604383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 330] Loss: 0.2989461996463629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 331] Loss: 0.29863447750821437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 332] Loss: 0.29808282132177905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 333] Loss: 0.298093324845157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 334] Loss: 0.2976427677011055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 335] Loss: 0.2971578444495346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 336] Loss: 0.2972784445725179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 337] Loss: 0.29707127522273236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 338] Loss: 0.2965625844023249\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 339] Loss: 0.29620103877104687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 340] Loss: 0.29606520675901155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 341] Loss: 0.29629541002213955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 342] Loss: 0.2959638530670358\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 343] Loss: 0.2954220553562486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 344] Loss: 0.295064603992268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 345] Loss: 0.29532229378819463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 346] Loss: 0.29520023939721396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 347] Loss: 0.2954023542721369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 348] Loss: 0.2950810936901382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 349] Loss: 0.29477697571869504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 350] Loss: 0.29435597234877986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 351] Loss: 0.2946563139127169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 352] Loss: 0.29405670295273534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 353] Loss: 0.29407168318913585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 354] Loss: 0.29410664205650205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 355] Loss: 0.29434440623436653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 356] Loss: 0.2940045946895906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 357] Loss: 0.29457025342112914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 358] Loss: 0.29419045076407385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 359] Loss: 0.29442143614935334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 360] Loss: 0.29453926210252335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 361] Loss: 0.2956720119070136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 362] Loss: 0.2952373743015511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 363] Loss: 0.2949801391854299\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 364] Loss: 0.29588573106401145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 365] Loss: 0.2959189484309819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 366] Loss: 0.29538873744060457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 367] Loss: 0.2953367687514803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 368] Loss: 0.29508882648114004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 369] Loss: 0.2947042381165774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 370] Loss: 0.2949002568648286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 371] Loss: 0.2950073696184354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 372] Loss: 0.29499388672918975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 373] Loss: 0.2950399841748826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 374] Loss: 0.294788738919629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 375] Loss: 0.2952673725381091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 376] Loss: 0.2948615074840517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 377] Loss: 0.2944075904025506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 378] Loss: 0.29420058287420475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 379] Loss: 0.2939909701957741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 380] Loss: 0.2938976857463519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 381] Loss: 0.2943052184827467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 382] Loss: 0.29466715239640573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 383] Loss: 0.29478005692362785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 384] Loss: 0.2947541576537419\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 385] Loss: 0.2950878660733763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 386] Loss: 0.2953398635029167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 387] Loss: 0.295065263159456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 388] Loss: 0.29465418849772634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 389] Loss: 0.29486389269974705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 390] Loss: 0.29537285386354895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 391] Loss: 0.29524535510129263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 392] Loss: 0.29556197511364324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 393] Loss: 0.29537199823589055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 394] Loss: 0.2952461938127148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 395] Loss: 0.29489052408398725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 396] Loss: 0.29525888200534883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 397] Loss: 0.2951904709203815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 398] Loss: 0.2954549914363383\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 399] Loss: 0.2954814939140366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 400] Loss: 0.2952161586171464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 401] Loss: 0.29505918451556656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 402] Loss: 0.29467756500097003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 403] Loss: 0.2944415472073471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 404] Loss: 0.2939528880225387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 405] Loss: 0.29368375411257147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 406] Loss: 0.29312885021256685\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 407] Loss: 0.2927497741557769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 408] Loss: 0.292393638740846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 409] Loss: 0.29241924583543055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 410] Loss: 0.292645433857853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 411] Loss: 0.2932464840633822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 412] Loss: 0.29298141681078904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 413] Loss: 0.29263343159839805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 414] Loss: 0.2927759404323795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 415] Loss: 0.2922482071308101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 416] Loss: 0.2923514664789476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 417] Loss: 0.2921417091692825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 418] Loss: 0.2921121938212732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 419] Loss: 0.2917994592104845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 420] Loss: 0.29178562008113745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 421] Loss: 0.29162473677514267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 422] Loss: 0.2917779403374635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 423] Loss: 0.291656937022386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 424] Loss: 0.29141672078280006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 425] Loss: 0.2914475371262857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 426] Loss: 0.29136528310116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 427] Loss: 0.2909972955653735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 428] Loss: 0.290793554421435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 429] Loss: 0.29046025188674907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 430] Loss: 0.2903326375870144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 431] Loss: 0.2899162642168047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 432] Loss: 0.28957377902866804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 433] Loss: 0.28951554969997606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 434] Loss: 0.2896105182511267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 435] Loss: 0.2894562175280826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 436] Loss: 0.28912466980534196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 437] Loss: 0.28900623806165876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 438] Loss: 0.2887121244714387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 439] Loss: 0.28842082923527135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 440] Loss: 0.288496097631153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 441] Loss: 0.2886027553793761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 442] Loss: 0.28863297476171085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 443] Loss: 0.28873565726696626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 444] Loss: 0.28864838541029253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 445] Loss: 0.28899305586449126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 446] Loss: 0.2888875808271151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 447] Loss: 0.28954332802292987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 448] Loss: 0.28958009427274595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 449] Loss: 0.2897920836568684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 450] Loss: 0.28991249717688294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 451] Loss: 0.2901544123052749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 452] Loss: 0.2899725819027397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 453] Loss: 0.2898643690527284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 454] Loss: 0.2899479409202701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 455] Loss: 0.29001217813955416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 456] Loss: 0.29004591972693106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 457] Loss: 0.290241517373814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 458] Loss: 0.2901727769515635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 459] Loss: 0.2901708276753646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 460] Loss: 0.2898474158330278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 461] Loss: 0.28943583723811206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 462] Loss: 0.28945966774120896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 463] Loss: 0.28917575756079766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 464] Loss: 0.2888010690609614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 465] Loss: 0.2884489236642485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 466] Loss: 0.2886182333215457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 467] Loss: 0.2894283759839091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 468] Loss: 0.28923017620139196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 469] Loss: 0.2890899578302071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 470] Loss: 0.288969862076544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 471] Loss: 0.2888129855558084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 472] Loss: 0.2886100738793931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 473] Loss: 0.28835089928191954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 474] Loss: 0.2883529456884368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 475] Loss: 0.28837410011824144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 476] Loss: 0.28839948181766867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 477] Loss: 0.28837720838264896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 478] Loss: 0.2882704731651895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 479] Loss: 0.28806378057360144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 480] Loss: 0.28813844608633143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 481] Loss: 0.28790939135962174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 482] Loss: 0.2878580338924936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 483] Loss: 0.2876275861363032\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 484] Loss: 0.2878428935258025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 485] Loss: 0.28782562278211116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 486] Loss: 0.28779441384664445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 487] Loss: 0.28771736531584097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 488] Loss: 0.288953830798467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 489] Loss: 0.28924124897265235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 490] Loss: 0.2889795280916175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 491] Loss: 0.28890108777049145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 492] Loss: 0.28882270307878694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 493] Loss: 0.28883331430862186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 494] Loss: 0.2884542102675984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 495] Loss: 0.28844001583602963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 496] Loss: 0.2885619259748585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 497] Loss: 0.28861139059006197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 498] Loss: 0.288761047680407\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 499] Loss: 0.2886290755198311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 500] Loss: 0.28854154458250664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 501] Loss: 0.2885272649056729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 502] Loss: 0.28916249640091085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 503] Loss: 0.2891711554523692\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 504] Loss: 0.28890429101212467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 505] Loss: 0.2890265461355448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 506] Loss: 0.2887221436925277\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8891\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 507] Loss: 0.2886277173768239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 508] Loss: 0.28838414073760654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 509] Loss: 0.28832623251669465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 510] Loss: 0.28868561435749035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 511] Loss: 0.2885814544858904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 512] Loss: 0.28878418482560847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 513] Loss: 0.2885888524969497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 514] Loss: 0.2886452054081591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 515] Loss: 0.28846829698658455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 516] Loss: 0.28859047502745855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 517] Loss: 0.28856308753893245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 518] Loss: 0.2882661907906421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 519] Loss: 0.2884568095584092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 520] Loss: 0.2884237187724669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 521] Loss: 0.2880382427382608\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 522] Loss: 0.2878722454091566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 523] Loss: 0.28784413804016057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 524] Loss: 0.28811376840450403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 525] Loss: 0.28837494095071003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 526] Loss: 0.28801176049201366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 527] Loss: 0.2878450176096967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 528] Loss: 0.28788288296636616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 529] Loss: 0.28780217363520433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 530] Loss: 0.2877432085502715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 531] Loss: 0.2880356156837351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 532] Loss: 0.2882699665297141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 533] Loss: 0.28878107539970765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 534] Loss: 0.2886493308839365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 535] Loss: 0.2885831049309587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 536] Loss: 0.2884766571995453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 537] Loss: 0.2885523859999682\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 538] Loss: 0.2883800998693559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 539] Loss: 0.2888896346148034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 540] Loss: 0.2886416057838458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 541] Loss: 0.2884321727359028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 542] Loss: 0.2885885091855761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 543] Loss: 0.28894106559039934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 544] Loss: 0.2889710955307966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 545] Loss: 0.2888364073027063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 546] Loss: 0.28869946763718196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 547] Loss: 0.288853800302282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 548] Loss: 0.2894315666092035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 549] Loss: 0.28926792661385503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 550] Loss: 0.2898102579587096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 551] Loss: 0.289493884263567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 552] Loss: 0.28952562995595515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 553] Loss: 0.2898264740097479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 554] Loss: 0.28974216933513164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 555] Loss: 0.2898594355718656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 556] Loss: 0.2897768797734904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 557] Loss: 0.2897629766873475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 558] Loss: 0.28985989130157674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 559] Loss: 0.29005073698150124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 560] Loss: 0.2900966233364097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 561] Loss: 0.28983996292616393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 562] Loss: 0.2894754762254669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 563] Loss: 0.2897430780513953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 564] Loss: 0.2897657700722674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 565] Loss: 0.2898716870562306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 566] Loss: 0.28973141552983334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 567] Loss: 0.29001097414258115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 568] Loss: 0.29023795871338665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 569] Loss: 0.28996953522746866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 570] Loss: 0.2899748344996334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 571] Loss: 0.28993170870329804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 572] Loss: 0.2904877238619475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 573] Loss: 0.290661341982933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 574] Loss: 0.29026763454105814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 575] Loss: 0.29017811235890056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 576] Loss: 0.29025771145279894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 577] Loss: 0.29047271693972027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 578] Loss: 0.29083534166294867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 579] Loss: 0.29069116985496746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 580] Loss: 0.29035619578931643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 581] Loss: 0.2902009367244318\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 582] Loss: 0.29041159169771025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 583] Loss: 0.2902908535435744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 584] Loss: 0.29026240160582595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 585] Loss: 0.2901914226212378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 586] Loss: 0.29054475529520113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 587] Loss: 0.2903613843587051\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 588] Loss: 0.2901705320585858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 589] Loss: 0.29033395899332143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 590] Loss: 0.2903110705507107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 591] Loss: 0.290105670626965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 592] Loss: 0.29036884798051144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 593] Loss: 0.2905135965544958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 594] Loss: 0.2903483682936642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 595] Loss: 0.2903516519246465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 596] Loss: 0.29028463322045234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 597] Loss: 0.29017309080557646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 598] Loss: 0.29017712566593246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 599] Loss: 0.29018217517119466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 600] Loss: 0.2901091205097046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 601] Loss: 0.2898824430577107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 602] Loss: 0.2902123076058113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 603] Loss: 0.29025540409927386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 604] Loss: 0.2899858810144394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 605] Loss: 0.28983337997148434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 606] Loss: 0.2896587863167788\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 607] Loss: 0.28957780694397184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 608] Loss: 0.28966548464339764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 609] Loss: 0.2894384297388082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 610] Loss: 0.2892017073498285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 611] Loss: 0.2893235940277183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 612] Loss: 0.2898034883610892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 613] Loss: 0.29000926440532665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 614] Loss: 0.2902821633836319\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 615] Loss: 0.29006828293204306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 616] Loss: 0.2904212437631065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 617] Loss: 0.2906236265195546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 618] Loss: 0.29081586468812887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 619] Loss: 0.29110961233899724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 620] Loss: 0.29125751842085906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 621] Loss: 0.2911271703151333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 622] Loss: 0.29121026088826274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 623] Loss: 0.29109926441003203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 624] Loss: 0.2909997429018645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 625] Loss: 0.2909109775697993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 626] Loss: 0.2907275274971833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 627] Loss: 0.2907122764676522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 628] Loss: 0.2913264187462449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 629] Loss: 0.29118217564880466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 630] Loss: 0.2913174518465996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 631] Loss: 0.29144153983210225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 632] Loss: 0.2913553963461371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 633] Loss: 0.29144251660983655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 634] Loss: 0.2913650631644009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 635] Loss: 0.29109870252155123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 636] Loss: 0.2910185311751207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 637] Loss: 0.29091441041872473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 638] Loss: 0.29071172304439696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 639] Loss: 0.29048048199539306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 640] Loss: 0.2907898013047346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 641] Loss: 0.29062536647016146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 642] Loss: 0.2905092095748596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 643] Loss: 0.29029385941716196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 644] Loss: 0.29033027610308687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 645] Loss: 0.2903606826905161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 646] Loss: 0.2903223819740105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 647] Loss: 0.2902872193825208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 648] Loss: 0.2900446503890988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 649] Loss: 0.290188341615548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 650] Loss: 0.2901829038710557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 651] Loss: 0.29040913133890633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 652] Loss: 0.2903759101120729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 653] Loss: 0.29021138891025827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 654] Loss: 0.2901895837442166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 655] Loss: 0.2900548610778955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 656] Loss: 0.2897695562989664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 657] Loss: 0.28993479331578215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 658] Loss: 0.2900150599012331\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 659] Loss: 0.28985276419544076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 660] Loss: 0.2897411305258292\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 661] Loss: 0.2895887843219609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 662] Loss: 0.2896385138210822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 663] Loss: 0.28959613383993915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 664] Loss: 0.28986775574354917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 665] Loss: 0.28978533963813924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 666] Loss: 0.2897285261105481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 667] Loss: 0.28966737853706426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 668] Loss: 0.2898554809807293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 669] Loss: 0.28984269291072723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 670] Loss: 0.28975372966518975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 671] Loss: 0.2897498682588786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 672] Loss: 0.28964423302231523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 673] Loss: 0.28983421797702413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 674] Loss: 0.2897155892733145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 675] Loss: 0.2903018417865483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 676] Loss: 0.29038368716236374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 677] Loss: 0.290489230149736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 678] Loss: 0.290291739431214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 679] Loss: 0.29018671419305686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 680] Loss: 0.2901801424777066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 681] Loss: 0.2899221722477463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 682] Loss: 0.28974350821364403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 683] Loss: 0.28965870415170986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 684] Loss: 0.28969385920418905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 685] Loss: 0.2895220491925583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 686] Loss: 0.28951823441161284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 687] Loss: 0.2899010267185151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 688] Loss: 0.2895805686603179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 689] Loss: 0.28962035687389776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 690] Loss: 0.2896658946671625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 691] Loss: 0.28957096381932934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 692] Loss: 0.28957370046141195\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 693] Loss: 0.2896245605052384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 694] Loss: 0.2895711168257522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 695] Loss: 0.28989790938254717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 696] Loss: 0.2896441686494133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 697] Loss: 0.289831875322785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 698] Loss: 0.2898186549630344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 699] Loss: 0.29009032488856945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 700] Loss: 0.2899598025911146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 701] Loss: 0.2900416076333187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 702] Loss: 0.29038470422306567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 703] Loss: 0.2904383963598392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 704] Loss: 0.2902962828803813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 705] Loss: 0.2903102379930871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 706] Loss: 0.2903065675824413\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 707] Loss: 0.29008246381950176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 708] Loss: 0.290010328938493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 709] Loss: 0.29014943103009666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 710] Loss: 0.28996645475321625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 711] Loss: 0.29009291657349545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 712] Loss: 0.2900355975795669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 713] Loss: 0.2899766282446809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 714] Loss: 0.2897541251333039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 715] Loss: 0.2898423126780651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 716] Loss: 0.2897624318840299\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 717] Loss: 0.2898669062768308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 718] Loss: 0.28968544944932717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 719] Loss: 0.2895682763923951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 720] Loss: 0.2895352591272001\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 721] Loss: 0.2895077980700984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 722] Loss: 0.28956622264583076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 723] Loss: 0.28952944503894756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 724] Loss: 0.28971505191006147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 725] Loss: 0.289743487464471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 726] Loss: 0.28984555346831864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 727] Loss: 0.2897759296742998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 728] Loss: 0.28964113456456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 729] Loss: 0.2897039940940906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 730] Loss: 0.2897083033056095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 731] Loss: 0.28966427035116626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 732] Loss: 0.28958491254418556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 733] Loss: 0.2897099226324277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 734] Loss: 0.2897252142122744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 735] Loss: 0.289677728498227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 736] Loss: 0.28947447398333476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 737] Loss: 0.2892996644082128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 738] Loss: 0.2896291089598674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 739] Loss: 0.28953288460013327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 740] Loss: 0.28957937494629904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 741] Loss: 0.2895267866653107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 742] Loss: 0.2899148994841925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 743] Loss: 0.2900443571470779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 744] Loss: 0.29024870003068237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 745] Loss: 0.2903609885657961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 746] Loss: 0.2904156612662169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 747] Loss: 0.29030999550319747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 748] Loss: 0.2902704900178261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 749] Loss: 0.2902392558052495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 750] Loss: 0.2900989700983835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 751] Loss: 0.29010899457830847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 752] Loss: 0.2899720142265081\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 753] Loss: 0.2899418580918548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 754] Loss: 0.2898571017448192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 755] Loss: 0.28986655951539675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 756] Loss: 0.2897685552425137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 757] Loss: 0.2897639730053538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 758] Loss: 0.2896932935117092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 759] Loss: 0.2896049220203879\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 760] Loss: 0.28963947153051955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 761] Loss: 0.28956720141309594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 762] Loss: 0.2893563849430733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 763] Loss: 0.289087465234435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 764] Loss: 0.2890766407602389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 765] Loss: 0.2888913405470942\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 766] Loss: 0.2887191094859514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 767] Loss: 0.2891770166264275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 768] Loss: 0.28914118425366453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 769] Loss: 0.2894758310910139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 770] Loss: 0.2896521727146666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 771] Loss: 0.289858401434462\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 772] Loss: 0.28976774564428154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 773] Loss: 0.2897862254079276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 774] Loss: 0.28952787521020334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 775] Loss: 0.28934216838958976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 776] Loss: 0.2892865822221673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 777] Loss: 0.2892197485086677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 778] Loss: 0.28918788260159795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 779] Loss: 0.2890704366684114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 780] Loss: 0.28898763699877644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 781] Loss: 0.2898007525783992\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 782] Loss: 0.2899613761782953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 783] Loss: 0.289990347817272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 784] Loss: 0.2898289298158096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 785] Loss: 0.2898327307918897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 786] Loss: 0.2898245461561768\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 787] Loss: 0.2897782433311195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 788] Loss: 0.2895436247437241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 789] Loss: 0.2896941306582671\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 790] Loss: 0.28969515980618776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 791] Loss: 0.2897847664208358\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 792] Loss: 0.2900173786880858\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 793] Loss: 0.2900310895561718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 794] Loss: 0.2901431980940628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 795] Loss: 0.29023369197038157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 796] Loss: 0.2901538397302067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 797] Loss: 0.28997417282538884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 798] Loss: 0.2898550901801559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 799] Loss: 0.2900082701793696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 800] Loss: 0.28984169702297485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 801] Loss: 0.2899889449247314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 802] Loss: 0.2900946352401821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 803] Loss: 0.2902027832106092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 804] Loss: 0.29005485762829175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 805] Loss: 0.2901275788713247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 806] Loss: 0.289981756029132\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 807] Loss: 0.290029813607509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 808] Loss: 0.28994446711737665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 809] Loss: 0.2898415221151576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 810] Loss: 0.28981929115071803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 811] Loss: 0.2897145491990055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 812] Loss: 0.2900057200624006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 813] Loss: 0.2899714559040005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 814] Loss: 0.28985401618723816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 815] Loss: 0.28988365809674616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 816] Loss: 0.29000115298129775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 817] Loss: 0.28985106035736685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 818] Loss: 0.29044287334692287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 819] Loss: 0.29033811048991265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 820] Loss: 0.2903483480672163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 821] Loss: 0.29040446832739547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 822] Loss: 0.2903321541052407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 823] Loss: 0.2902727353416183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 824] Loss: 0.2903907699408112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 825] Loss: 0.2903092052969264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 826] Loss: 0.2902816485128072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 827] Loss: 0.29058719628519963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 828] Loss: 0.2905081665620914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 829] Loss: 0.29067959279292127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 830] Loss: 0.29080277996532844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 831] Loss: 0.29084612870223586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 832] Loss: 0.29066537193584613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 833] Loss: 0.29058066479285843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 834] Loss: 0.2909838065540977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 835] Loss: 0.2912837243313531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 836] Loss: 0.29130287894308926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 837] Loss: 0.29123930836347145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 838] Loss: 0.29112213369713824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 839] Loss: 0.2911093054826168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 840] Loss: 0.2912994647722044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 841] Loss: 0.2913501450087893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 842] Loss: 0.2912904850097113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 843] Loss: 0.2911685324565181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 844] Loss: 0.290979134906816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 845] Loss: 0.2914580589515113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 846] Loss: 0.29167315933530025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 847] Loss: 0.29172632047760516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 848] Loss: 0.29185011596689586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 849] Loss: 0.2917737933890938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 850] Loss: 0.29164631531612406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 851] Loss: 0.29143403591169814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 852] Loss: 0.29143387583274627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 853] Loss: 0.29141103698573306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 854] Loss: 0.2912252526839995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 855] Loss: 0.291224215635482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 856] Loss: 0.2911253437469905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 857] Loss: 0.29103175016391164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 858] Loss: 0.291136528104369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 859] Loss: 0.29100279196817647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 860] Loss: 0.2909399537053722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 861] Loss: 0.29108490865399067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 862] Loss: 0.29134684329028726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 863] Loss: 0.29128448534018786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 864] Loss: 0.29145600230403773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 865] Loss: 0.2913511598092872\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 866] Loss: 0.29133894052205045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 867] Loss: 0.2912028132745534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 868] Loss: 0.29110072502221046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 869] Loss: 0.29097813127251965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 870] Loss: 0.29109816765681856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 871] Loss: 0.2909874191924964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 872] Loss: 0.2912533078965706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 873] Loss: 0.29140794612810633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 874] Loss: 0.29131873158611554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 875] Loss: 0.29110510411790047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 876] Loss: 0.2910830375196329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 877] Loss: 0.2912315151508932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 878] Loss: 0.2915997527804298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 879] Loss: 0.2914443064728236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 880] Loss: 0.29159369304350446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 881] Loss: 0.2915584177510243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 882] Loss: 0.2914576216716587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 883] Loss: 0.29162591189179987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 884] Loss: 0.2914083848820747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 885] Loss: 0.2914331054060974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 886] Loss: 0.29137500055836224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 887] Loss: 0.29117583352523324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 888] Loss: 0.2910313575970492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 889] Loss: 0.2909934916596742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 890] Loss: 0.29100659507310994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 891] Loss: 0.2912870755263564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 892] Loss: 0.2912562366285211\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 893] Loss: 0.2914097954577825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 894] Loss: 0.29129070473501734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 895] Loss: 0.2911207259119896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 896] Loss: 0.2909990058151961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 897] Loss: 0.2908533268061053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 898] Loss: 0.290842258624841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 899] Loss: 0.2908127951692015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 900] Loss: 0.2906444189685017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 901] Loss: 0.2907269258201787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 902] Loss: 0.29118230434730563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 903] Loss: 0.291178134516844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 904] Loss: 0.2910431816477797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 905] Loss: 0.2908874824560351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 906] Loss: 0.2906754713063102\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 907] Loss: 0.29054450439475066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 908] Loss: 0.290483410861075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 909] Loss: 0.2906931819625001\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 910] Loss: 0.2909503687399527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 911] Loss: 0.29107701438777234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 912] Loss: 0.2910260898785002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 913] Loss: 0.2909951737388509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 914] Loss: 0.2908984984878791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 915] Loss: 0.2909810792859439\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 916] Loss: 0.29099927640608975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 917] Loss: 0.29090462617674157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 918] Loss: 0.2907227375192684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 919] Loss: 0.2906614272188827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 920] Loss: 0.2906651966721634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 921] Loss: 0.290469071685487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 922] Loss: 0.2903396594485384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 923] Loss: 0.2901830397919632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 924] Loss: 0.29009229301498296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 925] Loss: 0.29026959243675937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 926] Loss: 0.2901875646572548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 927] Loss: 0.2900908593565161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 928] Loss: 0.29016497928168894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 929] Loss: 0.2900214148483751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 930] Loss: 0.28982011562263643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 931] Loss: 0.2897227338352291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 932] Loss: 0.289794378368407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 933] Loss: 0.28969713522859947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 934] Loss: 0.28947795001214494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 935] Loss: 0.28930834808176564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 936] Loss: 0.2891692071432842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 937] Loss: 0.289309007327508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 938] Loss: 0.289273311446909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 939] Loss: 0.2893025997699073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 940] Loss: 0.28926189466115626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 941] Loss: 0.2893251510273315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 942] Loss: 0.289380396609945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 943] Loss: 0.2893333023052607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 944] Loss: 0.2893139983470828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 945] Loss: 0.28974702929721236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 946] Loss: 0.2896964044788185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 947] Loss: 0.289862037614589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 948] Loss: 0.28976984074771844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 949] Loss: 0.28962407849129224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 950] Loss: 0.2894855033547159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 951] Loss: 0.28941765470906744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 952] Loss: 0.28937427561883566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 953] Loss: 0.28943797646538366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 954] Loss: 0.2892756439093795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 955] Loss: 0.28908908697335345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 956] Loss: 0.2889266716145943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 957] Loss: 0.28874399324393824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 958] Loss: 0.28869689925176023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 959] Loss: 0.2888965981327138\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 960] Loss: 0.2890412221794353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 961] Loss: 0.28903362084039835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 962] Loss: 0.2890994191060744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 963] Loss: 0.28929052090501733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 964] Loss: 0.2891801496942424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 965] Loss: 0.2890098987069602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 966] Loss: 0.288983005693068\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 967] Loss: 0.2889330741753821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 968] Loss: 0.2888248091561648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 969] Loss: 0.2888407900202942\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 970] Loss: 0.28874662990242705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 971] Loss: 0.28862460124967754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 972] Loss: 0.28859573171975816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 973] Loss: 0.288566072009627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 974] Loss: 0.2884548506915754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 975] Loss: 0.2882583137870449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 976] Loss: 0.28861840195900135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 977] Loss: 0.28843340711522497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 978] Loss: 0.28835200954071666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 979] Loss: 0.28840905345318496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 980] Loss: 0.28835407275419966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 981] Loss: 0.28842114935033636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 982] Loss: 0.2883391381835254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 983] Loss: 0.28816442898567957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 984] Loss: 0.28831012935146494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 985] Loss: 0.2882897038240822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 986] Loss: 0.28822250273857153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 987] Loss: 0.2883816766581079\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 988] Loss: 0.28832416324322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 989] Loss: 0.28849483131453757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 990] Loss: 0.28842977713509865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 991] Loss: 0.28836327167057846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 992] Loss: 0.2882063073439197\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 993] Loss: 0.28811676509165574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 994] Loss: 0.28814152050367864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 995] Loss: 0.288103831401377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 996] Loss: 0.2880861413797625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 997] Loss: 0.2879609791111321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 998] Loss: 0.28791507313258696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 999] Loss: 0.28776611924411305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1000] Loss: 0.28757431986343923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1001] Loss: 0.2874681627355426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1002] Loss: 0.2874151254848109\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1003] Loss: 0.28766794093625103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1004] Loss: 0.2877253345899038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1005] Loss: 0.2878081267774105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1006] Loss: 0.2878013534562571\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8904000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1007] Loss: 0.28783322156903274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1008] Loss: 0.2878058532239908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1009] Loss: 0.287908909478748\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1010] Loss: 0.2878180850501084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1011] Loss: 0.28772089031948955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1012] Loss: 0.28802594082298677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1013] Loss: 0.2880617854229751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1014] Loss: 0.2878759579568717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1015] Loss: 0.28805071369256124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1016] Loss: 0.2881030380195255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1017] Loss: 0.288005967797498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1018] Loss: 0.28811259089664787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1019] Loss: 0.28811881536915457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1020] Loss: 0.2884566010806361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1021] Loss: 0.2883776483514647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1022] Loss: 0.2884000686300422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1023] Loss: 0.2883650014700262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1024] Loss: 0.2883435298228989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1025] Loss: 0.28824203744822857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1026] Loss: 0.28827333152586054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1027] Loss: 0.28842241435134947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1028] Loss: 0.2883683134849936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1029] Loss: 0.2883774972287938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1030] Loss: 0.28831125067501534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1031] Loss: 0.28843439072661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1032] Loss: 0.28837055699154374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1033] Loss: 0.28851001013925565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1034] Loss: 0.28866167188385833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1035] Loss: 0.28866863762869416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1036] Loss: 0.28865525612081866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1037] Loss: 0.2885827949169532\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1038] Loss: 0.28857773614529836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1039] Loss: 0.2886780152526077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1040] Loss: 0.28861432907661955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1041] Loss: 0.28868833394239307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1042] Loss: 0.28881474639663585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1043] Loss: 0.2886759674244763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1044] Loss: 0.2887566531215067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1045] Loss: 0.2887318641377183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1046] Loss: 0.2888850285300375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1047] Loss: 0.28872241698305573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1048] Loss: 0.28872853069640303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1049] Loss: 0.28867603453782553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1050] Loss: 0.2885267515573205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1051] Loss: 0.28835613176540026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1052] Loss: 0.288419143603661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1053] Loss: 0.28855230576778185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1054] Loss: 0.28844285954510857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1055] Loss: 0.288398811618487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1056] Loss: 0.2882883891562526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1057] Loss: 0.2885201356086894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1058] Loss: 0.28874073615554163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1059] Loss: 0.2886940284121421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1060] Loss: 0.2885804872377224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1061] Loss: 0.2884553927984653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1062] Loss: 0.28841801045520643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1063] Loss: 0.2883178044554191\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1064] Loss: 0.28842560904728454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1065] Loss: 0.2884050111585068\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1066] Loss: 0.2883663181563125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1067] Loss: 0.28838489162551484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1068] Loss: 0.2882508360907814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1069] Loss: 0.28825037485282673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1070] Loss: 0.28834035758960974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1071] Loss: 0.2882659821220679\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1072] Loss: 0.28817984862593476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1073] Loss: 0.2881406219850542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1074] Loss: 0.2879880467121349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1075] Loss: 0.28799412595891505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1076] Loss: 0.2879490707796606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1077] Loss: 0.28788915753308963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1078] Loss: 0.2878832868877573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1079] Loss: 0.28793597428103845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1080] Loss: 0.2878471748912057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1081] Loss: 0.28776629068317466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1082] Loss: 0.28764565341596593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1083] Loss: 0.28759431862488305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1084] Loss: 0.28747458764256095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1085] Loss: 0.28740149133459286\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1086] Loss: 0.28732319804775175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1087] Loss: 0.28720434699999425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1088] Loss: 0.2870703604165207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1089] Loss: 0.28702313627539083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1090] Loss: 0.28696825432887274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1091] Loss: 0.2870274965769678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1092] Loss: 0.286905191461757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1093] Loss: 0.2869281003881684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1094] Loss: 0.28679182248393587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1095] Loss: 0.2868219379028049\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1096] Loss: 0.2868273165469646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1097] Loss: 0.28669972947010625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1098] Loss: 0.28686147099335263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1099] Loss: 0.2869623959936215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1100] Loss: 0.2870747843289484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1101] Loss: 0.2871620897883481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1102] Loss: 0.28704713462913917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1103] Loss: 0.2869264272827051\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1104] Loss: 0.2872863090287782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1105] Loss: 0.2871586203033274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1106] Loss: 0.2870966201879889\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1107] Loss: 0.2872262894431173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1108] Loss: 0.2874559716727662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1109] Loss: 0.2875444630322897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1110] Loss: 0.28776720857997823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1111] Loss: 0.2876854110879665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1112] Loss: 0.2876565888079847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1113] Loss: 0.28766086871066676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1114] Loss: 0.2876911333034015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1115] Loss: 0.28768657766752415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1116] Loss: 0.28768022792978826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1117] Loss: 0.28776149932804296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1118] Loss: 0.2877283343059569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1119] Loss: 0.28756255107385037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1120] Loss: 0.28744945430568514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1121] Loss: 0.28769042554535107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1122] Loss: 0.2876003732306144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1123] Loss: 0.2875928221227125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1124] Loss: 0.28752221846894777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1125] Loss: 0.287539749293189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1126] Loss: 0.28768967584533206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1127] Loss: 0.2877100784656942\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1128] Loss: 0.28788839193692933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1129] Loss: 0.2877707282459078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1130] Loss: 0.2878386780619621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1131] Loss: 0.28786513481295767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1132] Loss: 0.28788030440419143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1133] Loss: 0.2877238837288732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1134] Loss: 0.287672919606455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1135] Loss: 0.28753328384683197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1136] Loss: 0.28766456544794516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1137] Loss: 0.2875973724500135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1138] Loss: 0.2875496749515462\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1139] Loss: 0.2876558918473056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1140] Loss: 0.28759654361902354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1141] Loss: 0.28781384130625026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1142] Loss: 0.2877615587024072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1143] Loss: 0.2876327635756381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1144] Loss: 0.2875857410447131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1145] Loss: 0.2874342086087716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1146] Loss: 0.2872764568491843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1147] Loss: 0.2875823379084859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1148] Loss: 0.2877206184941223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1149] Loss: 0.2876490827102761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1150] Loss: 0.28761668147999125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1151] Loss: 0.28751644130672666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1152] Loss: 0.28765174322993003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1153] Loss: 0.2874986654218896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1154] Loss: 0.28766418263329124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1155] Loss: 0.28750848439724547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1156] Loss: 0.2875078307686424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1157] Loss: 0.28740530564553207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1158] Loss: 0.28731271453970736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1159] Loss: 0.287374026982995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1160] Loss: 0.28766044767904075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1161] Loss: 0.28762942222575416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1162] Loss: 0.2877983068379227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1163] Loss: 0.28791374361885647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1164] Loss: 0.2877992808407396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1165] Loss: 0.288106742938017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1166] Loss: 0.28826581156017655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1167] Loss: 0.28822229285812623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1168] Loss: 0.28840367941895334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1169] Loss: 0.2882899876261495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1170] Loss: 0.2883965788736875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1171] Loss: 0.2884528552330583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1172] Loss: 0.28841527665693123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1173] Loss: 0.28848208841701894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1174] Loss: 0.2883084207339813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1175] Loss: 0.2885459063908993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1176] Loss: 0.2885925600815797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1177] Loss: 0.2885220336629262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1178] Loss: 0.28866003114549094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1179] Loss: 0.28867897967565204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1180] Loss: 0.28849710670557427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1181] Loss: 0.2887779963677939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1182] Loss: 0.288732930593756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1183] Loss: 0.28870086373796083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1184] Loss: 0.28884567912475245\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1185] Loss: 0.28883549312792595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1186] Loss: 0.28876647897613544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1187] Loss: 0.2888811618127496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1188] Loss: 0.28900617685417757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1189] Loss: 0.2889398300227382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1190] Loss: 0.28894340175239347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1191] Loss: 0.28908727917989835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1192] Loss: 0.28919764183832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1193] Loss: 0.28908117738260763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1194] Loss: 0.2889553601309168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1195] Loss: 0.28892886436411314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1196] Loss: 0.2887894137943121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1197] Loss: 0.2888751077284449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1198] Loss: 0.28896309587127106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1199] Loss: 0.28880539464091937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1200] Loss: 0.2890097488668673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1201] Loss: 0.28885816254444346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1202] Loss: 0.28877589928177666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1203] Loss: 0.28894951651012757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1204] Loss: 0.28889738051865477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1205] Loss: 0.2887946147720019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1206] Loss: 0.28869970652781163\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1207] Loss: 0.28866356710427615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1208] Loss: 0.2885262031767731\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1209] Loss: 0.28859937627276905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1210] Loss: 0.2885311307810649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1211] Loss: 0.2884255811559719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1212] Loss: 0.28858352209307875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1213] Loss: 0.2888462833924503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1214] Loss: 0.2888598175668539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1215] Loss: 0.2887447747994553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1216] Loss: 0.2887520547633226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1217] Loss: 0.28865850689166256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1218] Loss: 0.28883967478586836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1219] Loss: 0.28878103947202494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1220] Loss: 0.2891834063478458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1221] Loss: 0.2893906698108798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1222] Loss: 0.2894718847011775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1223] Loss: 0.2895317821654878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1224] Loss: 0.2896215282166659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1225] Loss: 0.28949264951783127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1226] Loss: 0.2895523091437971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1227] Loss: 0.28949766499394286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1228] Loss: 0.2894830106409573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1229] Loss: 0.2893799272445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1230] Loss: 0.28935498026560763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1231] Loss: 0.2893996787170893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1232] Loss: 0.2893890317380671\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1233] Loss: 0.2896316183567144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1234] Loss: 0.2899174418715659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1235] Loss: 0.28991003308475505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1236] Loss: 0.29014389313134015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1237] Loss: 0.29000160986460843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1238] Loss: 0.28998429146923865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1239] Loss: 0.28988839452256066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1240] Loss: 0.28992906418527187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1241] Loss: 0.28984968191235777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1242] Loss: 0.2898096714768086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1243] Loss: 0.2897483010464897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1244] Loss: 0.2896063714028848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1245] Loss: 0.2897478629204054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1246] Loss: 0.2897233095779523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1247] Loss: 0.2896465693155061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1248] Loss: 0.2895496150453246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1249] Loss: 0.2896569258566359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1250] Loss: 0.2898745708616383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1251] Loss: 0.29028781884339416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1252] Loss: 0.29034291607504764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1253] Loss: 0.2903870848926883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1254] Loss: 0.2902992200538862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1255] Loss: 0.29035212438702585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1256] Loss: 0.29025788658218893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1257] Loss: 0.29017210557176093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1258] Loss: 0.2902904840309622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1259] Loss: 0.29027961549219905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1260] Loss: 0.290395315478285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1261] Loss: 0.2904527438448588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1262] Loss: 0.290697011530589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1263] Loss: 0.29059211647401556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1264] Loss: 0.2905110651309098\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1265] Loss: 0.29064844977406284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1266] Loss: 0.29075164116275015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1267] Loss: 0.29079506295347934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1268] Loss: 0.2908620304197238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1269] Loss: 0.29111407586974625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1270] Loss: 0.29117184880340524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1271] Loss: 0.2911300897821902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1272] Loss: 0.2912608726566103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1273] Loss: 0.2911770170266602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1274] Loss: 0.29114947323610313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1275] Loss: 0.2914309025925445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1276] Loss: 0.29143300637849007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1277] Loss: 0.29152135362954074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1278] Loss: 0.2915102011126884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1279] Loss: 0.29163360885188006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1280] Loss: 0.2916225390749819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1281] Loss: 0.291697238393852\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1282] Loss: 0.2917182091900681\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1283] Loss: 0.2918681195399291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1284] Loss: 0.2918076194048021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1285] Loss: 0.291687545547029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1286] Loss: 0.2915485388803631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1287] Loss: 0.29141402607798206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1288] Loss: 0.2914069890569451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1289] Loss: 0.2914292646682244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1290] Loss: 0.29133038495881086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1291] Loss: 0.29132479888678153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1292] Loss: 0.29121916537356174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1293] Loss: 0.29143590346464643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1294] Loss: 0.29143492141673505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1295] Loss: 0.29196263300471526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1296] Loss: 0.29206142670350144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1297] Loss: 0.29199822382545876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1298] Loss: 0.29237587861169334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1299] Loss: 0.2923525995308594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1300] Loss: 0.2924931269527402\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1301] Loss: 0.2924009422127755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1302] Loss: 0.29237216834550467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1303] Loss: 0.29233011342183285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1304] Loss: 0.29220994596324396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1305] Loss: 0.29237576372921464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1306] Loss: 0.29236893325494306\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1307] Loss: 0.2925709683373685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1308] Loss: 0.2924334554492009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1309] Loss: 0.29246217184957185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1310] Loss: 0.29243266462594614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1311] Loss: 0.29237688931809086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1312] Loss: 0.2923688469675541\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1313] Loss: 0.2922600719063836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1314] Loss: 0.29223788220569136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1315] Loss: 0.2921567460394088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1316] Loss: 0.29235033274924455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1317] Loss: 0.2924080281064096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1318] Loss: 0.29256427341395447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1319] Loss: 0.29253671570047396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1320] Loss: 0.2924857855862991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1321] Loss: 0.29242722985637587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1322] Loss: 0.29244509777202693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1323] Loss: 0.2924109593214685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1324] Loss: 0.2926017203120412\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1325] Loss: 0.29254816333678635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1326] Loss: 0.29255282916100434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1327] Loss: 0.2924907383665375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1328] Loss: 0.29252667164180823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1329] Loss: 0.292452933733827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1330] Loss: 0.2923980130676953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1331] Loss: 0.29243087497726644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1332] Loss: 0.2924606564153651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1333] Loss: 0.2925437655501876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1334] Loss: 0.2925268100270479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1335] Loss: 0.292756985396819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1336] Loss: 0.29270831490485494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1337] Loss: 0.2928494196552951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1338] Loss: 0.2929230701382323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1339] Loss: 0.2929560736134492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1340] Loss: 0.2928658434291011\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1341] Loss: 0.29284092676853707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1342] Loss: 0.29281109662847726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1343] Loss: 0.29274261458839534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1344] Loss: 0.29264798122746805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1345] Loss: 0.29255656734553737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1346] Loss: 0.29260590804725506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1347] Loss: 0.29258300025354733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1348] Loss: 0.29253195073075555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1349] Loss: 0.29267855874440146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1350] Loss: 0.2926094108797804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1351] Loss: 0.29256274574401653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1352] Loss: 0.2925470315622416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1353] Loss: 0.2925276084186769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1354] Loss: 0.29250120873624436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1355] Loss: 0.2924781832430098\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1356] Loss: 0.2924227873587238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1357] Loss: 0.2923730755632622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1358] Loss: 0.2923673717449615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1359] Loss: 0.29231652052883206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1360] Loss: 0.2923272099992006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1361] Loss: 0.29237783815612833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1362] Loss: 0.292281705433161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1363] Loss: 0.2921741392479905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1364] Loss: 0.2922454439624431\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1365] Loss: 0.29227841859135556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1366] Loss: 0.29227322365193575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1367] Loss: 0.29218722248611295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1368] Loss: 0.2922261806091489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1369] Loss: 0.2921281963848998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1370] Loss: 0.2920848621553554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1371] Loss: 0.29224659565776195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1372] Loss: 0.29248392952561114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1373] Loss: 0.2925297453650954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1374] Loss: 0.29265894649932644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1375] Loss: 0.2928146243312933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1376] Loss: 0.29266802244904616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1377] Loss: 0.292576964379834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1378] Loss: 0.2924852157737422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1379] Loss: 0.2924409670839317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1380] Loss: 0.29239494574069974\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1381] Loss: 0.2924650904809146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1382] Loss: 0.2924638961157075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1383] Loss: 0.2923427954402648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1384] Loss: 0.29228834366997225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1385] Loss: 0.29227594794786493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1386] Loss: 0.29217625323583213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1387] Loss: 0.29212532535346303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1388] Loss: 0.29199405914187343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1389] Loss: 0.29189860045565347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1390] Loss: 0.2917755273221202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1391] Loss: 0.29174609516154637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1392] Loss: 0.2916362328748954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1393] Loss: 0.2915316331401596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1394] Loss: 0.2914680919634772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1395] Loss: 0.29160954936695616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1396] Loss: 0.29148305218203474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1397] Loss: 0.2914105619560799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1398] Loss: 0.2913245348378813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1399] Loss: 0.29129373575620876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1400] Loss: 0.29134043854090474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1401] Loss: 0.2912869254196527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1402] Loss: 0.2912110391532257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1403] Loss: 0.29122215932040596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1404] Loss: 0.29115229722024544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1405] Loss: 0.29122760358133487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1406] Loss: 0.291293166694813\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1407] Loss: 0.29125487375552744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1408] Loss: 0.2913264397980482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1409] Loss: 0.29123360656753733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1410] Loss: 0.2912605446799794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1411] Loss: 0.2911518899350299\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1412] Loss: 0.2911262972533237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1413] Loss: 0.2910405740258284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1414] Loss: 0.2911825089049813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1415] Loss: 0.29110579187882707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1416] Loss: 0.2909721498536692\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1417] Loss: 0.2909794812204142\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1418] Loss: 0.2909525650786071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1419] Loss: 0.2908417419168197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1420] Loss: 0.29074811665834893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1421] Loss: 0.2906915120422672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1422] Loss: 0.29074870510553924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1423] Loss: 0.29077434015416964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1424] Loss: 0.29105889500027565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1425] Loss: 0.2910436484700357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1426] Loss: 0.29098746271765963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1427] Loss: 0.29105595358858294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1428] Loss: 0.2909713620693015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1429] Loss: 0.2909137130599846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1430] Loss: 0.2913299629876488\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1431] Loss: 0.2914886205322612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1432] Loss: 0.29145357431484925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1433] Loss: 0.29135785017068644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1434] Loss: 0.29132048600073707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1435] Loss: 0.2912359864249096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1436] Loss: 0.2913115682571106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1437] Loss: 0.29156841548919343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1438] Loss: 0.29156157092580576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1439] Loss: 0.2916545894625629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1440] Loss: 0.2916191523810297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1441] Loss: 0.2916146295876722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1442] Loss: 0.29158109002380467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1443] Loss: 0.2919124400396009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1444] Loss: 0.29201037444490124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1445] Loss: 0.29208119796175097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1446] Loss: 0.2920741822821328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1447] Loss: 0.2921206934710812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1448] Loss: 0.2920959873263074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1449] Loss: 0.2922425240463993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1450] Loss: 0.2922079507454869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1451] Loss: 0.29212226131274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1452] Loss: 0.2921539791489603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1453] Loss: 0.2921726433714615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1454] Loss: 0.29227942095656817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1455] Loss: 0.2923253365117928\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1456] Loss: 0.2924397550702999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1457] Loss: 0.29263044666940186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1458] Loss: 0.29256424120148544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1459] Loss: 0.29245309164796274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1460] Loss: 0.2925001993826574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1461] Loss: 0.29241533313396867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1462] Loss: 0.29244425624874903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1463] Loss: 0.29248249193523157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1464] Loss: 0.2923817449109533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1465] Loss: 0.29277126397172065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1466] Loss: 0.2927492950404857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1467] Loss: 0.2927327400070861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1468] Loss: 0.29262851503558623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1469] Loss: 0.2926127260622105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1470] Loss: 0.29254647844277143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1471] Loss: 0.2925528209925836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1472] Loss: 0.29289725072422884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1473] Loss: 0.2928297760152849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1474] Loss: 0.2928982851272379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1475] Loss: 0.29291012342892536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1476] Loss: 0.29292223587844907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1477] Loss: 0.2930294887425945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1478] Loss: 0.29292848822581713\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1479] Loss: 0.2928873148406862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1480] Loss: 0.29294814276493203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1481] Loss: 0.292850644638141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1482] Loss: 0.2930805654382899\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1483] Loss: 0.29311970214035576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1484] Loss: 0.29309056858463495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1485] Loss: 0.2930779658358645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1486] Loss: 0.2931878083284283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1487] Loss: 0.2931390109593891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1488] Loss: 0.2930506258117775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1489] Loss: 0.29305401865161973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1490] Loss: 0.2930285650973368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1491] Loss: 0.2929739628192714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1492] Loss: 0.29284385693959125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1493] Loss: 0.2927844179643979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1494] Loss: 0.2929644679676934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1495] Loss: 0.29288474940913634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1496] Loss: 0.29283197622403934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1497] Loss: 0.2928156609379814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1498] Loss: 0.2929452588184121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1499] Loss: 0.29290668074405657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1500] Loss: 0.2929382095418646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1501] Loss: 0.29282749788387413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1502] Loss: 0.2927752585662311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1503] Loss: 0.2927668414293765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1504] Loss: 0.29272886805032555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1505] Loss: 0.2927570982227723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1506] Loss: 0.29265650376667585\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8972\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1507] Loss: 0.2925549834202752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1508] Loss: 0.2925330818205061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1509] Loss: 0.2925250728347121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1510] Loss: 0.29259170879458274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1511] Loss: 0.29251350182916225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1512] Loss: 0.2924193452101871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1513] Loss: 0.2923853552359248\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1514] Loss: 0.2924383853082628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1515] Loss: 0.2926502729537866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1516] Loss: 0.29256928721005204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1517] Loss: 0.29253642965187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1518] Loss: 0.29254983183826266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1519] Loss: 0.2926030778516788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1520] Loss: 0.2924946116683113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1521] Loss: 0.29247005029746126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1522] Loss: 0.2924148524198472\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1523] Loss: 0.29237742602766265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 10, Batch 1524] Loss: 0.2924227816021466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 0] Loss: 0.2924199492572562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1] Loss: 0.2923329832922621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 2] Loss: 0.2923610741700904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 3] Loss: 0.29266633178967827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 4] Loss: 0.29265611316138523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 5] Loss: 0.292563698404148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 6] Loss: 0.2924489693894377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 7] Loss: 0.2923849215529757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 8] Loss: 0.292317471115884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 9] Loss: 0.29221189823647276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 10] Loss: 0.2922040359695363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 11] Loss: 0.2921222732405426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 12] Loss: 0.292051602023661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 13] Loss: 0.29211355695733043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 14] Loss: 0.2921038049247694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 15] Loss: 0.2922200762050936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 16] Loss: 0.2922531888228453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 17] Loss: 0.29222724692400354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 18] Loss: 0.2922140953938812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 19] Loss: 0.2922108480059303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 20] Loss: 0.2921196952026773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 21] Loss: 0.2920419396584797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 22] Loss: 0.2920187629527387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 23] Loss: 0.29198050521656976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 24] Loss: 0.2919974076112858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 25] Loss: 0.29190224743294485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 26] Loss: 0.29184132905491417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 27] Loss: 0.2920284109273185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 28] Loss: 0.29225913243180557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 29] Loss: 0.292176140771711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 30] Loss: 0.2920922847380561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 31] Loss: 0.2920239314744351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 32] Loss: 0.291942758509678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 33] Loss: 0.2920367334371448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 34] Loss: 0.2921659906599743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 35] Loss: 0.2922823162563744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 36] Loss: 0.2923864925131776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 37] Loss: 0.2923882716050038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 38] Loss: 0.2922772891551677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 39] Loss: 0.292246184822571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 40] Loss: 0.2922835862264037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 41] Loss: 0.292327732883494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 42] Loss: 0.29229860205713676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 43] Loss: 0.2922612698897672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 44] Loss: 0.29218284736680405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 45] Loss: 0.2921040273750552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 46] Loss: 0.2920549359995667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 47] Loss: 0.29201234599911796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 48] Loss: 0.2919925062356479\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 49] Loss: 0.29205346807903965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 50] Loss: 0.29192554969221923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 51] Loss: 0.2918718481077122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 52] Loss: 0.29177148508870115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 53] Loss: 0.2917523708594732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 54] Loss: 0.29163393564758944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 55] Loss: 0.29176107723561545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 56] Loss: 0.29164092207734055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 57] Loss: 0.29180018857036427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 58] Loss: 0.2918573657501635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 59] Loss: 0.2918882959341233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 60] Loss: 0.2918806137823606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 61] Loss: 0.291925597509427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 62] Loss: 0.29196941020854955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 63] Loss: 0.2919524426013702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 64] Loss: 0.29201867596029935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 65] Loss: 0.29205809029682955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 66] Loss: 0.29204150114542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 67] Loss: 0.2918984140512999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 68] Loss: 0.29184547933733435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 69] Loss: 0.29176629985031505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 70] Loss: 0.2916713741309238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 71] Loss: 0.2916030944667471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 72] Loss: 0.29171699961301073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 73] Loss: 0.29173298607587966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 74] Loss: 0.29165062764507615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 75] Loss: 0.29165093414835797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 76] Loss: 0.2916550528547519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 77] Loss: 0.29165704511969104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 78] Loss: 0.2917192361418983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 79] Loss: 0.2916029360683506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 80] Loss: 0.29154656423255804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 81] Loss: 0.2915721797034712\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 82] Loss: 0.29148432250586936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 83] Loss: 0.29160155006408395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 84] Loss: 0.2915504971087128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 85] Loss: 0.29156322159871134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 86] Loss: 0.29159709250793364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 87] Loss: 0.2915026971139593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 88] Loss: 0.29160036541410345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 89] Loss: 0.29150119407196234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 90] Loss: 0.29145324623547725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 91] Loss: 0.29155845623775567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 92] Loss: 0.2920284713560683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 93] Loss: 0.2919351931913107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 94] Loss: 0.29187422248510625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 95] Loss: 0.29194937748805655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 96] Loss: 0.292052418270176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 97] Loss: 0.2921796939995234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 98] Loss: 0.29216230348470923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 99] Loss: 0.2922435404158728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 100] Loss: 0.29236603835482655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 101] Loss: 0.29238528825852844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 102] Loss: 0.29229149863619985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 103] Loss: 0.2922116390493132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 104] Loss: 0.29217780649368397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 105] Loss: 0.2922403450929202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 106] Loss: 0.2922757709488218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 107] Loss: 0.2923733152600249\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 108] Loss: 0.2923865416419008\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 109] Loss: 0.2923227320828271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 110] Loss: 0.2923544278539763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 111] Loss: 0.29238125384036634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 112] Loss: 0.2923732198336545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 113] Loss: 0.29234230801216754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 114] Loss: 0.2923791041025479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 115] Loss: 0.29228521230023935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 116] Loss: 0.29232017595712595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 117] Loss: 0.29230580312781895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 118] Loss: 0.2922378647276479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 119] Loss: 0.292253886693276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 120] Loss: 0.29237849914082664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 121] Loss: 0.2923415619232973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 122] Loss: 0.2923534802375554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 123] Loss: 0.29228860367490833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 124] Loss: 0.29220436169899583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 125] Loss: 0.2921237632768132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 126] Loss: 0.292066745351398\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 127] Loss: 0.2920303849122274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 128] Loss: 0.2919674367345508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 129] Loss: 0.2919757545916076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 130] Loss: 0.2919033917965311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 131] Loss: 0.291965407983308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 132] Loss: 0.2920448511712632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 133] Loss: 0.2920511458057818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 134] Loss: 0.29196713591825574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 135] Loss: 0.29187191441159954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 136] Loss: 0.2917932402342558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 137] Loss: 0.2919493145390886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 138] Loss: 0.29209213047788285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 139] Loss: 0.2922291052294755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 140] Loss: 0.29226003717406684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 141] Loss: 0.29231639863101205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 142] Loss: 0.2922898633439188\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 143] Loss: 0.29229330952921617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 144] Loss: 0.29233054577623707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 145] Loss: 0.29222449920735916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 146] Loss: 0.2921391890383139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 147] Loss: 0.29212390892900386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 148] Loss: 0.29203661169401174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 149] Loss: 0.2920045359199671\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 150] Loss: 0.29192736326398966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 151] Loss: 0.29192061879226794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 152] Loss: 0.2920629492198879\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 153] Loss: 0.29198106340462765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 154] Loss: 0.2919085233460048\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 155] Loss: 0.2919228116700898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 156] Loss: 0.2920953248291852\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 157] Loss: 0.2921249261286974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 158] Loss: 0.2922118752395581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 159] Loss: 0.2922937227562824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 160] Loss: 0.2922364924901298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 161] Loss: 0.2922006468645949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 162] Loss: 0.2922529502817198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 163] Loss: 0.29219484262819173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 164] Loss: 0.29211565959234137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 165] Loss: 0.2920919105543227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 166] Loss: 0.2921264880687034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 167] Loss: 0.2921487656954696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 168] Loss: 0.2921604467953127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 169] Loss: 0.2921701085027539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 170] Loss: 0.2921497393730124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 171] Loss: 0.2921616197037739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 172] Loss: 0.2921913136281161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 173] Loss: 0.2921379882593656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 174] Loss: 0.2922201257423358\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 175] Loss: 0.29217884686492535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 176] Loss: 0.2921253198599619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 177] Loss: 0.29211637489256187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 178] Loss: 0.29208433269155604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 179] Loss: 0.2920804692672378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 180] Loss: 0.292079424761674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 181] Loss: 0.29200302752973334\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 182] Loss: 0.29194866559445226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 183] Loss: 0.29192196943972715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 184] Loss: 0.2918668562205325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 185] Loss: 0.29203270734055653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 186] Loss: 0.29194874713003005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 187] Loss: 0.29185095657258964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 188] Loss: 0.29180415506873814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 189] Loss: 0.2917008144969547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 190] Loss: 0.29174133869901037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 191] Loss: 0.29181833346202873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 192] Loss: 0.29175729103926046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 193] Loss: 0.2916969763374927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 194] Loss: 0.29159374881096733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 195] Loss: 0.2916406011268627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 196] Loss: 0.2918104525038968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 197] Loss: 0.29191613056733295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 198] Loss: 0.2918734494444933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 199] Loss: 0.2919589134931148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 200] Loss: 0.29193153014016704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 201] Loss: 0.29196696760071217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 202] Loss: 0.2919693672110118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 203] Loss: 0.29203403912791664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 204] Loss: 0.292061672824718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 205] Loss: 0.29199847368226534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 206] Loss: 0.2920505509281214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 207] Loss: 0.2919771010018389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 208] Loss: 0.29201866646676705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 209] Loss: 0.29196872421259684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 210] Loss: 0.29201151185986624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 211] Loss: 0.29191762449974135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 212] Loss: 0.2918289297585124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 213] Loss: 0.2917729295441957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 214] Loss: 0.29196166681102126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 215] Loss: 0.29185418448805467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 216] Loss: 0.2918745706604648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 217] Loss: 0.2918536590136669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 218] Loss: 0.29191239051289336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 219] Loss: 0.29184860668009627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 220] Loss: 0.2919535780775136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 221] Loss: 0.29188298316333294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 222] Loss: 0.29183983220264365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 223] Loss: 0.29187800359500254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 224] Loss: 0.29179164489577397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 225] Loss: 0.291746105821563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 226] Loss: 0.29183779099587037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 227] Loss: 0.29196190534657457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 228] Loss: 0.29193408893809164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 229] Loss: 0.29188549288550125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 230] Loss: 0.29180523412568227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 231] Loss: 0.29189580767444717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 232] Loss: 0.2919031719945065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 233] Loss: 0.2917869767323944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 234] Loss: 0.291752991477875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 235] Loss: 0.2917294156517398\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 236] Loss: 0.2917372002213311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 237] Loss: 0.2918575400646668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 238] Loss: 0.29180807302429407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 239] Loss: 0.29177418629034735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 240] Loss: 0.29169019242240624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 241] Loss: 0.2915824033248025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 242] Loss: 0.29160255869421325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 243] Loss: 0.29175391185540606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 244] Loss: 0.2917486645382683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 245] Loss: 0.29170949652411104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 246] Loss: 0.2917569035565057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 247] Loss: 0.29172928749368093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 248] Loss: 0.291641543564551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 249] Loss: 0.2916681739639063\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 250] Loss: 0.29164169770849624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 251] Loss: 0.2916115649351624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 252] Loss: 0.2917545903095677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 253] Loss: 0.29179704603641904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 254] Loss: 0.2918566986582459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 255] Loss: 0.29183192224569726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 256] Loss: 0.2918949325711609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 257] Loss: 0.2919640306616071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 258] Loss: 0.29193680577558395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 259] Loss: 0.29194600456313113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 260] Loss: 0.29188874054993136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 261] Loss: 0.2918684737174314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 262] Loss: 0.2918559617616914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 263] Loss: 0.2918343822556938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 264] Loss: 0.29180169550727036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 265] Loss: 0.2918048462029599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 266] Loss: 0.2918046566450182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 267] Loss: 0.29177164285124246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 268] Loss: 0.2916706076862761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 269] Loss: 0.29158682983354056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 270] Loss: 0.29158840941566994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 271] Loss: 0.2916431390019451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 272] Loss: 0.29165929023292847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 273] Loss: 0.29167823681718996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 274] Loss: 0.29169981948690804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 275] Loss: 0.29174072494662906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 276] Loss: 0.29177155303779184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 277] Loss: 0.29177137358144184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 278] Loss: 0.291783673044406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 279] Loss: 0.291785110589562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 280] Loss: 0.29180667866849236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 281] Loss: 0.291776619148314\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 282] Loss: 0.2917109963491344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 283] Loss: 0.2917948144338756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 284] Loss: 0.29172956559005836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 285] Loss: 0.2917949797250227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 286] Loss: 0.29182275125173107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 287] Loss: 0.2917993121842818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 288] Loss: 0.2918239869548222\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 289] Loss: 0.2917359754393745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 290] Loss: 0.29202409215806596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 291] Loss: 0.29197490076657356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 292] Loss: 0.2919860051258181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 293] Loss: 0.29197066024118407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 294] Loss: 0.29190561662878856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 295] Loss: 0.2919561602318911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 296] Loss: 0.2919815688923264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 297] Loss: 0.2919027660622266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 298] Loss: 0.291799778448637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 299] Loss: 0.29176334668526355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 300] Loss: 0.2916956639510917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 301] Loss: 0.29165539189248474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 302] Loss: 0.2916850910842026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 303] Loss: 0.2916913760955753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 304] Loss: 0.2917515325335492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 305] Loss: 0.29171930647467914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 306] Loss: 0.2916767781749286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 307] Loss: 0.2916959376162561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 308] Loss: 0.2917788139701346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 309] Loss: 0.29181896877246455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 310] Loss: 0.29187229796429803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 311] Loss: 0.29213084642236703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 312] Loss: 0.29206804114934787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 313] Loss: 0.2920880135955605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 314] Loss: 0.29209986706482677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 315] Loss: 0.29213621168516635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 316] Loss: 0.2920233352974155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 317] Loss: 0.29192343204564614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 318] Loss: 0.2919776692410872\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 319] Loss: 0.2918961784473705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 320] Loss: 0.29192230244288625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 321] Loss: 0.29205423282304577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 322] Loss: 0.292025876311377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 323] Loss: 0.292023809918779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 324] Loss: 0.2920122393113995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 325] Loss: 0.29194897347430226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 326] Loss: 0.29194215111603977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 327] Loss: 0.29191943240298024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 328] Loss: 0.291861323418007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 329] Loss: 0.2918449762569949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 330] Loss: 0.29196367500198855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 331] Loss: 0.291872794010844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 332] Loss: 0.2918955326707909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 333] Loss: 0.2918570595805991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 334] Loss: 0.29184367522163296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 335] Loss: 0.2918314327689194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 336] Loss: 0.2917791876845576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 337] Loss: 0.29184286809121657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 338] Loss: 0.2919689816441808\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 339] Loss: 0.291999872972274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 340] Loss: 0.29191968934510343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 341] Loss: 0.2918841270734904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 342] Loss: 0.29187725955734445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 343] Loss: 0.29191115860873756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 344] Loss: 0.29183737301295665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 345] Loss: 0.2918178880582227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 346] Loss: 0.2918704878079674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 347] Loss: 0.29187786439458635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 348] Loss: 0.29190825997219605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 349] Loss: 0.2918283816022373\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 350] Loss: 0.2918460791522169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 351] Loss: 0.29177511778507303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 352] Loss: 0.29176732641445774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 353] Loss: 0.2918364724499366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 354] Loss: 0.29181198069672704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 355] Loss: 0.291798055823644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 356] Loss: 0.29184286580728824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 357] Loss: 0.291761974721242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 358] Loss: 0.29184388714476517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 359] Loss: 0.2917488552094648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 360] Loss: 0.2916809190778022\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 361] Loss: 0.2917889671430887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 362] Loss: 0.29171101131360666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 363] Loss: 0.29164668806866645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 364] Loss: 0.29162808328930234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 365] Loss: 0.29175473511693334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 366] Loss: 0.2917745305207476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 367] Loss: 0.2918175799257868\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 368] Loss: 0.29178368582753306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 369] Loss: 0.2917616813980504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 370] Loss: 0.2917229648620363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 371] Loss: 0.29169652159096765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 372] Loss: 0.29177606327904204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 373] Loss: 0.2917598605754195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 374] Loss: 0.29172480048133803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 375] Loss: 0.29183839127067523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 376] Loss: 0.29189064767101647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 377] Loss: 0.2918713908636639\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 378] Loss: 0.29181192503619874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 379] Loss: 0.29180031551098184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 380] Loss: 0.2917430401554233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 381] Loss: 0.29168808454748585\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 382] Loss: 0.2916843395456907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 383] Loss: 0.29166051147175787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 384] Loss: 0.29169563147701133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 385] Loss: 0.29161498105432104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 386] Loss: 0.29163245420515976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 387] Loss: 0.29161521021958475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 388] Loss: 0.2915271254338371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 389] Loss: 0.2914634264025968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 390] Loss: 0.29158932586536984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 391] Loss: 0.2915712703333233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 392] Loss: 0.291706042913681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 393] Loss: 0.2916571841703244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 394] Loss: 0.29161939567174905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 395] Loss: 0.2915737980611953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 396] Loss: 0.29153378087335063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 397] Loss: 0.29149978805331433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 398] Loss: 0.29144869560261216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 399] Loss: 0.29134727307848163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 400] Loss: 0.29138179881653437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 401] Loss: 0.29131645444375287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 402] Loss: 0.29139176407261025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 403] Loss: 0.29138797712989356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 404] Loss: 0.29141259297976374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 405] Loss: 0.29136972315125653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 406] Loss: 0.29140464140649647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 407] Loss: 0.29143115634521055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 408] Loss: 0.29140978910152593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 409] Loss: 0.29139934107854365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 410] Loss: 0.2913702704205414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 411] Loss: 0.29142955395454323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 412] Loss: 0.29157119015266436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 413] Loss: 0.29148073431992866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 414] Loss: 0.29136528702300607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 415] Loss: 0.29129570243971603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 416] Loss: 0.2912321794385568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 417] Loss: 0.2912244607699736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 418] Loss: 0.291230686712468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 419] Loss: 0.29123783447021306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 420] Loss: 0.291172880352926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 421] Loss: 0.29116274168861705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 422] Loss: 0.2911283479836586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 423] Loss: 0.2910621150775908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 424] Loss: 0.29100827529735157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 425] Loss: 0.2909588505990401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 426] Loss: 0.29094111947352325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 427] Loss: 0.29088967233804414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 428] Loss: 0.290842588069166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 429] Loss: 0.2908181976765899\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 430] Loss: 0.2908534030921948\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 431] Loss: 0.29108277892360684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 432] Loss: 0.29100522710789056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 433] Loss: 0.29094145489743106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 434] Loss: 0.29099041260511993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 435] Loss: 0.29092064983475846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 436] Loss: 0.29085446317091673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 437] Loss: 0.2908984752115877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 438] Loss: 0.2908610867201308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 439] Loss: 0.2909137667206674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 440] Loss: 0.2908987297125313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 441] Loss: 0.29088475134970276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 442] Loss: 0.29091412800000593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 443] Loss: 0.2909301288513219\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 444] Loss: 0.29087370948566316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 445] Loss: 0.2908070782251637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 446] Loss: 0.29080730442067615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 447] Loss: 0.29075433779365834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 448] Loss: 0.2906919017971713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 449] Loss: 0.29062476788622404\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 450] Loss: 0.29053436140648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 451] Loss: 0.2906735863126455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 452] Loss: 0.29070102459322245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 453] Loss: 0.2907236441516719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 454] Loss: 0.29074072671380446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 455] Loss: 0.29073050893937485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 456] Loss: 0.2907204305044251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 457] Loss: 0.2906354692174319\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 458] Loss: 0.29067196586631666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 459] Loss: 0.29066279542949114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 460] Loss: 0.2906557977688734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 461] Loss: 0.2906117191682074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 462] Loss: 0.29064522974200613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 463] Loss: 0.29055894413456673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 464] Loss: 0.29051300488607656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 465] Loss: 0.2904367521855633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 466] Loss: 0.2903594570074977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 467] Loss: 0.29060919125009915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 468] Loss: 0.29057920625586986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 469] Loss: 0.29053439858826996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 470] Loss: 0.2904823324376315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 471] Loss: 0.29043124325382597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 472] Loss: 0.290452588278516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 473] Loss: 0.29042371603946737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 474] Loss: 0.2904385100403724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 475] Loss: 0.29044009269703003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 476] Loss: 0.2905782845080317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 477] Loss: 0.2905092360148683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 478] Loss: 0.2904818862624414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 479] Loss: 0.2904659096876522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 480] Loss: 0.2904230008758604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 481] Loss: 0.29043507624021\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8885\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 482] Loss: 0.290382918677517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 483] Loss: 0.2902913306594728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 484] Loss: 0.29034286698404543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 485] Loss: 0.2902583978382727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 486] Loss: 0.29020756744182596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 487] Loss: 0.29031794683800805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 488] Loss: 0.2903675700526729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 489] Loss: 0.2903130597450236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 490] Loss: 0.29027270811484823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 491] Loss: 0.29021814593342277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 492] Loss: 0.2903302316526118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 493] Loss: 0.2902907200279783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 494] Loss: 0.29023478588213275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 495] Loss: 0.29025304641750255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 496] Loss: 0.29028160514176954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 497] Loss: 0.2902605668168347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 498] Loss: 0.29038830245849634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 499] Loss: 0.29032587200226084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 500] Loss: 0.29040877137074966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 501] Loss: 0.2904717923058607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 502] Loss: 0.29043400041854817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 503] Loss: 0.29037892113168584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 504] Loss: 0.2904185691642255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 505] Loss: 0.29032301008333394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 506] Loss: 0.2902560880313915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 507] Loss: 0.2901831205638395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 508] Loss: 0.290131136514173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 509] Loss: 0.29028224287404986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 510] Loss: 0.290283119271101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 511] Loss: 0.2902422204525643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 512] Loss: 0.29021617890826945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 513] Loss: 0.2902915366965154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 514] Loss: 0.2902050006698777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 515] Loss: 0.29017016461132783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 516] Loss: 0.29016747316510244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 517] Loss: 0.2901302711653194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 518] Loss: 0.2901220427491539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 519] Loss: 0.2901247517736067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 520] Loss: 0.2901224040627187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 521] Loss: 0.29020481829708084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 522] Loss: 0.29020193357469404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 523] Loss: 0.2902132543866038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 524] Loss: 0.29022583859351164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 525] Loss: 0.29037591555226405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 526] Loss: 0.29042024333266686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 527] Loss: 0.29045154885527674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 528] Loss: 0.2904743537037575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 529] Loss: 0.2904119441070284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 530] Loss: 0.29047018273211106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 531] Loss: 0.29052829099814526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 532] Loss: 0.2905734341707785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 533] Loss: 0.290631201109158\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 534] Loss: 0.2906129245538384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 535] Loss: 0.2906534566895225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 536] Loss: 0.29056564173824245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 537] Loss: 0.290568836250668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 538] Loss: 0.290549097211617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 539] Loss: 0.2905490197179791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 540] Loss: 0.29064513547255577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 541] Loss: 0.29056823478125066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 542] Loss: 0.29048873454144464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 543] Loss: 0.29041232158923347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 544] Loss: 0.29039277579245526\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 545] Loss: 0.2903143714797699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 546] Loss: 0.29038299407020196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 547] Loss: 0.29043671556887674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 548] Loss: 0.29039284867564186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 549] Loss: 0.29034444589281266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 550] Loss: 0.2902576092828587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 551] Loss: 0.29033818916169873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 552] Loss: 0.2903808970256029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 553] Loss: 0.2903277027976438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 554] Loss: 0.29024234385720904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 555] Loss: 0.290226429015039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 556] Loss: 0.29016945768427643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 557] Loss: 0.2901298832516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 558] Loss: 0.2901142438527652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 559] Loss: 0.29013160720087694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 560] Loss: 0.29006848765465504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 561] Loss: 0.2899956420379038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 562] Loss: 0.2900240029119793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 563] Loss: 0.2900728168313857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 564] Loss: 0.2900169395203497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 565] Loss: 0.2899692328392173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 566] Loss: 0.289982816041916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 567] Loss: 0.2899478431068143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 568] Loss: 0.2898738625712721\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 569] Loss: 0.2898571078624093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 570] Loss: 0.2900027766146443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 571] Loss: 0.2900246989030065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 572] Loss: 0.2900872867853589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 573] Loss: 0.2899948500605844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 574] Loss: 0.2899509610247077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 575] Loss: 0.2899764435636969\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 576] Loss: 0.2901152472992833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 577] Loss: 0.2900876510190179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 578] Loss: 0.2900291876716484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 579] Loss: 0.2900921289603616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 580] Loss: 0.2900048048545917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 581] Loss: 0.29002417649884044\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 582] Loss: 0.29007577269092616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 583] Loss: 0.29012865346426403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 584] Loss: 0.2900726961807663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 585] Loss: 0.2900282139485069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 586] Loss: 0.29012824664668935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 587] Loss: 0.2901238444529555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 588] Loss: 0.2900527426460493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 589] Loss: 0.289970126976301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 590] Loss: 0.28991062514377997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 591] Loss: 0.2899140241564252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 592] Loss: 0.289894725270146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 593] Loss: 0.2898146830855305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 594] Loss: 0.2898659970868615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 595] Loss: 0.28987817589554665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 596] Loss: 0.289806044704522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 597] Loss: 0.2897218581241541\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 598] Loss: 0.28968398721855243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 599] Loss: 0.28968594239083256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 600] Loss: 0.2897134261732956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 601] Loss: 0.289951623961477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 602] Loss: 0.2899033492896267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 603] Loss: 0.2899702036024207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 604] Loss: 0.29006728436622464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 605] Loss: 0.2900408882183187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 606] Loss: 0.29002280065812597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 607] Loss: 0.2899650229175269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 608] Loss: 0.28991004104088797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 609] Loss: 0.28993401665567736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 610] Loss: 0.2900376607764495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 611] Loss: 0.29008966415127585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 612] Loss: 0.290079762932004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 613] Loss: 0.2899898315609144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 614] Loss: 0.29006779320614856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 615] Loss: 0.29015216233788943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 616] Loss: 0.29017919731371744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 617] Loss: 0.2901205920348895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 618] Loss: 0.2900797649625588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 619] Loss: 0.29009598166502876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 620] Loss: 0.290085933284364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 621] Loss: 0.2900804436614668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 622] Loss: 0.2902036452305918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 623] Loss: 0.29018511517452295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 624] Loss: 0.2903177424215837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 625] Loss: 0.29029795109600454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 626] Loss: 0.29023356102788883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 627] Loss: 0.29019456630217844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 628] Loss: 0.2901663249479936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 629] Loss: 0.2900915821002716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 630] Loss: 0.29001534510837046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 631] Loss: 0.2899474501090236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 632] Loss: 0.2898756069716302\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 633] Loss: 0.28984157259014115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 634] Loss: 0.28986959169405196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 635] Loss: 0.289853751856462\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 636] Loss: 0.2898225709001131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 637] Loss: 0.28975919313559556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 638] Loss: 0.28968862380290383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 639] Loss: 0.28966672516644915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 640] Loss: 0.28978717922129565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 641] Loss: 0.28976798138578197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 642] Loss: 0.2897464541630487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 643] Loss: 0.2896961010975304\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 644] Loss: 0.28966724720065673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 645] Loss: 0.2896384749312048\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 646] Loss: 0.2896019137074908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 647] Loss: 0.28961592734918984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 648] Loss: 0.2896297500465354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 649] Loss: 0.2895673437120454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 650] Loss: 0.28950928732203446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 651] Loss: 0.2894798644652349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 652] Loss: 0.2895549372641256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 653] Loss: 0.2895357664580231\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 654] Loss: 0.2895626835182323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 655] Loss: 0.28954653734105756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 656] Loss: 0.2896316146498601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 657] Loss: 0.28971836126598133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 658] Loss: 0.28979844954702916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 659] Loss: 0.2897663406716568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 660] Loss: 0.2897886444976844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 661] Loss: 0.28976325803137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 662] Loss: 0.28972947330826143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 663] Loss: 0.289719274089225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 664] Loss: 0.28965437837668284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 665] Loss: 0.28967867832912186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 666] Loss: 0.28975697721662264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 667] Loss: 0.289713084844214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 668] Loss: 0.2896517957279789\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 669] Loss: 0.28961518408569803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 670] Loss: 0.28957285909960256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 671] Loss: 0.2896010478688995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 672] Loss: 0.28952246696355133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 673] Loss: 0.28963046446544527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 674] Loss: 0.28963131966271394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 675] Loss: 0.2896309672699974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 676] Loss: 0.28957886704818797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 677] Loss: 0.2895026409360133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 678] Loss: 0.28950962668337316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 679] Loss: 0.2895425515670351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 680] Loss: 0.2895789491723884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 681] Loss: 0.2895722157274902\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 682] Loss: 0.2894912164160926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 683] Loss: 0.2895280689785711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 684] Loss: 0.2895929885930137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 685] Loss: 0.2895114009883128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 686] Loss: 0.28949843330861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 687] Loss: 0.28956445180618984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 688] Loss: 0.289549607594592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 689] Loss: 0.2895369898172027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 690] Loss: 0.28953472494539634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 691] Loss: 0.28945988490806884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 692] Loss: 0.2894762102480823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 693] Loss: 0.28947340048280606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 694] Loss: 0.28942771420865415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 695] Loss: 0.2893675068498465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 696] Loss: 0.2893389665289691\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 697] Loss: 0.28947493995854484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 698] Loss: 0.2895835381810788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 699] Loss: 0.28949604132063705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 700] Loss: 0.2895509641114119\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 701] Loss: 0.2895252759971365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 702] Loss: 0.28958862602147406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 703] Loss: 0.2896233147468537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 704] Loss: 0.28955999986945297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 705] Loss: 0.2897296323669091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 706] Loss: 0.28965395816868744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 707] Loss: 0.289632795039827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 708] Loss: 0.28969601708292103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 709] Loss: 0.2897545690023616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 710] Loss: 0.28968705421338703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 711] Loss: 0.28962726050137083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 712] Loss: 0.2895985332628091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 713] Loss: 0.28959527806588686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 714] Loss: 0.28959485200119534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 715] Loss: 0.28957320937107606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 716] Loss: 0.28958446448518893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 717] Loss: 0.2895292829643897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 718] Loss: 0.28948929947172564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 719] Loss: 0.2895441681159932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 720] Loss: 0.28958731510543395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 721] Loss: 0.289523938832832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 722] Loss: 0.28948741673386386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 723] Loss: 0.2894371944947589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 724] Loss: 0.2894346454046308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 725] Loss: 0.2893827378285223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 726] Loss: 0.28948561765605174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 727] Loss: 0.2894395701040095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 728] Loss: 0.2894326167850626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 729] Loss: 0.28944965524507027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 730] Loss: 0.28949810928106307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 731] Loss: 0.289503295380028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 732] Loss: 0.28943718863290535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 733] Loss: 0.2893898464802155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 734] Loss: 0.28940459595994816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 735] Loss: 0.28933847573670474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 736] Loss: 0.2893889454120757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 737] Loss: 0.2893675614113717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 738] Loss: 0.28932460846232566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 739] Loss: 0.28931126251137324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 740] Loss: 0.2893213038753092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 741] Loss: 0.28928896691856104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 742] Loss: 0.2892911954923713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 743] Loss: 0.2892618555241436\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 744] Loss: 0.2893213882397321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 745] Loss: 0.28925746847067446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 746] Loss: 0.2891791113122101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 747] Loss: 0.2891492932430174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 748] Loss: 0.28917130798337953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 749] Loss: 0.28926804029321607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 750] Loss: 0.289296503942706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 751] Loss: 0.28927007635119517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 752] Loss: 0.2892204605593858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 753] Loss: 0.28917593699402605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 754] Loss: 0.28917309938681784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 755] Loss: 0.2892506910483916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 756] Loss: 0.2893262921467502\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 757] Loss: 0.28925395910421586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 758] Loss: 0.2891819286597623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 759] Loss: 0.28920334870483644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 760] Loss: 0.2893544701369185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 761] Loss: 0.2892921668024556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 762] Loss: 0.289219700224456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 763] Loss: 0.28941647551115074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 764] Loss: 0.28936941834287344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 765] Loss: 0.28931095729249834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 766] Loss: 0.28930288042191965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 767] Loss: 0.289258082242944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 768] Loss: 0.28920779930154134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 769] Loss: 0.28921234991129646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 770] Loss: 0.28916450235911334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 771] Loss: 0.2891178137282522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 772] Loss: 0.28904050777472856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 773] Loss: 0.2891044216551377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 774] Loss: 0.28909798790461205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 775] Loss: 0.28908188949807795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 776] Loss: 0.2890202051859336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 777] Loss: 0.2890095736216762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 778] Loss: 0.2889239266255402\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 779] Loss: 0.2888758672663003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 780] Loss: 0.2888271098745906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 781] Loss: 0.2888713137824242\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 782] Loss: 0.28882591427019844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 783] Loss: 0.2888934959795389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 784] Loss: 0.2888536454605249\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 785] Loss: 0.28879876396014736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 786] Loss: 0.288771222679127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 787] Loss: 0.28880440371735144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 788] Loss: 0.2889864033788598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 789] Loss: 0.2890659317909871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 790] Loss: 0.28903642042632743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 791] Loss: 0.28897267215079636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 792] Loss: 0.2890272121554221\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 793] Loss: 0.2889731737683977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 794] Loss: 0.2889848828264218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 795] Loss: 0.28897174604510645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 796] Loss: 0.2889044704575201\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 797] Loss: 0.2889315797483093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 798] Loss: 0.2888719595846583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 799] Loss: 0.28887207225871114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 800] Loss: 0.2889166747316204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 801] Loss: 0.2888507487855046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 802] Loss: 0.2888505264256762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 803] Loss: 0.2887953421549196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 804] Loss: 0.2887745436197612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 805] Loss: 0.2887423042007672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 806] Loss: 0.28872792818043014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 807] Loss: 0.2886745868883883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 808] Loss: 0.28864891174220547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 809] Loss: 0.28869210875254425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 810] Loss: 0.2886834823906166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 811] Loss: 0.28867682170223546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 812] Loss: 0.28859540958462304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 813] Loss: 0.28863564013885523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 814] Loss: 0.2886033715534997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 815] Loss: 0.2886089484779687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 816] Loss: 0.2885776073639983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 817] Loss: 0.2886420498081237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 818] Loss: 0.28869041308786136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 819] Loss: 0.2886829651850775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 820] Loss: 0.28867348506728296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 821] Loss: 0.2886523840577365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 822] Loss: 0.2887461019169307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 823] Loss: 0.28876900069043515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 824] Loss: 0.28887202238835674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 825] Loss: 0.28885333429076776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 826] Loss: 0.288822956792915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 827] Loss: 0.2887883923493856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 828] Loss: 0.28879969303370845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 829] Loss: 0.28884761479178506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 830] Loss: 0.28886029879146435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 831] Loss: 0.28891842017486624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 832] Loss: 0.2888909142850867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 833] Loss: 0.288904399670822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 834] Loss: 0.2888950039248853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 835] Loss: 0.28903862376185235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 836] Loss: 0.289051002467048\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 837] Loss: 0.2890795761522571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 838] Loss: 0.28909080645876783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 839] Loss: 0.28914132196615694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 840] Loss: 0.2890631232852653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 841] Loss: 0.28898996454914055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 842] Loss: 0.2890500356328881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 843] Loss: 0.28896986997410046\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 844] Loss: 0.28894313034647207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 845] Loss: 0.28898476567001224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 846] Loss: 0.2889211963884224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 847] Loss: 0.2888646275119838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 848] Loss: 0.28887610857349794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 849] Loss: 0.28884112500299025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 850] Loss: 0.28884741306807926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 851] Loss: 0.288811826721016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 852] Loss: 0.2889688027297063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 853] Loss: 0.28894397609479006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 854] Loss: 0.28893624309516297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 855] Loss: 0.2888606861265082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 856] Loss: 0.28879845191282455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 857] Loss: 0.2887515068167039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 858] Loss: 0.28875057622978323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 859] Loss: 0.28881402293665664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 860] Loss: 0.28880029593070017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 861] Loss: 0.2887467464470453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 862] Loss: 0.28871728663155255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 863] Loss: 0.2888411587581783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 864] Loss: 0.2887911338096597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 865] Loss: 0.28889648782507177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 866] Loss: 0.2889090706434254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 867] Loss: 0.28897986626919714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 868] Loss: 0.28891941651676967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 869] Loss: 0.2889404808965973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 870] Loss: 0.28893518269560825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 871] Loss: 0.2888926222004286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 872] Loss: 0.28885490786172074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 873] Loss: 0.28877247690008717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 874] Loss: 0.2888023031933067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 875] Loss: 0.2888157034841111\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 876] Loss: 0.28877992464077096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 877] Loss: 0.2889204560019446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 878] Loss: 0.2889644499243648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 879] Loss: 0.28901845470176435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 880] Loss: 0.2889706354464094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 881] Loss: 0.28904757052597924\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 882] Loss: 0.2890469666995375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 883] Loss: 0.28926399169095196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 884] Loss: 0.28924307675905114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 885] Loss: 0.2892111868223876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 886] Loss: 0.2892362382247462\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 887] Loss: 0.2891840303914895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 888] Loss: 0.28920060454363444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 889] Loss: 0.28934591025076944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 890] Loss: 0.2894402850094673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 891] Loss: 0.2894205901445763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 892] Loss: 0.2894110576825751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 893] Loss: 0.2895614639437154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 894] Loss: 0.28956579278102074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 895] Loss: 0.2895210144929511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 896] Loss: 0.2895577060712489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 897] Loss: 0.28955009070417337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 898] Loss: 0.2897757129594252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 899] Loss: 0.28978792840708084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 900] Loss: 0.28978557674106487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 901] Loss: 0.2898142606155383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 902] Loss: 0.2898450735233521\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 903] Loss: 0.289912793171883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 904] Loss: 0.2898997749530836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 905] Loss: 0.289882077507137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 906] Loss: 0.2898602846331176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 907] Loss: 0.2899539206663571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 908] Loss: 0.2899328288138895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 909] Loss: 0.2898747684927132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 910] Loss: 0.2898230372142399\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 911] Loss: 0.28993920321721495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 912] Loss: 0.28992063305218163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 913] Loss: 0.2899118890144479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 914] Loss: 0.28986647362065493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 915] Loss: 0.2899206321158693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 916] Loss: 0.28992841291823995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 917] Loss: 0.29000605228228993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 918] Loss: 0.28999104242010915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 919] Loss: 0.29004866084562747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 920] Loss: 0.2901181854613003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 921] Loss: 0.29006537806665444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 922] Loss: 0.29009011600785917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 923] Loss: 0.29017291974079595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 924] Loss: 0.2901656878670089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 925] Loss: 0.29013567990442485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 926] Loss: 0.2901274939528373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 927] Loss: 0.29015333261181003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 928] Loss: 0.29007404293024946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 929] Loss: 0.2900286859160737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 930] Loss: 0.2899681616468089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 931] Loss: 0.28992533806889265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 932] Loss: 0.2898497670329803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 933] Loss: 0.2898031495393076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 934] Loss: 0.2899060402469316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 935] Loss: 0.2899055496738302\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 936] Loss: 0.2899559035606027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 937] Loss: 0.2899579744441848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 938] Loss: 0.28995790293150747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 939] Loss: 0.28992156878919706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 940] Loss: 0.2899243378905746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 941] Loss: 0.2898691952083228\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 942] Loss: 0.2898165417046783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 943] Loss: 0.28981098028106317\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 944] Loss: 0.28972251694528506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 945] Loss: 0.2897033649090579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 946] Loss: 0.2897591409118603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 947] Loss: 0.28973706204928895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 948] Loss: 0.28976860483036626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 949] Loss: 0.2897851730030333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 950] Loss: 0.28973579814559536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 951] Loss: 0.28967210989353487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 952] Loss: 0.2896567868207458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 953] Loss: 0.2896268010995092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 954] Loss: 0.28957259147251047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 955] Loss: 0.28952080678458164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 956] Loss: 0.28943483504092077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 957] Loss: 0.28944446354334424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 958] Loss: 0.289385362837202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 959] Loss: 0.289349797414334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 960] Loss: 0.28944377122567067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 961] Loss: 0.28942690137736216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 962] Loss: 0.28939202338709263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 963] Loss: 0.28949820454916164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 964] Loss: 0.2895055348188549\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 965] Loss: 0.2894561847420765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 966] Loss: 0.28938720029743564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 967] Loss: 0.28934856455070596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 968] Loss: 0.2893042877178723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 969] Loss: 0.289318668828546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 970] Loss: 0.28939839997983363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 971] Loss: 0.2894256494726966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 972] Loss: 0.2893653288812498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 973] Loss: 0.28946600333483785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 974] Loss: 0.2894734825069606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 975] Loss: 0.2894519068703862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 976] Loss: 0.28940131975254285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 977] Loss: 0.28932742456710475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 978] Loss: 0.2893296906245862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 979] Loss: 0.2893576488113489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 980] Loss: 0.28931953791081905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 981] Loss: 0.28924319765916684\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8963\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 982] Loss: 0.28930547810215457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 983] Loss: 0.28937995654675563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 984] Loss: 0.2893544041013089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 985] Loss: 0.28945712350204794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 986] Loss: 0.28948892363408807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 987] Loss: 0.2895617954311116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 988] Loss: 0.28957515903280684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 989] Loss: 0.289560745898211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 990] Loss: 0.28951866084717187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 991] Loss: 0.2894554482910178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 992] Loss: 0.2894471967474196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 993] Loss: 0.28941736342752306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 994] Loss: 0.2893816406528155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 995] Loss: 0.2894157052336346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 996] Loss: 0.28944449115467563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 997] Loss: 0.2894897288600272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 998] Loss: 0.2894606831778696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 999] Loss: 0.28948497499511944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1000] Loss: 0.2894288028812125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1001] Loss: 0.28950133453352683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1002] Loss: 0.28957705675350304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1003] Loss: 0.2896151552881176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1004] Loss: 0.2895574872153213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1005] Loss: 0.28950561254921525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1006] Loss: 0.28947385019450933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1007] Loss: 0.2895367384180987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1008] Loss: 0.2895356118384324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1009] Loss: 0.2895533030697271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1010] Loss: 0.28952837048665336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1011] Loss: 0.28951055062283454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1012] Loss: 0.2894821999915309\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1013] Loss: 0.2894123388969799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1014] Loss: 0.2894346908193549\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1015] Loss: 0.2893669989746233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1016] Loss: 0.2893453233989913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1017] Loss: 0.28937762903448255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1018] Loss: 0.28935544691844844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1019] Loss: 0.28929556485422486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1020] Loss: 0.28926458656787873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1021] Loss: 0.2892147746671611\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1022] Loss: 0.28926415079039536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1023] Loss: 0.28924439594917206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1024] Loss: 0.28926479176811454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1025] Loss: 0.2892734086349577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1026] Loss: 0.2893481510577947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1027] Loss: 0.2893082047847378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1028] Loss: 0.2893051482631797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1029] Loss: 0.2894236595315436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1030] Loss: 0.28944477477494407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1031] Loss: 0.28950405761056297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1032] Loss: 0.2894627788924498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1033] Loss: 0.28938804966675924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1034] Loss: 0.289462657217972\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1035] Loss: 0.28948631959652016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1036] Loss: 0.2894848355081906\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1037] Loss: 0.2894865097386976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1038] Loss: 0.2894578573466559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1039] Loss: 0.28951461650445715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1040] Loss: 0.289460292912554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1041] Loss: 0.2894474191388224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1042] Loss: 0.28939406093352477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1043] Loss: 0.2893742300945011\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1044] Loss: 0.2894047138494746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1045] Loss: 0.28934804894189853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1046] Loss: 0.28928990721330916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1047] Loss: 0.2892353992007983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1048] Loss: 0.28925813594107685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1049] Loss: 0.2892366972773304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1050] Loss: 0.2891739925289896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1051] Loss: 0.28929776569972565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1052] Loss: 0.2892987213994074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1053] Loss: 0.2892831174191268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1054] Loss: 0.2892904890630258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1055] Loss: 0.2893622293518585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1056] Loss: 0.2894362187279122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1057] Loss: 0.28937708854467126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1058] Loss: 0.28935444430730617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1059] Loss: 0.28934173423474774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1060] Loss: 0.289392168987398\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1061] Loss: 0.2893681955148157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1062] Loss: 0.28938147582627377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1063] Loss: 0.28940658849936973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1064] Loss: 0.28939891830317377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1065] Loss: 0.2893312490856855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1066] Loss: 0.28938094844893253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1067] Loss: 0.28940689312326523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1068] Loss: 0.2895748869730433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1069] Loss: 0.28954934458294723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1070] Loss: 0.2896388443884472\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1071] Loss: 0.28959734931973496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1072] Loss: 0.2896307743884577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1073] Loss: 0.2896678768647163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1074] Loss: 0.2896179583644582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1075] Loss: 0.2895964451177272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1076] Loss: 0.28963573552805855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1077] Loss: 0.28978493058330335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1078] Loss: 0.2897235939424023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1079] Loss: 0.28968980961428004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1080] Loss: 0.28965499877643125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1081] Loss: 0.28970578286233933\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1082] Loss: 0.28970563862698984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1083] Loss: 0.2899373219023976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1084] Loss: 0.28995887678511406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1085] Loss: 0.29011488701869337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1086] Loss: 0.2900559720100802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1087] Loss: 0.29005268965763836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1088] Loss: 0.2901231426305703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1089] Loss: 0.29008978861999857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1090] Loss: 0.29007998730887397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1091] Loss: 0.2900639919571975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1092] Loss: 0.2900474121848301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1093] Loss: 0.2900149188636365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1094] Loss: 0.28995737691382095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1095] Loss: 0.28991953757080474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1096] Loss: 0.289879147387722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1097] Loss: 0.28995412836661993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1098] Loss: 0.28994848628971664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1099] Loss: 0.28993269277977646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1100] Loss: 0.29001121041258326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1101] Loss: 0.2900026139782977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1102] Loss: 0.2899643107367826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1103] Loss: 0.2899459309441372\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1104] Loss: 0.2898701513271279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1105] Loss: 0.289870610949539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1106] Loss: 0.2898977858022207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1107] Loss: 0.2898876925878621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1108] Loss: 0.2898734867816601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1109] Loss: 0.2899042012976938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1110] Loss: 0.28986530154660173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1111] Loss: 0.28985331893386335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1112] Loss: 0.2897833216069181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1113] Loss: 0.2897871452034584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1114] Loss: 0.2898652317114146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1115] Loss: 0.289842740360089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1116] Loss: 0.2897882954004095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1117] Loss: 0.2897940717431285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1118] Loss: 0.28989705804613974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1119] Loss: 0.2898644766642904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1120] Loss: 0.2898808963568599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1121] Loss: 0.2898220420419741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1122] Loss: 0.2897577960182328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1123] Loss: 0.2897578757813019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1124] Loss: 0.28979449297347964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1125] Loss: 0.28976369592834733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1126] Loss: 0.2898059078856165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1127] Loss: 0.28976956864697195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1128] Loss: 0.2897486677505864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1129] Loss: 0.2897628777097558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1130] Loss: 0.28975379757442565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1131] Loss: 0.28976832730158103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1132] Loss: 0.2897581223364734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1133] Loss: 0.28976566283839\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1134] Loss: 0.28976760755525377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1135] Loss: 0.2897446295102662\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1136] Loss: 0.2898361300179982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1137] Loss: 0.2898905621291239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1138] Loss: 0.28990767241656734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1139] Loss: 0.2898867741092653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1140] Loss: 0.2898465068688742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1141] Loss: 0.28988733093335817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1142] Loss: 0.28995091638546705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1143] Loss: 0.2899700349243655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1144] Loss: 0.2899383743497747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1145] Loss: 0.2899380427839385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1146] Loss: 0.2899623234230195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1147] Loss: 0.29012108855103125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1148] Loss: 0.290092373378519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1149] Loss: 0.2903293261521539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1150] Loss: 0.2903023098543119\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1151] Loss: 0.29026108779618465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1152] Loss: 0.29019406251888163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1153] Loss: 0.29014712508824986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1154] Loss: 0.29020620394629065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1155] Loss: 0.2901465385901594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1156] Loss: 0.2901826709625833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1157] Loss: 0.29032408067004023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1158] Loss: 0.2903046183221326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1159] Loss: 0.2903156084644345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1160] Loss: 0.2903307862718826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1161] Loss: 0.29030309205389165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1162] Loss: 0.29024630111880395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1163] Loss: 0.29022066946343794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1164] Loss: 0.2902933369000649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1165] Loss: 0.29024476730568227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1166] Loss: 0.29024373895571204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1167] Loss: 0.2902419352352863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1168] Loss: 0.2902770577902196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1169] Loss: 0.29025216668556064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1170] Loss: 0.2902534232512947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1171] Loss: 0.29023534503901416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1172] Loss: 0.2902820048906341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1173] Loss: 0.29024414424609835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1174] Loss: 0.2901961837481063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1175] Loss: 0.2901362026528436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1176] Loss: 0.2901198163225097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1177] Loss: 0.290055912608211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1178] Loss: 0.29008455126593957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1179] Loss: 0.2900934960222456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1180] Loss: 0.2901263230542342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1181] Loss: 0.29006924922379773\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1182] Loss: 0.290123861296851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1183] Loss: 0.290153176325628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1184] Loss: 0.29015596424223933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1185] Loss: 0.2900884512942732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1186] Loss: 0.29006907181967123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1187] Loss: 0.2900617249588003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1188] Loss: 0.2900771014117244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1189] Loss: 0.29007417183601797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1190] Loss: 0.2900275805669517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1191] Loss: 0.2899932180744658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1192] Loss: 0.2899792063596678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1193] Loss: 0.2899849581687687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1194] Loss: 0.28994243664498176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1195] Loss: 0.2899115793667665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1196] Loss: 0.28986563584019753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1197] Loss: 0.28997370330867056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1198] Loss: 0.28991091376371997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1199] Loss: 0.2898440798028684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1200] Loss: 0.2899002383627436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1201] Loss: 0.2898853903568566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1202] Loss: 0.2899213370639204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1203] Loss: 0.2898845298080991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1204] Loss: 0.2899444650492129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1205] Loss: 0.28990069603154417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1206] Loss: 0.28989452221488915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1207] Loss: 0.28988781463741425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1208] Loss: 0.28983516375536966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1209] Loss: 0.28987049034835216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1210] Loss: 0.28984693194374495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1211] Loss: 0.289952373878941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1212] Loss: 0.2899565189847747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1213] Loss: 0.28991652437391363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1214] Loss: 0.29000594110474065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1215] Loss: 0.2900120222317889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1216] Loss: 0.2900093771920305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1217] Loss: 0.2901444986794698\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1218] Loss: 0.29018566409130003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1219] Loss: 0.290142319653103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1220] Loss: 0.2901119050427075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1221] Loss: 0.29014487029078756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1222] Loss: 0.29015296848693267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1223] Loss: 0.2901465481447991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1224] Loss: 0.2900993630246588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1225] Loss: 0.2900817738693269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1226] Loss: 0.2900417560341145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1227] Loss: 0.2899873279195115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1228] Loss: 0.28998710375056863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1229] Loss: 0.28998034944769335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1230] Loss: 0.28997318426587365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1231] Loss: 0.2899674896336347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1232] Loss: 0.28993698067118434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1233] Loss: 0.28996246425814515\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1234] Loss: 0.2899149365685258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1235] Loss: 0.2900027681446335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1236] Loss: 0.2900910355280463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1237] Loss: 0.2900455553256216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1238] Loss: 0.28997641597604995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1239] Loss: 0.2899792495846273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1240] Loss: 0.2899352525347385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1241] Loss: 0.2898939272948082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1242] Loss: 0.28989251289222656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1243] Loss: 0.2898317976351497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1244] Loss: 0.28975950999053357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1245] Loss: 0.28977624033771415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1246] Loss: 0.28979281089063236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1247] Loss: 0.2897307296112896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1248] Loss: 0.2896612732391266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1249] Loss: 0.28977472565963236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1250] Loss: 0.28978388902028546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1251] Loss: 0.28975090506822354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1252] Loss: 0.2897150637541627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1253] Loss: 0.2896665027731169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1254] Loss: 0.28969226768690215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1255] Loss: 0.2897564932220691\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1256] Loss: 0.28974804718863295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1257] Loss: 0.28976279958347473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1258] Loss: 0.2897687612907375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1259] Loss: 0.2897069455908974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1260] Loss: 0.28971562745759816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1261] Loss: 0.28972610645866875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1262] Loss: 0.28974506357520025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1263] Loss: 0.2897241743956801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1264] Loss: 0.289671554422841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1265] Loss: 0.28960232280422155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1266] Loss: 0.28962299174763534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1267] Loss: 0.289556177504759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1268] Loss: 0.2895718191802459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1269] Loss: 0.2895096359970152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1270] Loss: 0.289470141629378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1271] Loss: 0.2894438799939161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1272] Loss: 0.28957427214606957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1273] Loss: 0.2895416010196517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1274] Loss: 0.28961767203460526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1275] Loss: 0.2895759162461395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1276] Loss: 0.28956210328124793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1277] Loss: 0.2895077498343999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1278] Loss: 0.2894782880008263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1279] Loss: 0.2894150868361658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1280] Loss: 0.2894439165187733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1281] Loss: 0.2894353828805551\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1282] Loss: 0.28938109230391046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1283] Loss: 0.28932080090130985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1284] Loss: 0.2893360782356218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1285] Loss: 0.2892870036622845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1286] Loss: 0.28938092192049464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1287] Loss: 0.2893536378309996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1288] Loss: 0.2893800857686504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1289] Loss: 0.2893333572824046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1290] Loss: 0.2892881889254173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1291] Loss: 0.28931180475997315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1292] Loss: 0.2893023309993371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1293] Loss: 0.2893318078863718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1294] Loss: 0.2892834070140619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1295] Loss: 0.28927300409355977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1296] Loss: 0.2892773907885633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1297] Loss: 0.28924261290086767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1298] Loss: 0.28922065362767857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1299] Loss: 0.289241250936086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1300] Loss: 0.289169579163088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1301] Loss: 0.28923927599848775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1302] Loss: 0.28923373412232634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1303] Loss: 0.2893112420803513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1304] Loss: 0.28926685903084176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1305] Loss: 0.2892726674016598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1306] Loss: 0.2893254312212722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1307] Loss: 0.2893170278149103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1308] Loss: 0.2892651704404782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1309] Loss: 0.289256967443508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1310] Loss: 0.28922119779321415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1311] Loss: 0.2892301537330729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1312] Loss: 0.2892394922515851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1313] Loss: 0.28918943037382505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1314] Loss: 0.28912471846829185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1315] Loss: 0.2891043247737161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1316] Loss: 0.2890771035320416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1317] Loss: 0.28910627449046233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1318] Loss: 0.2890811134037877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1319] Loss: 0.2891180147825307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1320] Loss: 0.28915000044229167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1321] Loss: 0.2891547576731169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1322] Loss: 0.28917137726375874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1323] Loss: 0.2891857526407584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1324] Loss: 0.28918600426633145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1325] Loss: 0.28913304943506246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1326] Loss: 0.28909700913112596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1327] Loss: 0.28906703631229724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1328] Loss: 0.289039643068034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1329] Loss: 0.28905508415391046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1330] Loss: 0.2890018470663773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1331] Loss: 0.28894744525454163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1332] Loss: 0.2890790829965309\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1333] Loss: 0.2890990874013523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1334] Loss: 0.28919903917294243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1335] Loss: 0.2891425248411616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1336] Loss: 0.289170171585487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1337] Loss: 0.28919402189425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1338] Loss: 0.28913660271339603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1339] Loss: 0.2890798218519632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1340] Loss: 0.2890442109660252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1341] Loss: 0.28898317036627474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1342] Loss: 0.28893939683449427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1343] Loss: 0.2889381512853802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1344] Loss: 0.2889457767720038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1345] Loss: 0.2889353619430078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1346] Loss: 0.28889705195232696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1347] Loss: 0.2888729574816918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1348] Loss: 0.28895657315507345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1349] Loss: 0.2889241574662577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1350] Loss: 0.28892868035813657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1351] Loss: 0.2888786833161097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1352] Loss: 0.28882677777615845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1353] Loss: 0.2889406153192472\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1354] Loss: 0.28902362316338737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1355] Loss: 0.2891786302405855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1356] Loss: 0.289149658459139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1357] Loss: 0.28909539795963973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1358] Loss: 0.2891149454678281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1359] Loss: 0.2891169860648504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1360] Loss: 0.2890791677843986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1361] Loss: 0.28907376041339117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1362] Loss: 0.28910145774421636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1363] Loss: 0.2890731366096046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1364] Loss: 0.28914859645254654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1365] Loss: 0.2891198698863826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1366] Loss: 0.2890658485506321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1367] Loss: 0.28909289056727494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1368] Loss: 0.28911111947830115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1369] Loss: 0.2891967168710313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1370] Loss: 0.2891426660945465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1371] Loss: 0.2891276107423861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1372] Loss: 0.28909173518944825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1373] Loss: 0.2890984562469979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1374] Loss: 0.28911038960054075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1375] Loss: 0.28915937734134045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1376] Loss: 0.28910527682158255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1377] Loss: 0.28910409344870097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1378] Loss: 0.2890591865612401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1379] Loss: 0.2890829021697745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1380] Loss: 0.28905082609376004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1381] Loss: 0.2890711008541874\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1382] Loss: 0.2890371733995284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1383] Loss: 0.28902542316221674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1384] Loss: 0.288986742334856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1385] Loss: 0.2889938351234963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1386] Loss: 0.28906643266821025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1387] Loss: 0.28919758667158924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1388] Loss: 0.28916474135830855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1389] Loss: 0.28911719199727187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1390] Loss: 0.289230549983245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1391] Loss: 0.28945079188101563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1392] Loss: 0.2894081935813589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1393] Loss: 0.289385258125821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1394] Loss: 0.28945208534434713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1395] Loss: 0.2894203876728259\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1396] Loss: 0.28944390874984577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1397] Loss: 0.28943307735830454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1398] Loss: 0.28942511948955396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1399] Loss: 0.28947033592745147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1400] Loss: 0.2895669043038602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1401] Loss: 0.2895045996819282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1402] Loss: 0.28945645858507546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1403] Loss: 0.2894367853098645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1404] Loss: 0.2894293737971057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1405] Loss: 0.28944209086844047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1406] Loss: 0.2895163267937451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1407] Loss: 0.2894709009484989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1408] Loss: 0.2894511930059255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1409] Loss: 0.2894724303873662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1410] Loss: 0.28943237783609804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1411] Loss: 0.28950137395382497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1412] Loss: 0.28950611203349874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1413] Loss: 0.28949159985053186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1414] Loss: 0.28945456381986234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1415] Loss: 0.2894274233172456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1416] Loss: 0.28944722325756783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1417] Loss: 0.28954671626446965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1418] Loss: 0.28955445098554383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1419] Loss: 0.2895219412946953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1420] Loss: 0.2894890186104442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1421] Loss: 0.289520283530801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1422] Loss: 0.2894542397213258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1423] Loss: 0.28944031092187317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1424] Loss: 0.28945317607798166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1425] Loss: 0.2894584863264816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1426] Loss: 0.2894587665846861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1427] Loss: 0.28941808495333043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1428] Loss: 0.2894470332850029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1429] Loss: 0.28949481605088845\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1430] Loss: 0.289448874067452\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1431] Loss: 0.28945676926353026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1432] Loss: 0.2894271581224229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1433] Loss: 0.28943165820350575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1434] Loss: 0.2894723063830435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1435] Loss: 0.2894506854403846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1436] Loss: 0.2893805554255097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1437] Loss: 0.2893345057661968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1438] Loss: 0.2892975342221321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1439] Loss: 0.28928246865525525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1440] Loss: 0.28928142360134707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1441] Loss: 0.28921794770578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1442] Loss: 0.2892173449084374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1443] Loss: 0.28917826823274145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1444] Loss: 0.28915870122299825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1445] Loss: 0.2892518569104386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1446] Loss: 0.28922486616221943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1447] Loss: 0.28919909294803897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1448] Loss: 0.28920236784673603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1449] Loss: 0.2892067968072454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1450] Loss: 0.2891793896955272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1451] Loss: 0.2891973817601263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1452] Loss: 0.289189528949649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1453] Loss: 0.2891465803007465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1454] Loss: 0.28911013350081105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1455] Loss: 0.2892462771579999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1456] Loss: 0.2892232932030193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1457] Loss: 0.28925682080244514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1458] Loss: 0.28934306167491414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1459] Loss: 0.28931361519377996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1460] Loss: 0.2893176607887617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1461] Loss: 0.2893363464172556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1462] Loss: 0.28932278510370485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1463] Loss: 0.28928363648409655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1464] Loss: 0.289245929743785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1465] Loss: 0.28929438195136725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1466] Loss: 0.2893272290144762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1467] Loss: 0.28933038752355184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1468] Loss: 0.28931067169829705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1469] Loss: 0.28932092272744764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1470] Loss: 0.28940347610508715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1471] Loss: 0.2894447070425946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1472] Loss: 0.289430037449627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1473] Loss: 0.289426247242339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1474] Loss: 0.28962176656953953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1475] Loss: 0.28963133949469244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1476] Loss: 0.2896005353319629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1477] Loss: 0.2895710300188045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1478] Loss: 0.28963467132317533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1479] Loss: 0.28968852296814435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1480] Loss: 0.2896749303092559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1481] Loss: 0.28959661362067696\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8908\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1482] Loss: 0.2896040604697375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1483] Loss: 0.2896352993399451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1484] Loss: 0.2896608376389397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1485] Loss: 0.28967135492433327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1486] Loss: 0.289683093036155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1487] Loss: 0.2897142807785502\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1488] Loss: 0.28970435182836424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1489] Loss: 0.2896728092249286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1490] Loss: 0.28975803120761023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1491] Loss: 0.2898325076749241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1492] Loss: 0.2898487058588134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1493] Loss: 0.2898632840046624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1494] Loss: 0.28988176653830316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1495] Loss: 0.2898683215094542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1496] Loss: 0.2898281709888866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1497] Loss: 0.28988151867059947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1498] Loss: 0.2899839645813264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1499] Loss: 0.2899833792335014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1500] Loss: 0.28997152542313004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1501] Loss: 0.2899563898058354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1502] Loss: 0.2899432399209705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1503] Loss: 0.2900158065675289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1504] Loss: 0.28999674744366927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1505] Loss: 0.28993958575050693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1506] Loss: 0.28994511901440995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1507] Loss: 0.2899899815147983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1508] Loss: 0.2899848611341344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1509] Loss: 0.28999897818329823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1510] Loss: 0.28998206312285596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1511] Loss: 0.28995937218503054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1512] Loss: 0.28997745488138776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1513] Loss: 0.2899355604824574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1514] Loss: 0.2899411052561211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1515] Loss: 0.28995538370981444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1516] Loss: 0.2899419312869666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1517] Loss: 0.2899268260007926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1518] Loss: 0.28989314084223927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1519] Loss: 0.28989423123378627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1520] Loss: 0.2898521839971899\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1521] Loss: 0.2898055897135654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1522] Loss: 0.2898178606880076\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1523] Loss: 0.2897569526377435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 11, Batch 1524] Loss: 0.28971341316332444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 0] Loss: 0.2897628881795751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1] Loss: 0.2897595399470898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 2] Loss: 0.28975971788290217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 3] Loss: 0.28972950968720274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 4] Loss: 0.2897419182137119\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 5] Loss: 0.2897095125950262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 6] Loss: 0.2896642154128444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 7] Loss: 0.28974983875235716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 8] Loss: 0.2897157233783135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 9] Loss: 0.2897186573111765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 10] Loss: 0.28969241228387516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 11] Loss: 0.2896325698388126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 12] Loss: 0.2895919332816334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 13] Loss: 0.28959173015382766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 14] Loss: 0.2895657514624149\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 15] Loss: 0.2895581977295817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 16] Loss: 0.2895225785473074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 17] Loss: 0.28946429506980814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 18] Loss: 0.28951359037802454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 19] Loss: 0.28950663976366425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 20] Loss: 0.2894964349539692\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 21] Loss: 0.2894379938299545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 22] Loss: 0.28942667747436884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 23] Loss: 0.28945667532089286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 24] Loss: 0.2894404805279023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 25] Loss: 0.2894196134764422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 26] Loss: 0.2893865573271813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 27] Loss: 0.2893979066417766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 28] Loss: 0.28935329923863873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 29] Loss: 0.28942952976333647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 30] Loss: 0.2894245782022069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 31] Loss: 0.2894235108282674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 32] Loss: 0.2893965063016737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 33] Loss: 0.28935390819636275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 34] Loss: 0.2893812580747374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 35] Loss: 0.28939569616061334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 36] Loss: 0.2894363855663173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 37] Loss: 0.28948913778962254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 38] Loss: 0.2894936970075906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 39] Loss: 0.28950317723292435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 40] Loss: 0.28964912050329694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 41] Loss: 0.28961645491054244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 42] Loss: 0.28963128540690253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 43] Loss: 0.28966835313331857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 44] Loss: 0.2897044435136235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 45] Loss: 0.28964884151315806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 46] Loss: 0.2896301456576738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 47] Loss: 0.28961606798295947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 48] Loss: 0.2896237702341073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 49] Loss: 0.28962626724197504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 50] Loss: 0.2896317469801675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 51] Loss: 0.2896073837379518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 52] Loss: 0.2896840548528311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 53] Loss: 0.2896344810183141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 54] Loss: 0.28964636220604845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 55] Loss: 0.28960981595299895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 56] Loss: 0.28962642186396703\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 57] Loss: 0.2896179394420599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 58] Loss: 0.2896797521915641\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 59] Loss: 0.2896386991511819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 60] Loss: 0.28963914853723155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 61] Loss: 0.28959465944505053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 62] Loss: 0.289776678406611\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 63] Loss: 0.289808719768938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 64] Loss: 0.2898292335923959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 65] Loss: 0.2898218321395361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 66] Loss: 0.2897996085739929\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 67] Loss: 0.2898854768289143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 68] Loss: 0.2899067816837612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 69] Loss: 0.28987489383627035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 70] Loss: 0.2899212684368724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 71] Loss: 0.28987481397611303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 72] Loss: 0.2899074387635927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 73] Loss: 0.28997360558307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 74] Loss: 0.29003554037358564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 75] Loss: 0.2900036465603476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 76] Loss: 0.2899698019875181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 77] Loss: 0.28995284044722536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 78] Loss: 0.2899578335923699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 79] Loss: 0.2899261256202903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 80] Loss: 0.289900912078619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 81] Loss: 0.2898599190943739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 82] Loss: 0.2898290188852632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 83] Loss: 0.2898203565362755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 84] Loss: 0.2898131475988365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 85] Loss: 0.28977990921098773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 86] Loss: 0.2897906675327989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 87] Loss: 0.2897925262647475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 88] Loss: 0.2897865790071158\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 89] Loss: 0.28974595830324695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 90] Loss: 0.2896939034607897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 91] Loss: 0.2897163438456304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 92] Loss: 0.2897314486605349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 93] Loss: 0.28978527463507203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 94] Loss: 0.28977248827769864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 95] Loss: 0.28975508005067613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 96] Loss: 0.28972011277869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 97] Loss: 0.28969039093468274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 98] Loss: 0.289656090637538\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 99] Loss: 0.2897126991384717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 100] Loss: 0.2896760269253261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 101] Loss: 0.289697823340271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 102] Loss: 0.28973157855752807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 103] Loss: 0.2897027611124224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 104] Loss: 0.2897056605443287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 105] Loss: 0.28969609840876526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 106] Loss: 0.2896678413236966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 107] Loss: 0.28967604163424276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 108] Loss: 0.28963760629797264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 109] Loss: 0.289595631074654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 110] Loss: 0.28963857036550333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 111] Loss: 0.2895953385773173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 112] Loss: 0.2895882566591343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 113] Loss: 0.28961481813971307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 114] Loss: 0.2895835544023664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 115] Loss: 0.2895655795872872\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 116] Loss: 0.289545320696853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 117] Loss: 0.28951344870332485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 118] Loss: 0.28952104301603204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 119] Loss: 0.2894998278160369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 120] Loss: 0.2894623480078074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 121] Loss: 0.289539610155148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 122] Loss: 0.2895412817844409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 123] Loss: 0.28947497242849995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 124] Loss: 0.2894787305278916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 125] Loss: 0.2894560322764826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 126] Loss: 0.2894626223420221\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 127] Loss: 0.2894621372474228\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 128] Loss: 0.2894110688161876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 129] Loss: 0.2894649097441256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 130] Loss: 0.2894557278069455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 131] Loss: 0.28942516794127954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 132] Loss: 0.28938397602288085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 133] Loss: 0.2893720539691146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 134] Loss: 0.28932490191952953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 135] Loss: 0.2892862098619529\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 136] Loss: 0.28931119006873113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 137] Loss: 0.28929043628069345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 138] Loss: 0.2892643873443159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 139] Loss: 0.2892582950137799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 140] Loss: 0.2892660785027819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 141] Loss: 0.2894331816495007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 142] Loss: 0.28948035038444253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 143] Loss: 0.2895012292601963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 144] Loss: 0.2894538967864627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 145] Loss: 0.28945435976790596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 146] Loss: 0.2895235094060553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 147] Loss: 0.2894801659810644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 148] Loss: 0.2895288647068189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 149] Loss: 0.28948467011512746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 150] Loss: 0.28947716596797224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 151] Loss: 0.2895150169886378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 152] Loss: 0.2895210758189522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 153] Loss: 0.28951867016904836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 154] Loss: 0.2894816705482514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 155] Loss: 0.2895303007296752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 156] Loss: 0.2895964486015305\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 157] Loss: 0.28963721485262844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 158] Loss: 0.2895969358719757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 159] Loss: 0.2895809691085407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 160] Loss: 0.2895870891578391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 161] Loss: 0.28953613628443337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 162] Loss: 0.2894867757665784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 163] Loss: 0.28944266140530645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 164] Loss: 0.2894062753226962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 165] Loss: 0.28948020317542406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 166] Loss: 0.28949034842249205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 167] Loss: 0.2895782558818071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 168] Loss: 0.28963878463143283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 169] Loss: 0.2896065137568715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 170] Loss: 0.2895983871588162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 171] Loss: 0.2895357433871009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 172] Loss: 0.28950190878906606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 173] Loss: 0.2894938013523891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 174] Loss: 0.2894692149356994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 175] Loss: 0.2894557978367861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 176] Loss: 0.28950199545313393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 177] Loss: 0.28947036395983655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 178] Loss: 0.2895699937094212\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 179] Loss: 0.28972577025753354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 180] Loss: 0.28967303710274916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 181] Loss: 0.28961238151292623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 182] Loss: 0.2895714599739611\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 183] Loss: 0.2895521697665587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 184] Loss: 0.28957406858881585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 185] Loss: 0.2895673370340568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 186] Loss: 0.2896031682387903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 187] Loss: 0.28954343402430893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 188] Loss: 0.2895270159362622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 189] Loss: 0.2894986633893307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 190] Loss: 0.2895536614590579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 191] Loss: 0.2895902058326067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 192] Loss: 0.2895879952232982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 193] Loss: 0.2895564771624923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 194] Loss: 0.2896241878814059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 195] Loss: 0.289628094771624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 196] Loss: 0.289576352377092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 197] Loss: 0.2895762293158877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 198] Loss: 0.28956512972078424\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 199] Loss: 0.28951459324605183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 200] Loss: 0.28951992959082584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 201] Loss: 0.28954179442555944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 202] Loss: 0.2895125810138383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 203] Loss: 0.28946996722347446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 204] Loss: 0.28947956846650286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 205] Loss: 0.2894137909813569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 206] Loss: 0.28942347115913525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 207] Loss: 0.28938523405386185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 208] Loss: 0.2894065715716118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 209] Loss: 0.2894233355782903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 210] Loss: 0.2894665258763481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 211] Loss: 0.2894426265340161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 212] Loss: 0.28943962654705646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 213] Loss: 0.2894286068386191\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 214] Loss: 0.28955357697920664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 215] Loss: 0.28963601391625365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 216] Loss: 0.2896301435813312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 217] Loss: 0.28973047096216004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 218] Loss: 0.2897940855456155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 219] Loss: 0.2897572639094684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 220] Loss: 0.2897357624018448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 221] Loss: 0.28973699664378816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 222] Loss: 0.289739810574582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 223] Loss: 0.2897203960005682\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 224] Loss: 0.289907657390625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 225] Loss: 0.2899599092394478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 226] Loss: 0.28997080802384484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 227] Loss: 0.29000637675168717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 228] Loss: 0.29006836275728587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 229] Loss: 0.29007771596080406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 230] Loss: 0.2901294185312195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 231] Loss: 0.2900853806160509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 232] Loss: 0.29012432190946846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 233] Loss: 0.2901205682275843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 234] Loss: 0.29014512947776144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 235] Loss: 0.2901061403621897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 236] Loss: 0.2901128467770747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 237] Loss: 0.29014916355125175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 238] Loss: 0.2901783658476944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 239] Loss: 0.2901677932719565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 240] Loss: 0.29013055194980114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 241] Loss: 0.290094477727066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 242] Loss: 0.2900996406876845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 243] Loss: 0.2900596048644406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 244] Loss: 0.2900943515788878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 245] Loss: 0.29006995778293054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 246] Loss: 0.29007333323622103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 247] Loss: 0.2901491746974075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 248] Loss: 0.2901917685082337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 249] Loss: 0.2902105213168377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 250] Loss: 0.2901584180323506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 251] Loss: 0.29015934925630366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 252] Loss: 0.2901159642597003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 253] Loss: 0.29010093702695783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 254] Loss: 0.290055679479768\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 255] Loss: 0.2900211282288938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 256] Loss: 0.2900089226681169\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 257] Loss: 0.28999283898854206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 258] Loss: 0.2900456477023149\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 259] Loss: 0.29002555363561244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 260] Loss: 0.29006059259379147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 261] Loss: 0.29009407896072986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 262] Loss: 0.2900714062014609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 263] Loss: 0.2901660698976147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 264] Loss: 0.2901716134208374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 265] Loss: 0.2901503066270643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 266] Loss: 0.29015322760809625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 267] Loss: 0.290167950299582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 268] Loss: 0.29012146279881484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 269] Loss: 0.2901342728253348\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 270] Loss: 0.2901680639235592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 271] Loss: 0.29013863731562123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 272] Loss: 0.29015017550282535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 273] Loss: 0.2901094017551472\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 274] Loss: 0.29009578309775874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 275] Loss: 0.2900995995164905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 276] Loss: 0.2900994949410727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 277] Loss: 0.2901834905907862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 278] Loss: 0.2901694420181392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 279] Loss: 0.2901454402210032\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 280] Loss: 0.2902106718719006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 281] Loss: 0.29015643118364903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 282] Loss: 0.2901569201742829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 283] Loss: 0.29011901171291427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 284] Loss: 0.29006518102727985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 285] Loss: 0.2901582798095526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 286] Loss: 0.2901203860379185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 287] Loss: 0.2900985898450017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 288] Loss: 0.2901157534396527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 289] Loss: 0.2902084541505389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 290] Loss: 0.290222412129973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 291] Loss: 0.2902481576005707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 292] Loss: 0.2902221731988924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 293] Loss: 0.29020157575701044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 294] Loss: 0.29017301041361776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 295] Loss: 0.29030150217589684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 296] Loss: 0.2903446783550001\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 297] Loss: 0.29038452314361174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 298] Loss: 0.290389026131569\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 299] Loss: 0.29035207083444314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 300] Loss: 0.290314901192493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 301] Loss: 0.290336987825341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 302] Loss: 0.2903243786406884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 303] Loss: 0.2903286371538093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 304] Loss: 0.29033004526225764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 305] Loss: 0.2902925331879463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 306] Loss: 0.29031518009755014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 307] Loss: 0.29029945046407546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 308] Loss: 0.2902765655809557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 309] Loss: 0.29026756535706844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 310] Loss: 0.29023853015548784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 311] Loss: 0.29021598809066806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 312] Loss: 0.2902603388145579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 313] Loss: 0.2902270034543544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 314] Loss: 0.2901811034350159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 315] Loss: 0.29019305904373704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 316] Loss: 0.2902532112029595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 317] Loss: 0.29026493703717104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 318] Loss: 0.2903094221025023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 319] Loss: 0.2903696431808271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 320] Loss: 0.2903623086349128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 321] Loss: 0.290375369238494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 322] Loss: 0.29048602422076975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 323] Loss: 0.29061409487386813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 324] Loss: 0.29060914861820647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 325] Loss: 0.2906120147710826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 326] Loss: 0.29060167316993846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 327] Loss: 0.2906272081737788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 328] Loss: 0.29061463602127996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 329] Loss: 0.2905924373097104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 330] Loss: 0.29059496325585576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 331] Loss: 0.2906414634334056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 332] Loss: 0.29060900221593394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 333] Loss: 0.29062347248739184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 334] Loss: 0.2906537852546332\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 335] Loss: 0.29065303810976845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 336] Loss: 0.29071032677999403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 337] Loss: 0.29066412904840083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 338] Loss: 0.2906105670050566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 339] Loss: 0.2905927666432134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 340] Loss: 0.2906713177360583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 341] Loss: 0.29063690882517224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 342] Loss: 0.2906097514048364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 343] Loss: 0.290612364569382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 344] Loss: 0.2906039416466835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 345] Loss: 0.29065098711632875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 346] Loss: 0.2906554622300446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 347] Loss: 0.29069070128904095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 348] Loss: 0.2906667832663953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 349] Loss: 0.29062207615794367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 350] Loss: 0.29060137260579044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 351] Loss: 0.29060661820947103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 352] Loss: 0.290565727185924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 353] Loss: 0.2906192622611791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 354] Loss: 0.2906231000508744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 355] Loss: 0.29066590091870986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 356] Loss: 0.29069298926507575\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 357] Loss: 0.29069876705111825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 358] Loss: 0.2906598682155985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 359] Loss: 0.2906327025513969\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 360] Loss: 0.29065710374546644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 361] Loss: 0.2906955003364343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 362] Loss: 0.2907175767859529\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 363] Loss: 0.2907198324215821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 364] Loss: 0.29074094631488145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 365] Loss: 0.2906925230879675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 366] Loss: 0.2907120313990137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 367] Loss: 0.2906773615932007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 368] Loss: 0.2908101834295661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 369] Loss: 0.29079240309669213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 370] Loss: 0.2907729137312528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 371] Loss: 0.29079230827338276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 372] Loss: 0.2908007480369819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 373] Loss: 0.29081197454108526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 374] Loss: 0.29078613312407764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 375] Loss: 0.29075131134527643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 376] Loss: 0.2909094049984323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 377] Loss: 0.29099466862545625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 378] Loss: 0.2910515686087343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 379] Loss: 0.2910704617312387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 380] Loss: 0.29104682818055155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 381] Loss: 0.2910528548346736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 382] Loss: 0.29103829081083027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 383] Loss: 0.29109893808013115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 384] Loss: 0.2911473851184885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 385] Loss: 0.2911077589241913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 386] Loss: 0.29113610777314464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 387] Loss: 0.29113989556208253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 388] Loss: 0.2911275760838998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 389] Loss: 0.29116850397205135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 390] Loss: 0.2911595783303485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 391] Loss: 0.2911164638823781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 392] Loss: 0.2911462890044405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 393] Loss: 0.29115280878381755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 394] Loss: 0.2911438809723468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 395] Loss: 0.29127957074209876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 396] Loss: 0.2912584117206939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 397] Loss: 0.29121979168967443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 398] Loss: 0.2913215551455025\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 399] Loss: 0.2912939721069425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 400] Loss: 0.2912752743221073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 401] Loss: 0.2913409148073186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 402] Loss: 0.2912968150268422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 403] Loss: 0.2912902248053101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 404] Loss: 0.2912623039950371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 405] Loss: 0.29127829763876356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 406] Loss: 0.2912462207123581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 407] Loss: 0.291262444437871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 408] Loss: 0.29126405552495677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 409] Loss: 0.2912278342237087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 410] Loss: 0.29123238630582043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 411] Loss: 0.29123997453845935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 412] Loss: 0.2912198787391841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 413] Loss: 0.29129842313371723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 414] Loss: 0.2912992786462078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 415] Loss: 0.29132946698759504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 416] Loss: 0.29137945610128946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 417] Loss: 0.29139189992975906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 418] Loss: 0.2915781712804421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 419] Loss: 0.29159001452099814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 420] Loss: 0.2915804310117191\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 421] Loss: 0.29159781966300385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 422] Loss: 0.291593265476069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 423] Loss: 0.29159200702060223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 424] Loss: 0.2916110243887766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 425] Loss: 0.2916434464960088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 426] Loss: 0.29158864316169103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 427] Loss: 0.29155719547729925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 428] Loss: 0.29152097414751027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 429] Loss: 0.2915260350888206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 430] Loss: 0.2915644975156664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 431] Loss: 0.2915705515616182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 432] Loss: 0.29159579450371176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 433] Loss: 0.2915718418702937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 434] Loss: 0.2915516692227367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 435] Loss: 0.29152201175711107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 436] Loss: 0.2914819180359703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 437] Loss: 0.2914288828587015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 438] Loss: 0.2914574068673907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 439] Loss: 0.29146278064578224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 440] Loss: 0.2914501755456928\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 441] Loss: 0.2914661038175388\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 442] Loss: 0.2914621261385696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 443] Loss: 0.29146418583882683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 444] Loss: 0.2914905535379106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 445] Loss: 0.29148417479948363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 446] Loss: 0.2916083833332374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 447] Loss: 0.29160322708196146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 448] Loss: 0.2915544786804851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 449] Loss: 0.2915382141020292\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 450] Loss: 0.2915189266897663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 451] Loss: 0.2915160656838467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 452] Loss: 0.29149001991562173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 453] Loss: 0.29148480564189716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 454] Loss: 0.29153233245824295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 455] Loss: 0.29155502040577785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 456] Loss: 0.2915518753617891\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8839\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 457] Loss: 0.2915684603492935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 458] Loss: 0.2915453755190687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 459] Loss: 0.2915341490465025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 460] Loss: 0.29149290356949803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 461] Loss: 0.29144842637606133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 462] Loss: 0.2914237111758006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 463] Loss: 0.29144093909214847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 464] Loss: 0.2914255853486506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 465] Loss: 0.2914662169350943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 466] Loss: 0.2914421422833036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 467] Loss: 0.2914730320039687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 468] Loss: 0.2914928195495959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 469] Loss: 0.2915304479924416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 470] Loss: 0.29159418799057285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 471] Loss: 0.29158609660619933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 472] Loss: 0.2915600979155714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 473] Loss: 0.2915581821677375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 474] Loss: 0.29154552757828944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 475] Loss: 0.2915445701145059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 476] Loss: 0.2915547071154383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 477] Loss: 0.29157897303793695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 478] Loss: 0.2915276693142665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 479] Loss: 0.2914939734093194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 480] Loss: 0.2914811514212307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 481] Loss: 0.2914605131145658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 482] Loss: 0.29142681162287876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 483] Loss: 0.29142598421872506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 484] Loss: 0.2914442080781645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 485] Loss: 0.29151211638562907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 486] Loss: 0.2915176919900689\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 487] Loss: 0.291547923437441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 488] Loss: 0.29156446549804194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 489] Loss: 0.2915469426598482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 490] Loss: 0.29150390897415146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 491] Loss: 0.2914599571724705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 492] Loss: 0.29144147488506866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 493] Loss: 0.2916126713259888\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 494] Loss: 0.2915884120751401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 495] Loss: 0.2915370577862118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 496] Loss: 0.29149743701354724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 497] Loss: 0.2914677979061743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 498] Loss: 0.2914491478020429\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 499] Loss: 0.2914392641560505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 500] Loss: 0.2914547825492235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 501] Loss: 0.2914256360781055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 502] Loss: 0.2914396571617732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 503] Loss: 0.2913882429905765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 504] Loss: 0.2913933025080491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 505] Loss: 0.291376942999556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 506] Loss: 0.29147346435580007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 507] Loss: 0.2914555288320575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 508] Loss: 0.29141905625651426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 509] Loss: 0.2914055787314088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 510] Loss: 0.29135858805852766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 511] Loss: 0.2913490477121771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 512] Loss: 0.2913758569596807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 513] Loss: 0.2913992580533145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 514] Loss: 0.2913841427484149\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 515] Loss: 0.291472763290859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 516] Loss: 0.29144059578456327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 517] Loss: 0.2914082138573752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 518] Loss: 0.2913955002845404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 519] Loss: 0.29140239602266305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 520] Loss: 0.2913858908788937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 521] Loss: 0.2913460603168739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 522] Loss: 0.29134262527488247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 523] Loss: 0.29141405753002003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 524] Loss: 0.2915021103145813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 525] Loss: 0.2914858545440168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 526] Loss: 0.291490329713781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 527] Loss: 0.2914936710242912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 528] Loss: 0.2914570420386159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 529] Loss: 0.2914487810708503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 530] Loss: 0.29147611362429765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 531] Loss: 0.2914770258334309\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 532] Loss: 0.29145684904427477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 533] Loss: 0.2913973050222192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 534] Loss: 0.2913461951567545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 535] Loss: 0.2913510418294112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 536] Loss: 0.2913563312737428\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 537] Loss: 0.29141444981094017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 538] Loss: 0.2913721101777017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 539] Loss: 0.2913526043069266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 540] Loss: 0.29143415660268757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 541] Loss: 0.2914273304614403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 542] Loss: 0.29143002876767315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 543] Loss: 0.2914767730451597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 544] Loss: 0.2914378620190071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 545] Loss: 0.291408821175648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 546] Loss: 0.29140910066288683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 547] Loss: 0.29143328765481874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 548] Loss: 0.2914438707851995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 549] Loss: 0.29140228839382804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 550] Loss: 0.2913855394698234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 551] Loss: 0.2913872803508556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 552] Loss: 0.29136633223726877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 553] Loss: 0.29131547440818745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 554] Loss: 0.29131082609651915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 555] Loss: 0.2912634171638638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 556] Loss: 0.29127203979651256\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 557] Loss: 0.29126908491649506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 558] Loss: 0.29127120412627655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 559] Loss: 0.2912467802398229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 560] Loss: 0.2912773561884808\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 561] Loss: 0.2912575989575252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 562] Loss: 0.29127425246306293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 563] Loss: 0.2912744424520618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 564] Loss: 0.29126903680729943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 565] Loss: 0.2912795725631995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 566] Loss: 0.291236437517738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 567] Loss: 0.2911938110362338\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 568] Loss: 0.29116870647354565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 569] Loss: 0.2912077185253066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 570] Loss: 0.29124077767195544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 571] Loss: 0.2912885207627216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 572] Loss: 0.29128668556864795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 573] Loss: 0.2912880109953594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 574] Loss: 0.2913565141264412\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 575] Loss: 0.2913353245736239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 576] Loss: 0.29133860927072064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 577] Loss: 0.29131518882889074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 578] Loss: 0.2912948003083492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 579] Loss: 0.29127118852561046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 580] Loss: 0.2913193569111413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 581] Loss: 0.29130853819509306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 582] Loss: 0.29128008588803417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 583] Loss: 0.2912403119957976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 584] Loss: 0.2912083478615234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 585] Loss: 0.2912010148095773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 586] Loss: 0.2911533007813252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 587] Loss: 0.29114885744843877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 588] Loss: 0.29109714148546134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 589] Loss: 0.2911160402399748\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 590] Loss: 0.29113635209784877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 591] Loss: 0.29109552566257146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 592] Loss: 0.2910671694778318\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 593] Loss: 0.2910371320501355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 594] Loss: 0.290998689952125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 595] Loss: 0.29101048427234805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 596] Loss: 0.291015571281539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 597] Loss: 0.29100557436009705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 598] Loss: 0.29099064666267577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 599] Loss: 0.29109914695821565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 600] Loss: 0.2910743000872423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 601] Loss: 0.2910352983060883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 602] Loss: 0.29104702188657644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 603] Loss: 0.2910522648527516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 604] Loss: 0.2910884779786845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 605] Loss: 0.29110269791969695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 606] Loss: 0.29107100229941957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 607] Loss: 0.29106144763443614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 608] Loss: 0.2910437077343007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 609] Loss: 0.2911211056028175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 610] Loss: 0.29112071774584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 611] Loss: 0.2910957947929738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 612] Loss: 0.29110199751397087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 613] Loss: 0.2910667198199418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 614] Loss: 0.29102440333249924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 615] Loss: 0.2910177649502627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 616] Loss: 0.29102207396390073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 617] Loss: 0.2910706164912488\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 618] Loss: 0.29110494283377036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 619] Loss: 0.2911030705337002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 620] Loss: 0.29109662311484513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 621] Loss: 0.29116042478307796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 622] Loss: 0.29118083172889475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 623] Loss: 0.2911237143112129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 624] Loss: 0.29114471174193746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 625] Loss: 0.29113122773044603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 626] Loss: 0.2910870317713713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 627] Loss: 0.2910693577726114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 628] Loss: 0.2910542570058588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 629] Loss: 0.2910278981473453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 630] Loss: 0.2910161229876839\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 631] Loss: 0.2909857198095377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 632] Loss: 0.29098303083645477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 633] Loss: 0.2910890643043791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 634] Loss: 0.29108485396922074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 635] Loss: 0.2910468328308881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 636] Loss: 0.2910958735104797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 637] Loss: 0.29107924334761537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 638] Loss: 0.29112499137162484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 639] Loss: 0.2911787600378859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 640] Loss: 0.29120189584437833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 641] Loss: 0.29115217434655727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 642] Loss: 0.2911283252313672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 643] Loss: 0.2910911915854121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 644] Loss: 0.29118526003008677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 645] Loss: 0.2912377559324994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 646] Loss: 0.2912704805015662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 647] Loss: 0.29124540998777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 648] Loss: 0.2912593578918255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 649] Loss: 0.2912558267739136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 650] Loss: 0.2912938529738957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 651] Loss: 0.29126370818862185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 652] Loss: 0.2912356395456786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 653] Loss: 0.29123175193456424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 654] Loss: 0.2912073556638934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 655] Loss: 0.2912127623978902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 656] Loss: 0.2912023375371974\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 657] Loss: 0.2911661012698949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 658] Loss: 0.29116082287743356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 659] Loss: 0.2911409900913952\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 660] Loss: 0.2911146930163769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 661] Loss: 0.29112471421879976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 662] Loss: 0.29112080264246654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 663] Loss: 0.29109884888243165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 664] Loss: 0.29109256684752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 665] Loss: 0.29111158910164775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 666] Loss: 0.29114476411358203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 667] Loss: 0.2911229118923576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 668] Loss: 0.29110332786206156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 669] Loss: 0.29111322526773364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 670] Loss: 0.29113193532680115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 671] Loss: 0.2910977556461426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 672] Loss: 0.2910742619482538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 673] Loss: 0.29107723505030303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 674] Loss: 0.2910881386190837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 675] Loss: 0.29104492878341065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 676] Loss: 0.29101379177373154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 677] Loss: 0.2911178747553344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 678] Loss: 0.2910836634108321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 679] Loss: 0.2910965645353677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 680] Loss: 0.29109086005979734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 681] Loss: 0.2910543850768543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 682] Loss: 0.2910198628120794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 683] Loss: 0.29106417860008393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 684] Loss: 0.2910445342607123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 685] Loss: 0.29100963257692614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 686] Loss: 0.2909727260116932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 687] Loss: 0.29093047947446155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 688] Loss: 0.2908944719022715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 689] Loss: 0.29089304277335154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 690] Loss: 0.2909601002263337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 691] Loss: 0.29090320696352284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 692] Loss: 0.29091296507584435\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 693] Loss: 0.2909855640011477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 694] Loss: 0.2909520350468369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 695] Loss: 0.2909393501522787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 696] Loss: 0.2909126773076289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 697] Loss: 0.29094360138522085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 698] Loss: 0.2909209400090963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 699] Loss: 0.2909504004732236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 700] Loss: 0.2909098950392493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 701] Loss: 0.29086877747443324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 702] Loss: 0.29084085988356556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 703] Loss: 0.2908231256581374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 704] Loss: 0.29092129006600276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 705] Loss: 0.29091808070838454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 706] Loss: 0.2909208048682266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 707] Loss: 0.2908763382772504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 708] Loss: 0.29084059779000954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 709] Loss: 0.2909411636940821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 710] Loss: 0.2909384942433726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 711] Loss: 0.29093458246701537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 712] Loss: 0.29092727395184564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 713] Loss: 0.2909216169949905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 714] Loss: 0.29087663476483466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 715] Loss: 0.29085271086127673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 716] Loss: 0.2908596442752349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 717] Loss: 0.2909078109744047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 718] Loss: 0.29085442487080143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 719] Loss: 0.2908415804338997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 720] Loss: 0.2908212257957316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 721] Loss: 0.2908199213067316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 722] Loss: 0.29081810124263957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 723] Loss: 0.29076351235954845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 724] Loss: 0.2907350847591975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 725] Loss: 0.2907324349245832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 726] Loss: 0.2907224171104671\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 727] Loss: 0.29080002364126745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 728] Loss: 0.2907700773734346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 729] Loss: 0.2908351225546494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 730] Loss: 0.29080031081836744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 731] Loss: 0.29079088834173566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 732] Loss: 0.29080426813193755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 733] Loss: 0.2908141139733012\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 734] Loss: 0.2908571024202786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 735] Loss: 0.29086778708216215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 736] Loss: 0.2908122834696069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 737] Loss: 0.29082774312103915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 738] Loss: 0.29081135515459405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 739] Loss: 0.29080349956609575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 740] Loss: 0.2907711265020594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 741] Loss: 0.29073708289048533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 742] Loss: 0.2907860462357304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 743] Loss: 0.2907681321713448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 744] Loss: 0.29080800228676246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 745] Loss: 0.2908186090564586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 746] Loss: 0.29079387271032425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 747] Loss: 0.2907591957101826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 748] Loss: 0.2907265051502038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 749] Loss: 0.2907324307870648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 750] Loss: 0.29075287812056905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 751] Loss: 0.29075980320616035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 752] Loss: 0.2907899974636618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 753] Loss: 0.29079332814236547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 754] Loss: 0.29080364421778804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 755] Loss: 0.29079398825666625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 756] Loss: 0.29090738033748903\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 757] Loss: 0.29089486076036514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 758] Loss: 0.29087658310634756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 759] Loss: 0.29087312500649665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 760] Loss: 0.2908571309538536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 761] Loss: 0.29087771450943123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 762] Loss: 0.2908240674405562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 763] Loss: 0.2908398931906676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 764] Loss: 0.29090580638682917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 765] Loss: 0.2908727195700557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 766] Loss: 0.29097763659411513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 767] Loss: 0.29097486345111107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 768] Loss: 0.29101510372803246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 769] Loss: 0.29099953342883544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 770] Loss: 0.2910014207473667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 771] Loss: 0.29097895559599035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 772] Loss: 0.29100864808952775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 773] Loss: 0.29107158358568386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 774] Loss: 0.29108574978166396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 775] Loss: 0.2910949062955863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 776] Loss: 0.29108485572763493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 777] Loss: 0.2910698490341114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 778] Loss: 0.29108657285750394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 779] Loss: 0.29117020537051913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 780] Loss: 0.2911373782459817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 781] Loss: 0.29113548206456563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 782] Loss: 0.29110661156715006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 783] Loss: 0.2911015786882587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 784] Loss: 0.2911402269833998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 785] Loss: 0.2911269402175208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 786] Loss: 0.29109372421749047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 787] Loss: 0.29110495534886865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 788] Loss: 0.29122261160986096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 789] Loss: 0.29124499381222047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 790] Loss: 0.2912851564761013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 791] Loss: 0.2912577800753762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 792] Loss: 0.29124974571741247\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 793] Loss: 0.29126400102867533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 794] Loss: 0.2912777649807834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 795] Loss: 0.29126978232816325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 796] Loss: 0.2912282272562311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 797] Loss: 0.29126738114907896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 798] Loss: 0.2913128965808844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 799] Loss: 0.2913245471896028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 800] Loss: 0.29133398588577236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 801] Loss: 0.29128806538166974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 802] Loss: 0.291308725839091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 803] Loss: 0.29130739975321784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 804] Loss: 0.2912514278726644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 805] Loss: 0.29124148772811737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 806] Loss: 0.29122970357413447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 807] Loss: 0.29119110458191033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 808] Loss: 0.29117210358168566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 809] Loss: 0.2912012735554744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 810] Loss: 0.2911523251053811\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 811] Loss: 0.2912297388413125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 812] Loss: 0.29123218400350254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 813] Loss: 0.2912027630819797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 814] Loss: 0.2911770947887648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 815] Loss: 0.2911933585026644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 816] Loss: 0.29117396052727057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 817] Loss: 0.2911315966968568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 818] Loss: 0.29115074222901155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 819] Loss: 0.29120268439295593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 820] Loss: 0.29114714835566813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 821] Loss: 0.2911146201908326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 822] Loss: 0.29113523755824655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 823] Loss: 0.29115435883360447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 824] Loss: 0.2911237173849266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 825] Loss: 0.2910796522526239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 826] Loss: 0.2910911628441874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 827] Loss: 0.2911083446094027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 828] Loss: 0.291068289632864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 829] Loss: 0.2910269168039681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 830] Loss: 0.2910088351705382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 831] Loss: 0.2910582889015599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 832] Loss: 0.2911270408871203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 833] Loss: 0.29111924308028925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 834] Loss: 0.2911326666882386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 835] Loss: 0.2911548378704519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 836] Loss: 0.2911172550132259\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 837] Loss: 0.2911301867787845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 838] Loss: 0.2911770455999731\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 839] Loss: 0.2911266689836196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 840] Loss: 0.2911132758121309\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 841] Loss: 0.29116625968909454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 842] Loss: 0.29114107478334295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 843] Loss: 0.29116968451155956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 844] Loss: 0.29114827319527536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 845] Loss: 0.29113097244198494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 846] Loss: 0.2911773465681018\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 847] Loss: 0.2911820714730475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 848] Loss: 0.29122629572321357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 849] Loss: 0.2911948244712585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 850] Loss: 0.29115879367056646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 851] Loss: 0.29120719770650383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 852] Loss: 0.29118645687228845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 853] Loss: 0.2911826834443855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 854] Loss: 0.2912080292873014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 855] Loss: 0.2911835864310463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 856] Loss: 0.29117548266811055\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 857] Loss: 0.29120583695262625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 858] Loss: 0.291181305022025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 859] Loss: 0.2911517512459751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 860] Loss: 0.2911096507976745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 861] Loss: 0.2911436752303176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 862] Loss: 0.29119319971321883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 863] Loss: 0.29117316371087315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 864] Loss: 0.2911367474524573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 865] Loss: 0.29116236776818555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 866] Loss: 0.2911298445999104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 867] Loss: 0.2910986216500752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 868] Loss: 0.29106849928284406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 869] Loss: 0.2910595791129796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 870] Loss: 0.2910475510528185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 871] Loss: 0.2911279937670974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 872] Loss: 0.291090231383101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 873] Loss: 0.2910634429489261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 874] Loss: 0.2911159723101678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 875] Loss: 0.2911414396863583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 876] Loss: 0.29117092719832205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 877] Loss: 0.291123423295087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 878] Loss: 0.2910919314468284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 879] Loss: 0.2910657223403287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 880] Loss: 0.29103655909371984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 881] Loss: 0.29099976860548515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 882] Loss: 0.2909962969350927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 883] Loss: 0.29096510029604655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 884] Loss: 0.29100152267321855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 885] Loss: 0.2909472408498289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 886] Loss: 0.2911126305671616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 887] Loss: 0.2911590805641041\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 888] Loss: 0.2912190090291848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 889] Loss: 0.2911876112073054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 890] Loss: 0.29122681322080357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 891] Loss: 0.29120959064116836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 892] Loss: 0.2912293639831701\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 893] Loss: 0.2912493583681455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 894] Loss: 0.29125322804883913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 895] Loss: 0.29126888409654045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 896] Loss: 0.29135402365271224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 897] Loss: 0.2913466327162653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 898] Loss: 0.2914094786052697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 899] Loss: 0.2913672632243084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 900] Loss: 0.2913763919890078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 901] Loss: 0.291348620761116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 902] Loss: 0.2913521216657982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 903] Loss: 0.2913271580957699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 904] Loss: 0.2913097966519918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 905] Loss: 0.29128599654930304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 906] Loss: 0.29124923457253193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 907] Loss: 0.29123499484917503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 908] Loss: 0.29119048012496396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 909] Loss: 0.2912486668970091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 910] Loss: 0.2912678224313651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 911] Loss: 0.2912512990938927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 912] Loss: 0.2912264479042259\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 913] Loss: 0.29125077486651874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 914] Loss: 0.2912115467499154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 915] Loss: 0.29116251014003697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 916] Loss: 0.29115658399489786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 917] Loss: 0.2911453527557781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 918] Loss: 0.29121731548332036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 919] Loss: 0.2912928005135369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 920] Loss: 0.2912661149318026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 921] Loss: 0.29126192935081635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 922] Loss: 0.29122792035134665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 923] Loss: 0.29126736946730697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 924] Loss: 0.2912849946002436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 925] Loss: 0.2913030539402358\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 926] Loss: 0.2912741917943255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 927] Loss: 0.2912457234142695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 928] Loss: 0.29126296557060366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 929] Loss: 0.29127135223680456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 930] Loss: 0.2912663267346673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 931] Loss: 0.29125446008360933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 932] Loss: 0.29131473773499816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 933] Loss: 0.2912808831129364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 934] Loss: 0.29132584188546357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 935] Loss: 0.2913522785361014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 936] Loss: 0.29134261975225356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 937] Loss: 0.2913665855785206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 938] Loss: 0.2913443123472341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 939] Loss: 0.2913091523772906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 940] Loss: 0.2913063869076586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 941] Loss: 0.2912742390105294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 942] Loss: 0.2912577169704159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 943] Loss: 0.2912327658484056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 944] Loss: 0.29126447888862134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 945] Loss: 0.2913045321332109\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 946] Loss: 0.29126879867736605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 947] Loss: 0.291224328589202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 948] Loss: 0.2911883475585014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 949] Loss: 0.2912347546573887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 950] Loss: 0.29123439921697925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 951] Loss: 0.29123502951935754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 952] Loss: 0.2912580974936888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 953] Loss: 0.2912218397517706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 954] Loss: 0.29120900805654837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 955] Loss: 0.2911773640261963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 956] Loss: 0.2912125914061749\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8895\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 957] Loss: 0.29119577942539876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 958] Loss: 0.2912034663773539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 959] Loss: 0.2912042026382896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 960] Loss: 0.2911924798994996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 961] Loss: 0.2911987057112853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 962] Loss: 0.29117213325802305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 963] Loss: 0.2911289497631827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 964] Loss: 0.2911353961672159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 965] Loss: 0.29109645658021704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 966] Loss: 0.2910699659879072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 967] Loss: 0.2910845738622998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 968] Loss: 0.2911531390655213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 969] Loss: 0.29131715098515665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 970] Loss: 0.2913452368605404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 971] Loss: 0.2913584583244134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 972] Loss: 0.291327174984796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 973] Loss: 0.2913808449333271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 974] Loss: 0.291339304361193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 975] Loss: 0.2913263635265071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 976] Loss: 0.29127249379869985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 977] Loss: 0.29129728263921106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 978] Loss: 0.29129602431254603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 979] Loss: 0.2913153179195408\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 980] Loss: 0.29132814831892895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 981] Loss: 0.291302978706542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 982] Loss: 0.2912687726785076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 983] Loss: 0.2913187604199618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 984] Loss: 0.29138359005363473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 985] Loss: 0.291405529141537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 986] Loss: 0.2914075148810571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 987] Loss: 0.29138290804038414\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 988] Loss: 0.2913871017512615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 989] Loss: 0.2915061827837667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 990] Loss: 0.2915027094550218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 991] Loss: 0.29152102742853647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 992] Loss: 0.2915696840385399\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 993] Loss: 0.2915920269227651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 994] Loss: 0.29158625131651417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 995] Loss: 0.2915669534198645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 996] Loss: 0.29157137018516643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 997] Loss: 0.29157710390488945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 998] Loss: 0.29156107908552925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 999] Loss: 0.2916325578598709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1000] Loss: 0.2916215254834246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1001] Loss: 0.291628907298834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1002] Loss: 0.29161631251234965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1003] Loss: 0.2915962818198333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1004] Loss: 0.2915490981624741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1005] Loss: 0.2915763539949685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1006] Loss: 0.29153719205171114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1007] Loss: 0.29152748777796783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1008] Loss: 0.2915423010302905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1009] Loss: 0.291603082577782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1010] Loss: 0.29162593316263835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1011] Loss: 0.2916055090341504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1012] Loss: 0.291562398871288\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1013] Loss: 0.29151613577308033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1014] Loss: 0.2914848611259892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1015] Loss: 0.29145579737670757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1016] Loss: 0.2914904000618736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1017] Loss: 0.291499054893954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1018] Loss: 0.2914952298086719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1019] Loss: 0.2915126967425348\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1020] Loss: 0.29155116683424326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1021] Loss: 0.2915562680829454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1022] Loss: 0.2915221538586129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1023] Loss: 0.29150683634191926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1024] Loss: 0.29148150909335907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1025] Loss: 0.29144414950262854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1026] Loss: 0.2914527659397092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1027] Loss: 0.29142041236422006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1028] Loss: 0.2914432363450644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1029] Loss: 0.2914176020006822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1030] Loss: 0.2913783234787499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1031] Loss: 0.2913428742732849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1032] Loss: 0.291376297664366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1033] Loss: 0.2913767770654419\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1034] Loss: 0.29138882667078314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1035] Loss: 0.2913564954637824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1036] Loss: 0.29135415431489053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1037] Loss: 0.2913777250960827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1038] Loss: 0.2913873362752508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1039] Loss: 0.29149098705544146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1040] Loss: 0.2915348588572211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1041] Loss: 0.2915291535972134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1042] Loss: 0.29150030463077137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1043] Loss: 0.29157154621381437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1044] Loss: 0.2915142438440283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1045] Loss: 0.29157185252837853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1046] Loss: 0.29157726600332734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1047] Loss: 0.2915547329164087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1048] Loss: 0.29151391120199294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1049] Loss: 0.29148880206685446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1050] Loss: 0.2914576097977176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1051] Loss: 0.29145182294087135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1052] Loss: 0.29141980981009163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1053] Loss: 0.2913929904856991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1054] Loss: 0.2914040293321868\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1055] Loss: 0.2913886245358281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1056] Loss: 0.29142807794326514\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1057] Loss: 0.29140751623979494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1058] Loss: 0.29137073459783297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1059] Loss: 0.2914131363930061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1060] Loss: 0.2914552556030933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1061] Loss: 0.291433365764237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1062] Loss: 0.291502987334175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1063] Loss: 0.2915293166183566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1064] Loss: 0.2915053307632949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1065] Loss: 0.29149829437117797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1066] Loss: 0.2915963298776845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1067] Loss: 0.2915795820979003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1068] Loss: 0.2916522008200907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1069] Loss: 0.2917055647981416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1070] Loss: 0.29174357305392895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1071] Loss: 0.29173679398218905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1072] Loss: 0.2917314629828956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1073] Loss: 0.29177513278228323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1074] Loss: 0.29178952422281523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1075] Loss: 0.29176814453766764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1076] Loss: 0.2917663045797033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1077] Loss: 0.2917359943408112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1078] Loss: 0.29174190004661826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1079] Loss: 0.29173469337861474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1080] Loss: 0.2917755204366915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1081] Loss: 0.2917912421884377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1082] Loss: 0.2917952829891121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1083] Loss: 0.2917811990022486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1084] Loss: 0.2917897487219381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1085] Loss: 0.291795339318078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1086] Loss: 0.2918187538804399\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1087] Loss: 0.29184007665209133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1088] Loss: 0.29197745596976016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1089] Loss: 0.2919430987676042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1090] Loss: 0.2919215129465405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1091] Loss: 0.29192213894262536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1092] Loss: 0.29195988946183454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1093] Loss: 0.29195768670248723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1094] Loss: 0.2919398750889327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1095] Loss: 0.2919652824204613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1096] Loss: 0.29192580163579607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1097] Loss: 0.29188866538091424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1098] Loss: 0.29185550288534845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1099] Loss: 0.291825975920703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1100] Loss: 0.2917929214433249\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1101] Loss: 0.29177388108932506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1102] Loss: 0.29172386705861025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1103] Loss: 0.2917033280412107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1104] Loss: 0.2917743664954076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1105] Loss: 0.2917273812803877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1106] Loss: 0.29174412889628204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1107] Loss: 0.29182320450269306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1108] Loss: 0.29178847125674223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1109] Loss: 0.2917834556821624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1110] Loss: 0.29177710975909205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1111] Loss: 0.29174292802853696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1112] Loss: 0.2917305844669477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1113] Loss: 0.29177647677463886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1114] Loss: 0.2917739930140568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1115] Loss: 0.2917627694479261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1116] Loss: 0.29181147046070266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1117] Loss: 0.2918390879109466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1118] Loss: 0.2918182100645201\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1119] Loss: 0.2917740699369013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1120] Loss: 0.291795068193956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1121] Loss: 0.29176576444329233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1122] Loss: 0.2917870314112807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1123] Loss: 0.2917637909868028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1124] Loss: 0.2917811687840035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1125] Loss: 0.29178633914112473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1126] Loss: 0.2917421093088664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1127] Loss: 0.29171968733111425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1128] Loss: 0.291692644759402\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1129] Loss: 0.2916524212755652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1130] Loss: 0.2916212439751197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1131] Loss: 0.2916224270245467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1132] Loss: 0.29166129182935874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1133] Loss: 0.2916562564567609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1134] Loss: 0.2916663668463178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1135] Loss: 0.29165082350729754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1136] Loss: 0.2916952976197382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1137] Loss: 0.2917008695634913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1138] Loss: 0.2917006865959546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1139] Loss: 0.2916701665258544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1140] Loss: 0.29168500208797704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1141] Loss: 0.2916724090898259\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1142] Loss: 0.29165526957622373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1143] Loss: 0.29164753680641353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1144] Loss: 0.2917541709000867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1145] Loss: 0.2917971321463869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1146] Loss: 0.29176822732809593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1147] Loss: 0.29175932132390164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1148] Loss: 0.29177413278452796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1149] Loss: 0.29179251334697903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1150] Loss: 0.2917723734877533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1151] Loss: 0.2917686176403746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1152] Loss: 0.2917508608192269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1153] Loss: 0.29175188986956363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1154] Loss: 0.29174701095507466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1155] Loss: 0.2917939401169618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1156] Loss: 0.2918302996602407\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1157] Loss: 0.291837780341428\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1158] Loss: 0.2918823510940206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1159] Loss: 0.29187627737857863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1160] Loss: 0.29187526006193987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1161] Loss: 0.2918368385652106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1162] Loss: 0.2918282462917543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1163] Loss: 0.2918130435977315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1164] Loss: 0.29186868368446417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1165] Loss: 0.291982533313741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1166] Loss: 0.291991420803232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1167] Loss: 0.2920293595610011\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1168] Loss: 0.29204322620536605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1169] Loss: 0.29202561938669314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1170] Loss: 0.2920061945950575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1171] Loss: 0.2919928869886405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1172] Loss: 0.29201764124517393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1173] Loss: 0.2920074587455571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1174] Loss: 0.2920287970500299\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1175] Loss: 0.29200906896901924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1176] Loss: 0.2919971629415917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1177] Loss: 0.2919935832256279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1178] Loss: 0.2919602676389795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1179] Loss: 0.29194354737233935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1180] Loss: 0.29194507288509575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1181] Loss: 0.29195387069782935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1182] Loss: 0.29195601375553665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1183] Loss: 0.29194123959107116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1184] Loss: 0.29193161634808035\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1185] Loss: 0.29191406490039035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1186] Loss: 0.2918822238016174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1187] Loss: 0.29188435826457626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1188] Loss: 0.2918758094768391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1189] Loss: 0.29185198978413146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1190] Loss: 0.29188460241453423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1191] Loss: 0.2918985627056913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1192] Loss: 0.29188304154805933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1193] Loss: 0.29187276878549334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1194] Loss: 0.2918439599689545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1195] Loss: 0.29181241750154857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1196] Loss: 0.2918203340136648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1197] Loss: 0.29181889187806287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1198] Loss: 0.29182836794077777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1199] Loss: 0.2918168951763397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1200] Loss: 0.2918050305511982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1201] Loss: 0.2918366656458383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1202] Loss: 0.2918054405481416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1203] Loss: 0.29179000032262387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1204] Loss: 0.29175174453379715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1205] Loss: 0.29174578949633767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1206] Loss: 0.29177282051643577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1207] Loss: 0.29173768251461457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1208] Loss: 0.29173508915036755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1209] Loss: 0.291716653911452\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1210] Loss: 0.2916811778213387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1211] Loss: 0.2916576974338999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1212] Loss: 0.29163047875040765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1213] Loss: 0.29163845635149166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1214] Loss: 0.2916546968275685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1215] Loss: 0.2916283195882056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1216] Loss: 0.29159582268671375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1217] Loss: 0.29163322359994787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1218] Loss: 0.29161793496193195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1219] Loss: 0.2916587460122122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1220] Loss: 0.29165337319810114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1221] Loss: 0.29165908423908654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1222] Loss: 0.2916699237361959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1223] Loss: 0.29166688207247154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1224] Loss: 0.2916236955050312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1225] Loss: 0.2916597260489397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1226] Loss: 0.29165752543286066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1227] Loss: 0.2916995177031512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1228] Loss: 0.291653637485498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1229] Loss: 0.2916202884242392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1230] Loss: 0.2915806475612852\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1231] Loss: 0.29159396132358445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1232] Loss: 0.2915778097591864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1233] Loss: 0.291591893698691\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1234] Loss: 0.29164501636711954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1235] Loss: 0.29161646615400494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1236] Loss: 0.29161738894777134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1237] Loss: 0.2916034430279347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1238] Loss: 0.2916005416406863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1239] Loss: 0.29165088854222293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1240] Loss: 0.29164717818502783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1241] Loss: 0.29163871645732736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1242] Loss: 0.2916458035255051\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1243] Loss: 0.2916976018433473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1244] Loss: 0.2917173523590486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1245] Loss: 0.29170653153132725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1246] Loss: 0.2917463758196816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1247] Loss: 0.2918117535886427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1248] Loss: 0.2918919716268993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1249] Loss: 0.2918871581887555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1250] Loss: 0.2918665616212541\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1251] Loss: 0.291827942020983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1252] Loss: 0.29184059129430995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1253] Loss: 0.29184628213530867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1254] Loss: 0.291828026336053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1255] Loss: 0.2918523586593395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1256] Loss: 0.2918204748403247\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1257] Loss: 0.291863709390302\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1258] Loss: 0.2918710710349649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1259] Loss: 0.2918378114423344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1260] Loss: 0.29183177264033017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1261] Loss: 0.29178790470409605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1262] Loss: 0.29181152635967755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1263] Loss: 0.2917884630942328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1264] Loss: 0.2918251018498303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1265] Loss: 0.29184104305996694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1266] Loss: 0.291825220484837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1267] Loss: 0.291823601514336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1268] Loss: 0.29180107718308124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1269] Loss: 0.2918344809966033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1270] Loss: 0.291814467758113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1271] Loss: 0.29183818404221834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1272] Loss: 0.29197943365412343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1273] Loss: 0.2919572153529424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1274] Loss: 0.29195310629974996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1275] Loss: 0.291958607983327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1276] Loss: 0.29201067565968125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1277] Loss: 0.29198234688795754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1278] Loss: 0.2919892191269898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1279] Loss: 0.2919875423645361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1280] Loss: 0.2919584749354793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1281] Loss: 0.29193327719043427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1282] Loss: 0.29197073693752895\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1283] Loss: 0.29201366451642924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1284] Loss: 0.29198128327324585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1285] Loss: 0.2919995417513005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1286] Loss: 0.2920094615016653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1287] Loss: 0.29201846629112876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1288] Loss: 0.2920112187915907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1289] Loss: 0.29196596721012746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1290] Loss: 0.2919213618219518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1291] Loss: 0.2919994623304046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1292] Loss: 0.2919872843806222\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1293] Loss: 0.29194815530033363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1294] Loss: 0.29194337666364384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1295] Loss: 0.2919088822390352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1296] Loss: 0.2919133912134324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1297] Loss: 0.29189560837264655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1298] Loss: 0.29185970158805336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1299] Loss: 0.2918282210977806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1300] Loss: 0.29178048951167646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1301] Loss: 0.29177933569279235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1302] Loss: 0.29175622736225737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1303] Loss: 0.2917630620431001\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1304] Loss: 0.2917666835712548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1305] Loss: 0.2918342262026907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1306] Loss: 0.29183131998858214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1307] Loss: 0.2918051978169173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1308] Loss: 0.2918083549757648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1309] Loss: 0.29178271274953793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1310] Loss: 0.291795156357209\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1311] Loss: 0.2917804954499964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1312] Loss: 0.2917675472760053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1313] Loss: 0.2918057167999183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1314] Loss: 0.2917870845774714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1315] Loss: 0.29179676847378594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1316] Loss: 0.2918509007340805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1317] Loss: 0.29185364082462467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1318] Loss: 0.29189067740533814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1319] Loss: 0.29189888036559747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1320] Loss: 0.29195394916427914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1321] Loss: 0.292021883790694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1322] Loss: 0.29203176837246453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1323] Loss: 0.29201938239550523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1324] Loss: 0.2919936622783941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1325] Loss: 0.29197986538541126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1326] Loss: 0.29195528931692466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1327] Loss: 0.29192785570382634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1328] Loss: 0.2919164774362266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1329] Loss: 0.29196388526872025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1330] Loss: 0.2919789035184043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1331] Loss: 0.2919752140149774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1332] Loss: 0.29194019673920285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1333] Loss: 0.2919674917626675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1334] Loss: 0.2919443002450028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1335] Loss: 0.2919409307374802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1336] Loss: 0.2919120496809415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1337] Loss: 0.29190638456672147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1338] Loss: 0.29188915184173525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1339] Loss: 0.29185416495740196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1340] Loss: 0.2918405394053921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1341] Loss: 0.2918392263057294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1342] Loss: 0.29181137892959164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1343] Loss: 0.29181356296799027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1344] Loss: 0.2918036284574916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1345] Loss: 0.291775471695056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1346] Loss: 0.29177912654012844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1347] Loss: 0.2917941548032025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1348] Loss: 0.2917725887143636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1349] Loss: 0.2917614103543742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1350] Loss: 0.2918046164363453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1351] Loss: 0.291779100894928\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1352] Loss: 0.29174620599022283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1353] Loss: 0.29171776848118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1354] Loss: 0.29169139921475495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1355] Loss: 0.29167871498587455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1356] Loss: 0.29166017311577147\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1357] Loss: 0.29164716714095223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1358] Loss: 0.2916202446039281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1359] Loss: 0.2915810620163539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1360] Loss: 0.29158860247806306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1361] Loss: 0.2915623303345534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1362] Loss: 0.29153995387998327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1363] Loss: 0.29155251750901695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1364] Loss: 0.2915382338546252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1365] Loss: 0.2915024138632275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1366] Loss: 0.29147416950488303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1367] Loss: 0.29152060190007995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1368] Loss: 0.2915856451127039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1369] Loss: 0.2916291909326943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1370] Loss: 0.2916347424414258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1371] Loss: 0.29164275601236284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1372] Loss: 0.291633362140703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1373] Loss: 0.29163698411570677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1374] Loss: 0.2916019775460656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1375] Loss: 0.29159297327515227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1376] Loss: 0.29157573233778405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1377] Loss: 0.29157040006013973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1378] Loss: 0.29156167692669693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1379] Loss: 0.2915529904223534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1380] Loss: 0.2916087745037456\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1381] Loss: 0.29158464265701944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1382] Loss: 0.29154815180309207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1383] Loss: 0.29153743293052226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1384] Loss: 0.2915216634110151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1385] Loss: 0.2915263630500227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1386] Loss: 0.2915342404641408\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1387] Loss: 0.2915778493656632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1388] Loss: 0.2915736450793752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1389] Loss: 0.2915720015421449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1390] Loss: 0.29156383084807347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1391] Loss: 0.2915241078492943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1392] Loss: 0.29153247524354275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1393] Loss: 0.29149877946318514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1394] Loss: 0.291508807491668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1395] Loss: 0.29150905496868734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1396] Loss: 0.29150157792259246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1397] Loss: 0.2914849001090707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1398] Loss: 0.29148104623708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1399] Loss: 0.2914887922526774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1400] Loss: 0.2914840602013204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1401] Loss: 0.2915030474306574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1402] Loss: 0.29147057966895656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1403] Loss: 0.29145099788321277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1404] Loss: 0.2914417732392276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1405] Loss: 0.2914866923432002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1406] Loss: 0.2914754019740913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1407] Loss: 0.29145149244137797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1408] Loss: 0.2914686442080734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1409] Loss: 0.291468047473762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1410] Loss: 0.2914593996239699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1411] Loss: 0.291436850337422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1412] Loss: 0.2914467992915924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1413] Loss: 0.29147359312496524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1414] Loss: 0.29150637322094813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1415] Loss: 0.29149123034168517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1416] Loss: 0.2914535343209421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1417] Loss: 0.29144218815002193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1418] Loss: 0.29139442451226455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1419] Loss: 0.2914125479267089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1420] Loss: 0.2914163251890314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1421] Loss: 0.2914262181902834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1422] Loss: 0.2914681987462093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1423] Loss: 0.2914909227044991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1424] Loss: 0.29149390126060226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1425] Loss: 0.29149283821584127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1426] Loss: 0.29150197349340423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1427] Loss: 0.291493401572788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1428] Loss: 0.2915113546069159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1429] Loss: 0.29155122989494725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1430] Loss: 0.29163998863051055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1431] Loss: 0.29159979420689947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1432] Loss: 0.29159122911486374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1433] Loss: 0.29158245127227167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1434] Loss: 0.29158230208411073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1435] Loss: 0.29156171752339494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1436] Loss: 0.29153983328217936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1437] Loss: 0.29162709013671545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1438] Loss: 0.29158777445587974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1439] Loss: 0.2915873554012639\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1440] Loss: 0.29158766901280436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1441] Loss: 0.29155730803563984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1442] Loss: 0.291554893018914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1443] Loss: 0.2916366422578772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1444] Loss: 0.29163373546003896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1445] Loss: 0.29161525063034155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1446] Loss: 0.2916510140469812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1447] Loss: 0.29162064740037874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1448] Loss: 0.2916120317686435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1449] Loss: 0.29165103095183653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1450] Loss: 0.2916529722089099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1451] Loss: 0.2916693483741152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1452] Loss: 0.2916439686890891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1453] Loss: 0.2916861924265373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1454] Loss: 0.291681338685384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1455] Loss: 0.29167564342419305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1456] Loss: 0.291730044689795\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8948\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1457] Loss: 0.29171113848606783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1458] Loss: 0.2917444809738488\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1459] Loss: 0.2917376947630574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1460] Loss: 0.2917182341995303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1461] Loss: 0.2917682585573916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1462] Loss: 0.291798013334981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1463] Loss: 0.2917827906809291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1464] Loss: 0.29182426794044825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1465] Loss: 0.2918365980298186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1466] Loss: 0.2918409019365091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1467] Loss: 0.291848815452466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1468] Loss: 0.2918775245834554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1469] Loss: 0.29188713326713495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1470] Loss: 0.29190441598065797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1471] Loss: 0.2918709905915444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1472] Loss: 0.291869736503573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1473] Loss: 0.29186859161268564\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1474] Loss: 0.29191026832511874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1475] Loss: 0.29190569010653855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1476] Loss: 0.291913298244393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1477] Loss: 0.2919469064794405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1478] Loss: 0.2919234220476349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1479] Loss: 0.29191077269027443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1480] Loss: 0.2919752759795163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1481] Loss: 0.29203964647694175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1482] Loss: 0.29204896650344353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1483] Loss: 0.29211768986124137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1484] Loss: 0.29215883406207743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1485] Loss: 0.2922077921085537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1486] Loss: 0.2922028056986151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1487] Loss: 0.29218677317909403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1488] Loss: 0.2921432065807271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1489] Loss: 0.2921691172064849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1490] Loss: 0.2921976459427206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1491] Loss: 0.2921795103882934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1492] Loss: 0.29220786162257745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1493] Loss: 0.29218116825887314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1494] Loss: 0.29218025591555186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1495] Loss: 0.292137668795457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1496] Loss: 0.29215749336106744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1497] Loss: 0.29211403291552723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1498] Loss: 0.29216803873547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1499] Loss: 0.29212925882688656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1500] Loss: 0.29211517451915014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1501] Loss: 0.292101939608295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1502] Loss: 0.2920916126698931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1503] Loss: 0.29209197264266257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1504] Loss: 0.2920924451478132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1505] Loss: 0.2920799346628425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1506] Loss: 0.2920681056796758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1507] Loss: 0.29210293709557433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1508] Loss: 0.2920969678982912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1509] Loss: 0.29208370903650027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1510] Loss: 0.2920514441495015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1511] Loss: 0.29211463488285166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1512] Loss: 0.29213068262805325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1513] Loss: 0.2921509396410684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1514] Loss: 0.29212445771290196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1515] Loss: 0.29212555616865293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1516] Loss: 0.2920947188862294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1517] Loss: 0.2921125090671626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1518] Loss: 0.29207045517644675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1519] Loss: 0.29207331786302754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1520] Loss: 0.2920888499698529\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1521] Loss: 0.2921014482296589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1522] Loss: 0.2921244972349779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1523] Loss: 0.29210877487510967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 12, Batch 1524] Loss: 0.29212893462299805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 0] Loss: 0.29211212804039666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1] Loss: 0.29213776684901405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 2] Loss: 0.2921896816939376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 3] Loss: 0.29216868816183506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 4] Loss: 0.2921519661310019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 5] Loss: 0.29213849567487593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 6] Loss: 0.2920955697243864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 7] Loss: 0.29207982028483925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 8] Loss: 0.29205662188844006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 9] Loss: 0.29205559546196663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 10] Loss: 0.29201246205733594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 11] Loss: 0.2920055694762626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 12] Loss: 0.29198821262825997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 13] Loss: 0.2919728790822181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 14] Loss: 0.2920050571248403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 15] Loss: 0.2919830880043551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 16] Loss: 0.29201272282240903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 17] Loss: 0.2920339998675635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 18] Loss: 0.2920754718258912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 19] Loss: 0.2920589797859567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 20] Loss: 0.2920775447422253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 21] Loss: 0.2920857855802241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 22] Loss: 0.2920560423197153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 23] Loss: 0.29210896956623444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 24] Loss: 0.2921053167735824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 25] Loss: 0.2921576549831101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 26] Loss: 0.2921514548409934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 27] Loss: 0.2922078993626073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 28] Loss: 0.2922745085118412\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 29] Loss: 0.2922415017488729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 30] Loss: 0.2922108985888569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 31] Loss: 0.2921983801319117\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 32] Loss: 0.2921960090943934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 33] Loss: 0.29216170508285827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 34] Loss: 0.29217629093065095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 35] Loss: 0.2921499480561118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 36] Loss: 0.2921809873976787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 37] Loss: 0.2921552457851755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 38] Loss: 0.2921322091828592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 39] Loss: 0.2922567753745641\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 40] Loss: 0.29225190088411473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 41] Loss: 0.29226997020775364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 42] Loss: 0.29226511778715425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 43] Loss: 0.29224302818033254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 44] Loss: 0.2922790772941627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 45] Loss: 0.2922673720886622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 46] Loss: 0.2922319015423465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 47] Loss: 0.2923316297343032\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 48] Loss: 0.29236936964007515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 49] Loss: 0.2923284670163424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 50] Loss: 0.29232638436510705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 51] Loss: 0.2923317764456642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 52] Loss: 0.29240759242727404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 53] Loss: 0.2924119199805643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 54] Loss: 0.2923949570330601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 55] Loss: 0.29237303927943514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 56] Loss: 0.29242437364919277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 57] Loss: 0.29246874454889954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 58] Loss: 0.2924705666208973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 59] Loss: 0.29249267196888334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 60] Loss: 0.29250838436813903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 61] Loss: 0.29248838034472335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 62] Loss: 0.29254385178654563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 63] Loss: 0.29254518373240773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 64] Loss: 0.29251495333140043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 65] Loss: 0.2925121555902554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 66] Loss: 0.2925012611974484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 67] Loss: 0.2924968561309022\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 68] Loss: 0.2925054875410107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 69] Loss: 0.2925179237662448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 70] Loss: 0.29252438485333376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 71] Loss: 0.29251970936546445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 72] Loss: 0.2925019741478869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 73] Loss: 0.2924955994702736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 74] Loss: 0.2924493605224456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 75] Loss: 0.2924489141793759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 76] Loss: 0.2924614889427756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 77] Loss: 0.29243681651175546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 78] Loss: 0.2924125432365425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 79] Loss: 0.2924612188536676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 80] Loss: 0.2924263060541563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 81] Loss: 0.2924144811122501\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 82] Loss: 0.2923718950457339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 83] Loss: 0.2923696157997868\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 84] Loss: 0.2923474596980615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 85] Loss: 0.29238228255644777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 86] Loss: 0.2923997019407983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 87] Loss: 0.2923989301654999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 88] Loss: 0.29238479456173705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 89] Loss: 0.29237288546859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 90] Loss: 0.29239627874332436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 91] Loss: 0.29237930311381216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 92] Loss: 0.29236346007301184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 93] Loss: 0.29235959185018523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 94] Loss: 0.2923529214268841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 95] Loss: 0.29234770910469027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 96] Loss: 0.2923428829914247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 97] Loss: 0.2923490532702033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 98] Loss: 0.2923797773805406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 99] Loss: 0.2923955351407892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 100] Loss: 0.292378703050108\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 101] Loss: 0.29234473978304243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 102] Loss: 0.2923325663496268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 103] Loss: 0.29232023871610796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 104] Loss: 0.29234150227415906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 105] Loss: 0.2923939551197909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 106] Loss: 0.29240626779323564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 107] Loss: 0.29241668760636336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 108] Loss: 0.29247094534079504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 109] Loss: 0.2924791295053511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 110] Loss: 0.2924967766126506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 111] Loss: 0.29246111719355167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 112] Loss: 0.29243378639093887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 113] Loss: 0.2923996800951741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 114] Loss: 0.29242400238393823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 115] Loss: 0.29243123677649646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 116] Loss: 0.2924032218776551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 117] Loss: 0.292425799778664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 118] Loss: 0.29247008497690075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 119] Loss: 0.2924587208818184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 120] Loss: 0.2924719012018714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 121] Loss: 0.29244464738768977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 122] Loss: 0.2924556247223064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 123] Loss: 0.2924532780648802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 124] Loss: 0.2924692340236448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 125] Loss: 0.2924338823563248\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 126] Loss: 0.2925045893763968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 127] Loss: 0.29251104583125026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 128] Loss: 0.29246991424452196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 129] Loss: 0.29245334279698293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 130] Loss: 0.2924724586926242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 131] Loss: 0.2924763638224279\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 132] Loss: 0.29245198322028265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 133] Loss: 0.2924413367117336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 134] Loss: 0.29242555875819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 135] Loss: 0.29240071731248396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 136] Loss: 0.292399574261154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 137] Loss: 0.292383410419619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 138] Loss: 0.2923625953694902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 139] Loss: 0.29234635166825446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 140] Loss: 0.2923261450623378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 141] Loss: 0.29232162668133865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 142] Loss: 0.2923116099440409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 143] Loss: 0.2922960145838848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 144] Loss: 0.2922910976385256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 145] Loss: 0.29227449232643254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 146] Loss: 0.2922313117182816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 147] Loss: 0.2922142230673996\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 148] Loss: 0.2921940847937588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 149] Loss: 0.2921982721207909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 150] Loss: 0.29226902390088316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 151] Loss: 0.2923212592280712\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 152] Loss: 0.2922939188046281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 153] Loss: 0.2923312296374304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 154] Loss: 0.2923199595247887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 155] Loss: 0.2923212936646724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 156] Loss: 0.29230387112061906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 157] Loss: 0.29230122364819466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 158] Loss: 0.2922666788892269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 159] Loss: 0.29226361494452285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 160] Loss: 0.292225735577859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 161] Loss: 0.2922254952028649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 162] Loss: 0.2922244247597023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 163] Loss: 0.2922102038156475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 164] Loss: 0.2921819971236162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 165] Loss: 0.29215427594143334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 166] Loss: 0.2921200806473231\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 167] Loss: 0.2921623306430315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 168] Loss: 0.2922111535834053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 169] Loss: 0.29218838297225275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 170] Loss: 0.2922031131962055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 171] Loss: 0.29224087377906827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 172] Loss: 0.2922273995185869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 173] Loss: 0.29220020026911064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 174] Loss: 0.2921780305718265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 175] Loss: 0.2921478581299772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 176] Loss: 0.2922106889393251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 177] Loss: 0.29218357696435515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 178] Loss: 0.292164389441055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 179] Loss: 0.29213881911660877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 180] Loss: 0.2921581811951964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 181] Loss: 0.29215451673385195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 182] Loss: 0.2921780343606491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 183] Loss: 0.2921947346466881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 184] Loss: 0.2921720907847496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 185] Loss: 0.29215679400131406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 186] Loss: 0.2921641216111268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 187] Loss: 0.29212508421091316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 188] Loss: 0.2921304523039761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 189] Loss: 0.29213996352575033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 190] Loss: 0.2921892124862105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 191] Loss: 0.2921816235225483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 192] Loss: 0.2921527335166005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 193] Loss: 0.29217951273326237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 194] Loss: 0.2922444017126994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 195] Loss: 0.2922252372126139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 196] Loss: 0.2922698138090202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 197] Loss: 0.2922965975969448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 198] Loss: 0.29228876030928885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 199] Loss: 0.292282244253456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 200] Loss: 0.2922653779621769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 201] Loss: 0.29235568124308225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 202] Loss: 0.29239272574947994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 203] Loss: 0.2923662601986722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 204] Loss: 0.2924084337198015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 205] Loss: 0.29238622591601615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 206] Loss: 0.29242977438653683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 207] Loss: 0.29240122061750745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 208] Loss: 0.29243183455185545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 209] Loss: 0.29241563749721194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 210] Loss: 0.2924196934085885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 211] Loss: 0.29238586331749034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 212] Loss: 0.2923583041335912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 213] Loss: 0.29237003674753637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 214] Loss: 0.2923937473075471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 215] Loss: 0.29239354450158317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 216] Loss: 0.2924112660278723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 217] Loss: 0.29239985077377895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 218] Loss: 0.29242196687985034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 219] Loss: 0.2924075286098775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 220] Loss: 0.2924142887127549\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 221] Loss: 0.29246365976275146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 222] Loss: 0.29243499523678634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 223] Loss: 0.2923930121265894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 224] Loss: 0.29238129949098485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 225] Loss: 0.2923842330996386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 226] Loss: 0.29237724976261475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 227] Loss: 0.29237609317812396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 228] Loss: 0.29238370251766144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 229] Loss: 0.29235707827503915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 230] Loss: 0.29235958639066667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 231] Loss: 0.29235599432090154\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 232] Loss: 0.2923581330481915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 233] Loss: 0.2923852060406674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 234] Loss: 0.292412830674889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 235] Loss: 0.2923756435389449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 236] Loss: 0.2923452240886586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 237] Loss: 0.2923173747146132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 238] Loss: 0.2923213835393455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 239] Loss: 0.2923213347674879\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 240] Loss: 0.2923698963175321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 241] Loss: 0.2923556844702111\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 242] Loss: 0.2923158195074137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 243] Loss: 0.2923215698306065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 244] Loss: 0.29229820478362417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 245] Loss: 0.2923109760212007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 246] Loss: 0.2924202480879411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 247] Loss: 0.2924099645987447\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 248] Loss: 0.29237392047577104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 249] Loss: 0.2923707004468776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 250] Loss: 0.29233174029258524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 251] Loss: 0.292328660233996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 252] Loss: 0.2923362396645106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 253] Loss: 0.29236833395124745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 254] Loss: 0.2923368435726805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 255] Loss: 0.29237272457600877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 256] Loss: 0.29244493343741923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 257] Loss: 0.2924646370842982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 258] Loss: 0.2924291687448877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 259] Loss: 0.29241553553298877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 260] Loss: 0.29239573837653204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 261] Loss: 0.2924359047515894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 262] Loss: 0.29243826920888233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 263] Loss: 0.29240820718747645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 264] Loss: 0.2923881654875383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 265] Loss: 0.29241232529143885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 266] Loss: 0.29238520085509645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 267] Loss: 0.2923766255104544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 268] Loss: 0.2923380554310666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 269] Loss: 0.2923708423744255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 270] Loss: 0.292346655846812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 271] Loss: 0.29233669573039067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 272] Loss: 0.2923523420618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 273] Loss: 0.2924188770001495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 274] Loss: 0.29242752504271236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 275] Loss: 0.29239835878613074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 276] Loss: 0.2923624088347352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 277] Loss: 0.29234905154349167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 278] Loss: 0.2923748930808612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 279] Loss: 0.29235097516852815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 280] Loss: 0.2923328821668305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 281] Loss: 0.29232117302533006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 282] Loss: 0.2923390333758021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 283] Loss: 0.2923461060495861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 284] Loss: 0.29234750598797987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 285] Loss: 0.29232976485262446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 286] Loss: 0.2923132236295148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 287] Loss: 0.2923368655708142\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 288] Loss: 0.2923341131044077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 289] Loss: 0.2923052347146157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 290] Loss: 0.2923394774719153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 291] Loss: 0.29234128470830134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 292] Loss: 0.2923926184684075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 293] Loss: 0.2923965990595457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 294] Loss: 0.292379101212933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 295] Loss: 0.29234433425753115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 296] Loss: 0.2923547314029023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 297] Loss: 0.2924138686836121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 298] Loss: 0.2924342718183529\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 299] Loss: 0.2924378027301301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 300] Loss: 0.29240950117579967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 301] Loss: 0.2924089859173029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 302] Loss: 0.2923929178047381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 303] Loss: 0.2924201275374325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 304] Loss: 0.2924210441069105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 305] Loss: 0.2923927856485049\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 306] Loss: 0.29239609613984063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 307] Loss: 0.2923818958775416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 308] Loss: 0.2924211072632772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 309] Loss: 0.2924657203554863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 310] Loss: 0.29245975804591523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 311] Loss: 0.29249642275409654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 312] Loss: 0.29247524043402434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 313] Loss: 0.2924603221008408\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 314] Loss: 0.29248619083403615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 315] Loss: 0.2925141409222521\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 316] Loss: 0.2925041028166154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 317] Loss: 0.29250094305677404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 318] Loss: 0.29250226818701885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 319] Loss: 0.29247638774438733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 320] Loss: 0.29256597223914477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 321] Loss: 0.29259234872016165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 322] Loss: 0.2925784558724865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 323] Loss: 0.29256240840359454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 324] Loss: 0.29256060303420645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 325] Loss: 0.2925523684798636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 326] Loss: 0.2925157616983223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 327] Loss: 0.29252208440230043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 328] Loss: 0.29250200000154625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 329] Loss: 0.29248083921221235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 330] Loss: 0.2925233135524453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 331] Loss: 0.29250556091749286\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 332] Loss: 0.2924994039598143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 333] Loss: 0.29247458315451197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 334] Loss: 0.2924973028157435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 335] Loss: 0.2924792029335715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 336] Loss: 0.29255938758527655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 337] Loss: 0.2925612553589719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 338] Loss: 0.2925278198994095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 339] Loss: 0.2925031531574066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 340] Loss: 0.2925035461525446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 341] Loss: 0.2924992076398714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 342] Loss: 0.29247024697357793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 343] Loss: 0.29247779123759576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 344] Loss: 0.2924469147540104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 345] Loss: 0.2924578812257562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 346] Loss: 0.2924721131678686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 347] Loss: 0.29248993806215184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 348] Loss: 0.29247043833997055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 349] Loss: 0.29247331511873764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 350] Loss: 0.29251390503401437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 351] Loss: 0.29250264269983206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 352] Loss: 0.2924838161178475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 353] Loss: 0.2924515550444215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 354] Loss: 0.2924475095436812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 355] Loss: 0.2924261942626861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 356] Loss: 0.29242563396057025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 357] Loss: 0.2924389978672274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 358] Loss: 0.2924278040443753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 359] Loss: 0.29240642390203514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 360] Loss: 0.292419731325298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 361] Loss: 0.29244070345714546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 362] Loss: 0.29243717700188615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 363] Loss: 0.29243477306445953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 364] Loss: 0.2924655723409296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 365] Loss: 0.29242921417396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 366] Loss: 0.2924146596991383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 367] Loss: 0.29239111995833444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 368] Loss: 0.29241761485089796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 369] Loss: 0.29246229521808026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 370] Loss: 0.29245724563566056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 371] Loss: 0.2924981748534827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 372] Loss: 0.292467813950917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 373] Loss: 0.29243379471521735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 374] Loss: 0.29248061186172863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 375] Loss: 0.2924942867775778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 376] Loss: 0.2924659624379437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 377] Loss: 0.2924491353927898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 378] Loss: 0.2924377120468124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 379] Loss: 0.2924517160592043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 380] Loss: 0.29243884488639205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 381] Loss: 0.2924610357803062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 382] Loss: 0.29248896883183756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 383] Loss: 0.29248687338370544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 384] Loss: 0.2924672784295037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 385] Loss: 0.2925009354798633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 386] Loss: 0.29254530430218934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 387] Loss: 0.2925609353195522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 388] Loss: 0.29256746204612766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 389] Loss: 0.2925682649899813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 390] Loss: 0.29257443998339433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 391] Loss: 0.29257060911951704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 392] Loss: 0.2925790206201492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 393] Loss: 0.29258027096747535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 394] Loss: 0.29254229188619796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 395] Loss: 0.29254710173228715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 396] Loss: 0.29252841128422713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 397] Loss: 0.29257355000840984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 398] Loss: 0.2926499545649174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 399] Loss: 0.2926211302553079\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 400] Loss: 0.2925956766745094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 401] Loss: 0.2925882763835391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 402] Loss: 0.2926040900614913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 403] Loss: 0.2926087715368431\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 404] Loss: 0.29258939802532796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 405] Loss: 0.29256499624282273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 406] Loss: 0.2925657779232889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 407] Loss: 0.2925870741556777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 408] Loss: 0.2925798226239189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 409] Loss: 0.2925869201989532\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 410] Loss: 0.2926173921046128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 411] Loss: 0.29265124086927063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 412] Loss: 0.29265666180426747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 413] Loss: 0.29268852807618806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 414] Loss: 0.2926724992922685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 415] Loss: 0.292660609681305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 416] Loss: 0.29268675777916203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 417] Loss: 0.2926736079068253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 418] Loss: 0.2926459112566834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 419] Loss: 0.29267182523721824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 420] Loss: 0.29267360499811795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 421] Loss: 0.2926424314818218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 422] Loss: 0.29265058038380143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 423] Loss: 0.2926398740143879\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 424] Loss: 0.2926248256154403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 425] Loss: 0.29262882384720507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 426] Loss: 0.29260257453328853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 427] Loss: 0.2925950957833826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 428] Loss: 0.2925634841431303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 429] Loss: 0.29255017367398223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 430] Loss: 0.2925575867697597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 431] Loss: 0.2926011437628775\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8861\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 432] Loss: 0.2926077987061816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 433] Loss: 0.29258786885441246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 434] Loss: 0.2926278531143396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 435] Loss: 0.2926521153165982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 436] Loss: 0.29266022464697283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 437] Loss: 0.29263803657030474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 438] Loss: 0.2926045895685046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 439] Loss: 0.2925916790626058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 440] Loss: 0.2926190480069367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 441] Loss: 0.29259928515709893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 442] Loss: 0.2926123803949722\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 443] Loss: 0.2926562176896573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 444] Loss: 0.292649776660319\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 445] Loss: 0.292710263768614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 446] Loss: 0.2927317117820635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 447] Loss: 0.2927270123454021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 448] Loss: 0.29278407515360333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 449] Loss: 0.2927484459734148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 450] Loss: 0.29274616645300294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 451] Loss: 0.2927538351565926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 452] Loss: 0.2927349727246235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 453] Loss: 0.2927494223848677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 454] Loss: 0.2927467425333063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 455] Loss: 0.2927053403958159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 456] Loss: 0.29270969616331477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 457] Loss: 0.29268899078139826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 458] Loss: 0.2927335276681109\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 459] Loss: 0.2927247425920922\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 460] Loss: 0.292732158064783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 461] Loss: 0.2927328353061683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 462] Loss: 0.2927422583420037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 463] Loss: 0.29274741508408614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 464] Loss: 0.2928101673654185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 465] Loss: 0.2927941482066516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 466] Loss: 0.2928360647982766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 467] Loss: 0.29285108508105456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 468] Loss: 0.2928216788738836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 469] Loss: 0.29286075836462644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 470] Loss: 0.29287484684249476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 471] Loss: 0.2928657216171238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 472] Loss: 0.29289348631098316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 473] Loss: 0.29288602962826965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 474] Loss: 0.29290509954963434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 475] Loss: 0.29290329985416563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 476] Loss: 0.29296799611375535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 477] Loss: 0.29302088162090983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 478] Loss: 0.2930062006252763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 479] Loss: 0.29297259694179656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 480] Loss: 0.2929766369795445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 481] Loss: 0.29294310714750804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 482] Loss: 0.2929076195288205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 483] Loss: 0.2928744166936116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 484] Loss: 0.2928487390580506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 485] Loss: 0.2929149790905585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 486] Loss: 0.2929731563140367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 487] Loss: 0.29299035540194246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 488] Loss: 0.2930025992839173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 489] Loss: 0.2929772982886715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 490] Loss: 0.29297388633013716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 491] Loss: 0.2929436511742317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 492] Loss: 0.29291594198031745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 493] Loss: 0.29289759815904715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 494] Loss: 0.29294094337388804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 495] Loss: 0.2930325846563793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 496] Loss: 0.2930123639251468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 497] Loss: 0.29298588852334684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 498] Loss: 0.293009535072117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 499] Loss: 0.29298864256514906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 500] Loss: 0.29301968424983277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 501] Loss: 0.29299948240344376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 502] Loss: 0.2930034829233112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 503] Loss: 0.2929831858472478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 504] Loss: 0.2929776669294523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 505] Loss: 0.29295071025493696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 506] Loss: 0.2929931515761724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 507] Loss: 0.29296168098246367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 508] Loss: 0.29294378173402935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 509] Loss: 0.29293576012589045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 510] Loss: 0.29289781026923517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 511] Loss: 0.29288677491293114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 512] Loss: 0.2929819039648581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 513] Loss: 0.29298007108172913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 514] Loss: 0.29299660969487473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 515] Loss: 0.29297549137177137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 516] Loss: 0.29302853890674535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 517] Loss: 0.29305309779445526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 518] Loss: 0.2930379603687768\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 519] Loss: 0.29307051882730484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 520] Loss: 0.2930666286769105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 521] Loss: 0.29313291953058895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 522] Loss: 0.2931271885263456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 523] Loss: 0.29315167412545107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 524] Loss: 0.2931582546355236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 525] Loss: 0.29319594447904057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 526] Loss: 0.2931809347558597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 527] Loss: 0.2931664356878161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 528] Loss: 0.29317077307016676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 529] Loss: 0.2932185795093125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 530] Loss: 0.29321019086329375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 531] Loss: 0.293200161844825\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 532] Loss: 0.29317396668179135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 533] Loss: 0.2931871079470559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 534] Loss: 0.29320635408787843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 535] Loss: 0.29322636958054765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 536] Loss: 0.2932139311746037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 537] Loss: 0.29318862135586077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 538] Loss: 0.2931757763176923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 539] Loss: 0.293186659411538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 540] Loss: 0.2931954304693146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 541] Loss: 0.2931967470477855\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 542] Loss: 0.29331793662755284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 543] Loss: 0.29331211214304065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 544] Loss: 0.29330937637655025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 545] Loss: 0.2933725967012775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 546] Loss: 0.2933570651309834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 547] Loss: 0.29336662319824985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 548] Loss: 0.2933570940168331\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 549] Loss: 0.2933657247108113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 550] Loss: 0.29333124899276297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 551] Loss: 0.29331721708373476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 552] Loss: 0.29330562723219417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 553] Loss: 0.2932727771949168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 554] Loss: 0.2932551488829929\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 555] Loss: 0.29322400354902917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 556] Loss: 0.29322380838109746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 557] Loss: 0.2932212094852444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 558] Loss: 0.29320498076289336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 559] Loss: 0.29317293040124637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 560] Loss: 0.2931634540809409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 561] Loss: 0.2931424855062009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 562] Loss: 0.29316871812013984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 563] Loss: 0.29320831598448005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 564] Loss: 0.2932465978831715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 565] Loss: 0.2932227922115609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 566] Loss: 0.29328815691968035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 567] Loss: 0.29334430764227126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 568] Loss: 0.2933808403961874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 569] Loss: 0.2933756663589237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 570] Loss: 0.29334099587589385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 571] Loss: 0.29331850539186405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 572] Loss: 0.2933204882730774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 573] Loss: 0.29329509307050344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 574] Loss: 0.29326354371004304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 575] Loss: 0.29326459650743114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 576] Loss: 0.2932974962677605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 577] Loss: 0.29329892347362496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 578] Loss: 0.2932848367657695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 579] Loss: 0.29330763418736056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 580] Loss: 0.293380142722315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 581] Loss: 0.29336071530202595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 582] Loss: 0.29339840636525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 583] Loss: 0.2933618378088633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 584] Loss: 0.2934012287258408\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 585] Loss: 0.2934041231090183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 586] Loss: 0.2933935858280368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 587] Loss: 0.29343050672653365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 588] Loss: 0.29340343078805664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 589] Loss: 0.29343958379836893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 590] Loss: 0.29342763459439886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 591] Loss: 0.29344021147696275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 592] Loss: 0.2934147276241204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 593] Loss: 0.2934315378395088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 594] Loss: 0.29343823787029816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 595] Loss: 0.2934171187652623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 596] Loss: 0.29339257591381307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 597] Loss: 0.2933815731309197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 598] Loss: 0.29344648648710814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 599] Loss: 0.2934305187949684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 600] Loss: 0.2934119425267269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 601] Loss: 0.29338282852748265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 602] Loss: 0.29346360720273423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 603] Loss: 0.2934284135411216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 604] Loss: 0.29341452410801017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 605] Loss: 0.2933972925451643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 606] Loss: 0.29340315962115293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 607] Loss: 0.2934134399523048\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 608] Loss: 0.2934034388050489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 609] Loss: 0.2933898407301978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 610] Loss: 0.29337045505901793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 611] Loss: 0.2934096475404861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 612] Loss: 0.29338423541266456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 613] Loss: 0.2933548128017813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 614] Loss: 0.2933581272344231\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 615] Loss: 0.29335902284259624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 616] Loss: 0.2933317885001079\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 617] Loss: 0.2932973687020092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 618] Loss: 0.29326433833019266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 619] Loss: 0.2932681102606459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 620] Loss: 0.2932809766188178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 621] Loss: 0.293256194996905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 622] Loss: 0.29325919392416816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 623] Loss: 0.2932870005315355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 624] Loss: 0.2933111428012401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 625] Loss: 0.29330474584147376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 626] Loss: 0.2932910679770007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 627] Loss: 0.29331395697950374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 628] Loss: 0.2932921148544208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 629] Loss: 0.29331868243924836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 630] Loss: 0.2932997930006912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 631] Loss: 0.29328559750325406\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 632] Loss: 0.2932687558245196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 633] Loss: 0.29326092581834423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 634] Loss: 0.29329681707593364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 635] Loss: 0.2933110038740033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 636] Loss: 0.2932780431243625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 637] Loss: 0.2932510169758457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 638] Loss: 0.29324355540073227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 639] Loss: 0.29321197394755777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 640] Loss: 0.29318195683542003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 641] Loss: 0.2931702993173925\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 642] Loss: 0.2931605977644531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 643] Loss: 0.29312601342651495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 644] Loss: 0.2931088307335841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 645] Loss: 0.2931098535485487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 646] Loss: 0.2930890390509454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 647] Loss: 0.29305730463188223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 648] Loss: 0.29306370468689547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 649] Loss: 0.2931070768583618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 650] Loss: 0.2930884648771007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 651] Loss: 0.293078614927199\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 652] Loss: 0.29308785370161183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 653] Loss: 0.2931630069052217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 654] Loss: 0.29319992343000895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 655] Loss: 0.2932666351450117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 656] Loss: 0.2932491156144636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 657] Loss: 0.2932231003934917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 658] Loss: 0.29320416555760603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 659] Loss: 0.29318922057499913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 660] Loss: 0.29319327210234636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 661] Loss: 0.2931778925303473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 662] Loss: 0.29316707995979385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 663] Loss: 0.2931680009851099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 664] Loss: 0.2931458900547752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 665] Loss: 0.2931630147434237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 666] Loss: 0.29313416494593036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 667] Loss: 0.293101828783631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 668] Loss: 0.29312009698490504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 669] Loss: 0.2931014468356907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 670] Loss: 0.2931374086010206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 671] Loss: 0.2931539709744643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 672] Loss: 0.29314938204228674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 673] Loss: 0.2931703186256272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 674] Loss: 0.2931564538934979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 675] Loss: 0.2931360173938499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 676] Loss: 0.29316612586572205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 677] Loss: 0.29315439528463133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 678] Loss: 0.2931191375728969\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 679] Loss: 0.29311694382235853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 680] Loss: 0.29310951646169026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 681] Loss: 0.29309045109231685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 682] Loss: 0.2930788913257959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 683] Loss: 0.29304239819918093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 684] Loss: 0.2930253956099631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 685] Loss: 0.2930104992227482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 686] Loss: 0.2930620229651349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 687] Loss: 0.29307715412791246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 688] Loss: 0.2930397836228612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 689] Loss: 0.2930445374503383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 690] Loss: 0.29310996154245317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 691] Loss: 0.29310411081423304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 692] Loss: 0.29313007188257906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 693] Loss: 0.2931011330117614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 694] Loss: 0.29311396798407685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 695] Loss: 0.29309016174344027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 696] Loss: 0.29306454632814694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 697] Loss: 0.29305740748996273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 698] Loss: 0.29306272471616374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 699] Loss: 0.29304797694576284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 700] Loss: 0.293083967144055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 701] Loss: 0.2930809513025595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 702] Loss: 0.29307135279616836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 703] Loss: 0.2930674918734912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 704] Loss: 0.29306123601212664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 705] Loss: 0.2930513575602482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 706] Loss: 0.2930563856579979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 707] Loss: 0.2930925868413474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 708] Loss: 0.29306222429247053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 709] Loss: 0.2930645677267229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 710] Loss: 0.2930520880939157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 711] Loss: 0.29305487662649726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 712] Loss: 0.2930479415215267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 713] Loss: 0.2930427107698376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 714] Loss: 0.2930401406764849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 715] Loss: 0.29303522878868354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 716] Loss: 0.2930180363630509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 717] Loss: 0.2930050642538413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 718] Loss: 0.2930130577433443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 719] Loss: 0.2930006150048495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 720] Loss: 0.2929717510713296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 721] Loss: 0.2929490116898177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 722] Loss: 0.29294103371910046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 723] Loss: 0.29293905329765096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 724] Loss: 0.2929195722752938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 725] Loss: 0.29289328322813574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 726] Loss: 0.2928798829326322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 727] Loss: 0.2928880874826387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 728] Loss: 0.29289271065978656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 729] Loss: 0.29296663668126877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 730] Loss: 0.29296460052987316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 731] Loss: 0.29297061060435636\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 732] Loss: 0.2929586273758918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 733] Loss: 0.292952382767423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 734] Loss: 0.29293410125754626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 735] Loss: 0.29298606437686253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 736] Loss: 0.292976288569342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 737] Loss: 0.29296604261596887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 738] Loss: 0.2929653862734082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 739] Loss: 0.2929523267591487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 740] Loss: 0.2929244055866971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 741] Loss: 0.292918545690056\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 742] Loss: 0.2929463343279638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 743] Loss: 0.2929908435166993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 744] Loss: 0.2930013787703807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 745] Loss: 0.2930129196827746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 746] Loss: 0.29298585669282595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 747] Loss: 0.29298270155337824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 748] Loss: 0.29300825943698916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 749] Loss: 0.29303976010113064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 750] Loss: 0.2930132126841778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 751] Loss: 0.2929980724775529\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 752] Loss: 0.2930020311919427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 753] Loss: 0.2929698872262209\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 754] Loss: 0.29295018080350466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 755] Loss: 0.292924973683458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 756] Loss: 0.29293513877042426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 757] Loss: 0.29292418113737456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 758] Loss: 0.2929028776507858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 759] Loss: 0.2928942120945975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 760] Loss: 0.2928929308132316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 761] Loss: 0.2928716208309891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 762] Loss: 0.29286187678010994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 763] Loss: 0.29285378411437385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 764] Loss: 0.2928440914823515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 765] Loss: 0.29285478195718434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 766] Loss: 0.2928461220207578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 767] Loss: 0.2928488020391998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 768] Loss: 0.29284082862707994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 769] Loss: 0.2928337026642132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 770] Loss: 0.2928437588575962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 771] Loss: 0.2928625178990505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 772] Loss: 0.29283436195445794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 773] Loss: 0.2928422140454322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 774] Loss: 0.29282576205281396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 775] Loss: 0.2927940661782857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 776] Loss: 0.29280467688329337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 777] Loss: 0.2928644180902593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 778] Loss: 0.2928654610756826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 779] Loss: 0.29286785682137156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 780] Loss: 0.29285533401592867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 781] Loss: 0.2928223595819592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 782] Loss: 0.2927981818078144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 783] Loss: 0.29282941906953913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 784] Loss: 0.29281128018253944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 785] Loss: 0.29278620751826817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 786] Loss: 0.29279654846847325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 787] Loss: 0.292766652789905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 788] Loss: 0.2927383412886863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 789] Loss: 0.29272286020974253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 790] Loss: 0.2927089410166798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 791] Loss: 0.29269860761726196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 792] Loss: 0.2926752834675673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 793] Loss: 0.2926555342754165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 794] Loss: 0.2926616224152641\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 795] Loss: 0.29265445643985594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 796] Loss: 0.2926449626781382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 797] Loss: 0.29264413229170205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 798] Loss: 0.2926441868017643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 799] Loss: 0.2926186202255279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 800] Loss: 0.29259431441652706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 801] Loss: 0.29262877159962286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 802] Loss: 0.29261301998368017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 803] Loss: 0.29258107484252277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 804] Loss: 0.29259122257333375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 805] Loss: 0.29256598940838213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 806] Loss: 0.2925833110570065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 807] Loss: 0.29260551209218755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 808] Loss: 0.292610674966286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 809] Loss: 0.29258598758454524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 810] Loss: 0.29257860169036237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 811] Loss: 0.29256975152605064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 812] Loss: 0.29256703268801665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 813] Loss: 0.29254027846221387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 814] Loss: 0.2925228533344717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 815] Loss: 0.29251714321398137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 816] Loss: 0.29251749019805623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 817] Loss: 0.2925430464710268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 818] Loss: 0.2925156168390754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 819] Loss: 0.292492538690567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 820] Loss: 0.29246394243333246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 821] Loss: 0.29247206307987716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 822] Loss: 0.29251588564863545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 823] Loss: 0.29251843719607973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 824] Loss: 0.29248966977226765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 825] Loss: 0.29249946918611286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 826] Loss: 0.29249287043747146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 827] Loss: 0.2925239630555674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 828] Loss: 0.2925159660378135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 829] Loss: 0.29256231289899975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 830] Loss: 0.29256391225589645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 831] Loss: 0.2925385522706666\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 832] Loss: 0.2925238072381157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 833] Loss: 0.29251521906322314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 834] Loss: 0.292519188821757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 835] Loss: 0.2925037899522402\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 836] Loss: 0.2924700955296286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 837] Loss: 0.2924750044360407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 838] Loss: 0.2924786874991373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 839] Loss: 0.29247962443351216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 840] Loss: 0.29249193846848887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 841] Loss: 0.29249565472688327\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 842] Loss: 0.2925049954996405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 843] Loss: 0.2925149292364901\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 844] Loss: 0.2925353975017704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 845] Loss: 0.29253709791769944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 846] Loss: 0.29253456748724865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 847] Loss: 0.2925375639258968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 848] Loss: 0.2925555795629159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 849] Loss: 0.29253926266051455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 850] Loss: 0.292537114793964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 851] Loss: 0.29253572492202784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 852] Loss: 0.29255910024832904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 853] Loss: 0.2925384613325867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 854] Loss: 0.292510153577391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 855] Loss: 0.2925029788698469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 856] Loss: 0.2925097879188508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 857] Loss: 0.2924955434423137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 858] Loss: 0.2924846534012606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 859] Loss: 0.2924452269411061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 860] Loss: 0.29243212680351227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 861] Loss: 0.29241985844832674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 862] Loss: 0.29242407750552285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 863] Loss: 0.29245777824472247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 864] Loss: 0.2924784062691458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 865] Loss: 0.2924489718242424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 866] Loss: 0.29244291035676373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 867] Loss: 0.2924226980744773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 868] Loss: 0.292429137466672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 869] Loss: 0.29240775343402603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 870] Loss: 0.2924257993369418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 871] Loss: 0.29240240173356497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 872] Loss: 0.2923817822804831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 873] Loss: 0.29238913192504606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 874] Loss: 0.2923649432278573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 875] Loss: 0.29233059352203944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 876] Loss: 0.2923088571439103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 877] Loss: 0.29231136458410745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 878] Loss: 0.2922744726524216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 879] Loss: 0.29225675405920437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 880] Loss: 0.2922347144194699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 881] Loss: 0.29221145528223597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 882] Loss: 0.29219634808813133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 883] Loss: 0.2921938989936551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 884] Loss: 0.2921835338901066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 885] Loss: 0.29217019750916773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 886] Loss: 0.29218881718180745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 887] Loss: 0.29217821705334784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 888] Loss: 0.29216238676671713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 889] Loss: 0.2921409381246104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 890] Loss: 0.29212893576194077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 891] Loss: 0.2921368961226392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 892] Loss: 0.2921529707019035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 893] Loss: 0.2921662433187145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 894] Loss: 0.2921785802947218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 895] Loss: 0.2922443333996182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 896] Loss: 0.2922568804902381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 897] Loss: 0.2923086318571563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 898] Loss: 0.29230175827900334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 899] Loss: 0.2923505119059809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 900] Loss: 0.29235910821222516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 901] Loss: 0.2923716772868589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 902] Loss: 0.2923577911307632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 903] Loss: 0.2923295110159157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 904] Loss: 0.29233473763931483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 905] Loss: 0.2923118701781312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 906] Loss: 0.29235298689686096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 907] Loss: 0.29233533675890017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 908] Loss: 0.2922959404012481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 909] Loss: 0.2923596626605555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 910] Loss: 0.29236386321947305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 911] Loss: 0.292352431071618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 912] Loss: 0.29232459201245403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 913] Loss: 0.29232057067391515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 914] Loss: 0.29237623931058865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 915] Loss: 0.29238282739708615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 916] Loss: 0.2923496078361658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 917] Loss: 0.2923657601569897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 918] Loss: 0.29233533624863745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 919] Loss: 0.29235955157671323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 920] Loss: 0.2923486534227873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 921] Loss: 0.29238868142613683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 922] Loss: 0.29241094368429227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 923] Loss: 0.2924046016593416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 924] Loss: 0.2923829417972726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 925] Loss: 0.29240093972869524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 926] Loss: 0.2923990059101729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 927] Loss: 0.2923930226720982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 928] Loss: 0.292370005414871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 929] Loss: 0.2923491261296542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 930] Loss: 0.29234088138287717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 931] Loss: 0.2923215369060243\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8954000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 932] Loss: 0.292328567252576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 933] Loss: 0.29233015306847715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 934] Loss: 0.29232758018567206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 935] Loss: 0.2923343991361673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 936] Loss: 0.29242830006882703\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 937] Loss: 0.2924749426271774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 938] Loss: 0.29249554135841394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 939] Loss: 0.2925037877743377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 940] Loss: 0.29250186802633443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 941] Loss: 0.29247423772034464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 942] Loss: 0.2924806907454016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 943] Loss: 0.292461854669742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 944] Loss: 0.2924488884439752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 945] Loss: 0.29243254154677406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 946] Loss: 0.2924109159349488\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 947] Loss: 0.2923861203530527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 948] Loss: 0.29238654607478237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 949] Loss: 0.292353621151951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 950] Loss: 0.2923868268849733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 951] Loss: 0.2923979708596624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 952] Loss: 0.2923877896543367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 953] Loss: 0.2923845119643488\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 954] Loss: 0.292374015384496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 955] Loss: 0.2923552935978406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 956] Loss: 0.2923722373678272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 957] Loss: 0.2923946707445932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 958] Loss: 0.2923773218919395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 959] Loss: 0.29238502381156894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 960] Loss: 0.29238416922108607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 961] Loss: 0.2923671442135034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 962] Loss: 0.2923682206125145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 963] Loss: 0.292391504975904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 964] Loss: 0.2923671187550896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 965] Loss: 0.2923671822116627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 966] Loss: 0.292352974844149\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 967] Loss: 0.2923268840653448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 968] Loss: 0.2923006268440816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 969] Loss: 0.2923108012864232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 970] Loss: 0.29228685080924405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 971] Loss: 0.2922905783893894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 972] Loss: 0.2923328403930658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 973] Loss: 0.29229754826614146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 974] Loss: 0.29229658163551775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 975] Loss: 0.2923027399090951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 976] Loss: 0.2923054819892362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 977] Loss: 0.29231684311904715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 978] Loss: 0.29234398043464005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 979] Loss: 0.29233370533498004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 980] Loss: 0.2923072417778475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 981] Loss: 0.2923165078923694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 982] Loss: 0.292290215666328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 983] Loss: 0.29229913532932494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 984] Loss: 0.29228283539360744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 985] Loss: 0.2922657274092027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 986] Loss: 0.29223995429280636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 987] Loss: 0.2922133386486379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 988] Loss: 0.2922832927605946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 989] Loss: 0.2922910017734231\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 990] Loss: 0.2922887700964971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 991] Loss: 0.29230557937450363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 992] Loss: 0.2922711581974975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 993] Loss: 0.2922612877856286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 994] Loss: 0.29224079993963886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 995] Loss: 0.2922417941105762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 996] Loss: 0.2922206234991379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 997] Loss: 0.2922057898252405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 998] Loss: 0.2922695380028059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 999] Loss: 0.29227781028871924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1000] Loss: 0.2923217851300749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1001] Loss: 0.2922940882475516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1002] Loss: 0.2922875640308117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1003] Loss: 0.29228449437988524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1004] Loss: 0.29228789694433494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1005] Loss: 0.2922762949704589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1006] Loss: 0.29228484524357823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1007] Loss: 0.2922836563678354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1008] Loss: 0.2922816231309855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1009] Loss: 0.292260953744192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1010] Loss: 0.2922308887418453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1011] Loss: 0.2922422556739579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1012] Loss: 0.2922309235656659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1013] Loss: 0.292280559284743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1014] Loss: 0.29226331464828853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1015] Loss: 0.29224758118044186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1016] Loss: 0.2922389529834842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1017] Loss: 0.2922557560413281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1018] Loss: 0.29222635840550437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1019] Loss: 0.2922219081094769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1020] Loss: 0.29223060814233925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1021] Loss: 0.2922185900929447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1022] Loss: 0.29222790063365506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1023] Loss: 0.2922292558521954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1024] Loss: 0.2922026876587931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1025] Loss: 0.2921915849289583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1026] Loss: 0.2921819216262195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1027] Loss: 0.2921925016880376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1028] Loss: 0.29219650676012804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1029] Loss: 0.29225325983621153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1030] Loss: 0.29227124106671126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1031] Loss: 0.29228859502935894\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1032] Loss: 0.29233875115804697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1033] Loss: 0.29229811736032324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1034] Loss: 0.2923138164818755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1035] Loss: 0.29229294014338614\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1036] Loss: 0.2922632485745083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1037] Loss: 0.2922576059307754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1038] Loss: 0.29223281011973823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1039] Loss: 0.2922036938992259\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1040] Loss: 0.29220180445335764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1041] Loss: 0.29218086394752074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1042] Loss: 0.29219290834512357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1043] Loss: 0.29217499217926746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1044] Loss: 0.29217187422193325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1045] Loss: 0.2921439703404638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1046] Loss: 0.2921560892193151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1047] Loss: 0.2921377595886124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1048] Loss: 0.2921957158615853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1049] Loss: 0.29218476460717885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1050] Loss: 0.29217221655773523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1051] Loss: 0.2922076715457654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1052] Loss: 0.29221070613228356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1053] Loss: 0.2921934165676654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1054] Loss: 0.2921743852723586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1055] Loss: 0.29216098062462276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1056] Loss: 0.29213948917852955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1057] Loss: 0.2921576950734179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1058] Loss: 0.2921376469642369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1059] Loss: 0.2921365356282244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1060] Loss: 0.2921157976503271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1061] Loss: 0.2921621191302618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1062] Loss: 0.2921355793765873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1063] Loss: 0.29212981971705254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1064] Loss: 0.29217739539458304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1065] Loss: 0.2921577059043226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1066] Loss: 0.29218404813860643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1067] Loss: 0.29216686960693705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1068] Loss: 0.29214228060375336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1069] Loss: 0.2921216610999453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1070] Loss: 0.29210595655261623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1071] Loss: 0.29210961554209364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1072] Loss: 0.2920842478484264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1073] Loss: 0.29206689122497204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1074] Loss: 0.2920429494778456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1075] Loss: 0.2920177899914785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1076] Loss: 0.29198998230497253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1077] Loss: 0.2919989035855527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1078] Loss: 0.2919759385672955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1079] Loss: 0.29197883069156483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1080] Loss: 0.2919801386083122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1081] Loss: 0.2919608137841099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1082] Loss: 0.2919555639390691\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1083] Loss: 0.29196370739009386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1084] Loss: 0.29193793805161256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1085] Loss: 0.2919618053340364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1086] Loss: 0.2919939051618038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1087] Loss: 0.29206392307816326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1088] Loss: 0.2920392915427411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1089] Loss: 0.2920284493506892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1090] Loss: 0.2920478305131092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1091] Loss: 0.2920475918888477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1092] Loss: 0.2920884097482534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1093] Loss: 0.2920750454897049\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1094] Loss: 0.2920771564388553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1095] Loss: 0.2920561283484877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1096] Loss: 0.2920204743038123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1097] Loss: 0.29202711403643367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1098] Loss: 0.29203873707918776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1099] Loss: 0.29205290903793185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1100] Loss: 0.29203427059730314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1101] Loss: 0.2920113460577432\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1102] Loss: 0.29200368282636063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1103] Loss: 0.29198822538814445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1104] Loss: 0.29202114484198377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1105] Loss: 0.2919977980090658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1106] Loss: 0.29202726493846204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1107] Loss: 0.2920142827565052\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1108] Loss: 0.29200206237460324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1109] Loss: 0.29201097570741485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1110] Loss: 0.291998435139761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1111] Loss: 0.2919753972749814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1112] Loss: 0.2919501204257362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1113] Loss: 0.2919349531650795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1114] Loss: 0.2919285797489772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1115] Loss: 0.29193481694425316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1116] Loss: 0.29193570464587404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1117] Loss: 0.29193891561029245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1118] Loss: 0.29196235692618433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1119] Loss: 0.29195954404033997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1120] Loss: 0.2919759344499434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1121] Loss: 0.2920597390798835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1122] Loss: 0.29204920040298243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1123] Loss: 0.29204951155516545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1124] Loss: 0.29210772283158054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1125] Loss: 0.29210417775389813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1126] Loss: 0.2921258774769147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1127] Loss: 0.2921132066777365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1128] Loss: 0.2920924657874042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1129] Loss: 0.2920675236450034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1130] Loss: 0.2920548926529131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1131] Loss: 0.2920371681279288\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1132] Loss: 0.29203247420832135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1133] Loss: 0.2920232434429956\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1134] Loss: 0.2919961753736288\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1135] Loss: 0.2919764484782474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1136] Loss: 0.29196232278650786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1137] Loss: 0.29196181516576736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1138] Loss: 0.29193660730173643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1139] Loss: 0.2919546287590995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1140] Loss: 0.2919753101682078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1141] Loss: 0.29200559076964805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1142] Loss: 0.2919977611316895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1143] Loss: 0.2919880543592075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1144] Loss: 0.29197069928754155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1145] Loss: 0.2919934607631578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1146] Loss: 0.2919763350081118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1147] Loss: 0.29198213176531707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1148] Loss: 0.2919711967335376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1149] Loss: 0.29198499165656455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1150] Loss: 0.29199988043547925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1151] Loss: 0.2920406712552839\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1152] Loss: 0.2920522562761025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1153] Loss: 0.2920940429670045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1154] Loss: 0.2920799410630572\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1155] Loss: 0.292086598144348\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1156] Loss: 0.29206184387581774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1157] Loss: 0.2920601572877182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1158] Loss: 0.2921158873668596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1159] Loss: 0.292094657206373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1160] Loss: 0.29211044355212706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1161] Loss: 0.2921372087000553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1162] Loss: 0.2921194705828785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1163] Loss: 0.29212326414918227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1164] Loss: 0.2921254995717876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1165] Loss: 0.29214553359600387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1166] Loss: 0.29215207008574795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1167] Loss: 0.2921444525156824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1168] Loss: 0.2921224038810338\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1169] Loss: 0.29213101861991625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1170] Loss: 0.29215563026870167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1171] Loss: 0.29221827758353175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1172] Loss: 0.29224793222132395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1173] Loss: 0.29224971241907816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1174] Loss: 0.29225091677169446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1175] Loss: 0.29223332897905063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1176] Loss: 0.29221708543295233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1177] Loss: 0.2922010494223341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1178] Loss: 0.2921903715024993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1179] Loss: 0.29216064316129703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1180] Loss: 0.2921494412072327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1181] Loss: 0.29212464359926565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1182] Loss: 0.2921447883138591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1183] Loss: 0.29215603919857464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1184] Loss: 0.29212656234535733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1185] Loss: 0.2921462758184515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1186] Loss: 0.2921654217947039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1187] Loss: 0.2921563199326067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1188] Loss: 0.2921314931236215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1189] Loss: 0.2921383674760249\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1190] Loss: 0.2921517439691039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1191] Loss: 0.29215826736780903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1192] Loss: 0.29212364282734443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1193] Loss: 0.2921315801538218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1194] Loss: 0.2921023310810369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1195] Loss: 0.2921159893235536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1196] Loss: 0.29220899731628897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1197] Loss: 0.2921958835894032\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1198] Loss: 0.29221657067976486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1199] Loss: 0.2922786878579141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1200] Loss: 0.29225799437334676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1201] Loss: 0.292246155665448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1202] Loss: 0.2922267556714561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1203] Loss: 0.29222273219376155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1204] Loss: 0.2922490556457586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1205] Loss: 0.29224551184223846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1206] Loss: 0.2922529920282931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1207] Loss: 0.29222807427328185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1208] Loss: 0.29222088912292393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1209] Loss: 0.29220447344806993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1210] Loss: 0.2922249692108701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1211] Loss: 0.29221114836526235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1212] Loss: 0.29221910159018205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1213] Loss: 0.29224541047035607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1214] Loss: 0.2922303607342134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1215] Loss: 0.29221186268375987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1216] Loss: 0.2922276964316494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1217] Loss: 0.29223640526367917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1218] Loss: 0.2922470025916448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1219] Loss: 0.29230585070235227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1220] Loss: 0.29230268693726913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1221] Loss: 0.292331654333354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1222] Loss: 0.29234029481172025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1223] Loss: 0.2923351617614552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1224] Loss: 0.292315079747324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1225] Loss: 0.2923030832070436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1226] Loss: 0.29228003396992647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1227] Loss: 0.29227439595596205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1228] Loss: 0.2922646982743745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1229] Loss: 0.2922466171712357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1230] Loss: 0.2922337828769252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1231] Loss: 0.2922109375417058\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1232] Loss: 0.29218765466978985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1233] Loss: 0.2922036007416468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1234] Loss: 0.2922878201439884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1235] Loss: 0.2922887304225017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1236] Loss: 0.2922981724135329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1237] Loss: 0.2923091868471418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1238] Loss: 0.2923397173060838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1239] Loss: 0.29234778561835695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1240] Loss: 0.2923677526516984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1241] Loss: 0.29238723546799933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1242] Loss: 0.2923652508192877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1243] Loss: 0.29235278675324655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1244] Loss: 0.2923421208706044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1245] Loss: 0.2923391090073364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1246] Loss: 0.29235245725665204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1247] Loss: 0.2923639478853258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1248] Loss: 0.29234432462755244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1249] Loss: 0.2923349734676788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1250] Loss: 0.29231350241785814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1251] Loss: 0.292305347140897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1252] Loss: 0.2922770197473515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1253] Loss: 0.2922917896340787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1254] Loss: 0.2922823021129531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1255] Loss: 0.2923163462056111\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1256] Loss: 0.29231718952559416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1257] Loss: 0.2922905112672818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1258] Loss: 0.29231436005886197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1259] Loss: 0.29230329784316866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1260] Loss: 0.2923182967788257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1261] Loss: 0.2923155411735466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1262] Loss: 0.2923081583740759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1263] Loss: 0.29230500465061293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1264] Loss: 0.2923417013811562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1265] Loss: 0.2923680744714071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1266] Loss: 0.2923679135017214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1267] Loss: 0.2923529560864227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1268] Loss: 0.29241402220183876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1269] Loss: 0.29239481291091696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1270] Loss: 0.2923980523117703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1271] Loss: 0.29238061886870725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1272] Loss: 0.2923578386198511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1273] Loss: 0.29235473875388\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1274] Loss: 0.29235170521913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1275] Loss: 0.29235278093044936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1276] Loss: 0.29232245195673584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1277] Loss: 0.2922993247335602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1278] Loss: 0.29231769049721074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1279] Loss: 0.2923460208066795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1280] Loss: 0.2923283369711831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1281] Loss: 0.29232366419876565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1282] Loss: 0.29234575709774935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1283] Loss: 0.2923164806525544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1284] Loss: 0.29228745622233926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1285] Loss: 0.29227382765810883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1286] Loss: 0.29225130339122574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1287] Loss: 0.2922916962465105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1288] Loss: 0.2923119777960454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1289] Loss: 0.29228702696324166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1290] Loss: 0.29227886380621393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1291] Loss: 0.29226259917854713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1292] Loss: 0.2922893382446203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1293] Loss: 0.2923274930110975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1294] Loss: 0.2923021827840931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1295] Loss: 0.2922974666201042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1296] Loss: 0.2923214550994075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1297] Loss: 0.29232311925262044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1298] Loss: 0.2923147867445789\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1299] Loss: 0.29231627993469733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1300] Loss: 0.29230712416250215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1301] Loss: 0.29228287017862187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1302] Loss: 0.29228869058085194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1303] Loss: 0.29227340902017407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1304] Loss: 0.2922542126501118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1305] Loss: 0.2922460338422593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1306] Loss: 0.2922950943195248\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1307] Loss: 0.29230247411965027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1308] Loss: 0.29228365895734143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1309] Loss: 0.2922746088342473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1310] Loss: 0.29225882908494094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1311] Loss: 0.2922300907348192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1312] Loss: 0.2921989928957598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1313] Loss: 0.2921880993731833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1314] Loss: 0.29224005434472167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1315] Loss: 0.2922115634724865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1316] Loss: 0.29225166777298844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1317] Loss: 0.29223130745754716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1318] Loss: 0.29223134187834704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1319] Loss: 0.29222782293869454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1320] Loss: 0.29219253366917425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1321] Loss: 0.29222161099479105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1322] Loss: 0.2922106364657198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1323] Loss: 0.29223531048290396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1324] Loss: 0.2922461876031966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1325] Loss: 0.29222997029496006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1326] Loss: 0.29223219871273315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1327] Loss: 0.29222086924419616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1328] Loss: 0.2922182843637349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1329] Loss: 0.29222987105829634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1330] Loss: 0.29219799369952437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1331] Loss: 0.29218323250364153\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1332] Loss: 0.2922817544045286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1333] Loss: 0.2922893612705993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1334] Loss: 0.2923594200819589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1335] Loss: 0.2923530404505641\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1336] Loss: 0.29234045238195944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1337] Loss: 0.2923810205330739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1338] Loss: 0.29236012709200304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1339] Loss: 0.29233685041905294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1340] Loss: 0.29233801691539196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1341] Loss: 0.2923798945171037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1342] Loss: 0.29238355310863146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1343] Loss: 0.2923922861515014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1344] Loss: 0.2923803010266961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1345] Loss: 0.29238158321214347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1346] Loss: 0.29238826057988904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1347] Loss: 0.29245029596993705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1348] Loss: 0.2924628060282123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1349] Loss: 0.292463306603822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1350] Loss: 0.29243457639917125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1351] Loss: 0.2924155593761478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1352] Loss: 0.29240903887760594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1353] Loss: 0.2924131629785714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1354] Loss: 0.29239262023577206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1355] Loss: 0.2923965712727877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1356] Loss: 0.2924392853288022\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1357] Loss: 0.2924566633304195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1358] Loss: 0.29252960497083574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1359] Loss: 0.2925241626277604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1360] Loss: 0.29250523109887344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1361] Loss: 0.2925284884228129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1362] Loss: 0.2925392199654336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1363] Loss: 0.2925053174019905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1364] Loss: 0.2924907843060955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1365] Loss: 0.29247966734872916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1366] Loss: 0.2924876523522956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1367] Loss: 0.29251351969368994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1368] Loss: 0.2925107815990186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1369] Loss: 0.2925024122867867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1370] Loss: 0.2925075706759286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1371] Loss: 0.29251236484480553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1372] Loss: 0.29249855549959014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1373] Loss: 0.29249146678482624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1374] Loss: 0.2925036675365664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1375] Loss: 0.29251692322646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1376] Loss: 0.2925006372736954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1377] Loss: 0.2925398037656929\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1378] Loss: 0.2925767111513706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1379] Loss: 0.29256674043670144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1380] Loss: 0.2925450355916464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1381] Loss: 0.29269140725916243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1382] Loss: 0.2926772043098926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1383] Loss: 0.2927004518067158\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1384] Loss: 0.2927048204427283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1385] Loss: 0.2926847327916588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1386] Loss: 0.2926696195217729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1387] Loss: 0.2927370210842176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1388] Loss: 0.29271089105287545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1389] Loss: 0.292715368014411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1390] Loss: 0.2927149898003812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1391] Loss: 0.29269629042183787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1392] Loss: 0.2927036422939958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1393] Loss: 0.29271100832825037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1394] Loss: 0.29273180595376125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1395] Loss: 0.29275038926569436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1396] Loss: 0.29277934841599357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1397] Loss: 0.2927965771513647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1398] Loss: 0.29279623148418543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1399] Loss: 0.2928502326171229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1400] Loss: 0.29284284250121223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1401] Loss: 0.2928650582471578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1402] Loss: 0.2928486858099719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1403] Loss: 0.29289689771837274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1404] Loss: 0.2928662155725244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1405] Loss: 0.2928307547841112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1406] Loss: 0.29280314084461617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1407] Loss: 0.29279119444093876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1408] Loss: 0.29278835451480295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1409] Loss: 0.2928035454801202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1410] Loss: 0.29283038780068277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1411] Loss: 0.2928292880130938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1412] Loss: 0.29281570481286523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1413] Loss: 0.29280344420988264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1414] Loss: 0.2928026364331836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1415] Loss: 0.29277058787637683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1416] Loss: 0.29278929456022923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1417] Loss: 0.29277487391245394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1418] Loss: 0.29275586591310593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1419] Loss: 0.29272980179023733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1420] Loss: 0.2927953296529132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1421] Loss: 0.2927753309863992\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1422] Loss: 0.2927856681948013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1423] Loss: 0.292782389527616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1424] Loss: 0.2927700470266618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1425] Loss: 0.2927889154280992\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1426] Loss: 0.2927793688796277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1427] Loss: 0.2927501739569757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1428] Loss: 0.2927530467666738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1429] Loss: 0.29275795991126846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1430] Loss: 0.29273482973252735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1431] Loss: 0.29272407626181\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8891\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1432] Loss: 0.2927189579085186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1433] Loss: 0.29270636004600725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1434] Loss: 0.2926985685323315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1435] Loss: 0.2927186014985264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1436] Loss: 0.2927011325744502\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1437] Loss: 0.29268757325575556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1438] Loss: 0.2927089286868903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1439] Loss: 0.2927255237555321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1440] Loss: 0.2927189526822226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1441] Loss: 0.29268747933656397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1442] Loss: 0.292678917761265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1443] Loss: 0.29269780296291464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1444] Loss: 0.29269293988929423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1445] Loss: 0.2926815483499247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1446] Loss: 0.29265638419669715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1447] Loss: 0.2926562749888288\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1448] Loss: 0.2926491950500859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1449] Loss: 0.29264750808457596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1450] Loss: 0.2926208803832432\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1451] Loss: 0.2926474162531323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1452] Loss: 0.29265290525581344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1453] Loss: 0.2926583564582802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1454] Loss: 0.2926755270439572\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1455] Loss: 0.292653604964745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1456] Loss: 0.2926492223394321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1457] Loss: 0.29261766439011705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1458] Loss: 0.2926055569122604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1459] Loss: 0.2926211879942563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1460] Loss: 0.2926455923314415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1461] Loss: 0.29265662791929203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1462] Loss: 0.29271193463182377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1463] Loss: 0.2927088052856869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1464] Loss: 0.29271094478175347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1465] Loss: 0.29269847500368695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1466] Loss: 0.292720177043612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1467] Loss: 0.2927066837828523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1468] Loss: 0.292709732837247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1469] Loss: 0.29268687080988565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1470] Loss: 0.29267983287449506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1471] Loss: 0.292692830645817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1472] Loss: 0.2926655807197883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1473] Loss: 0.29269984271176974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1474] Loss: 0.292745349334564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1475] Loss: 0.29273136577964126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1476] Loss: 0.292707159022178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1477] Loss: 0.29268716636084713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1478] Loss: 0.29270176527770386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1479] Loss: 0.29271587379526787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1480] Loss: 0.29274761403271976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1481] Loss: 0.2927388325481351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1482] Loss: 0.29272996750337077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1483] Loss: 0.29271655889157516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1484] Loss: 0.2926959984821674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1485] Loss: 0.2927235822518634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1486] Loss: 0.292695494283783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1487] Loss: 0.292729614326195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1488] Loss: 0.292698722244165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1489] Loss: 0.29268721347472754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1490] Loss: 0.2926672993824132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1491] Loss: 0.2926417183001292\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1492] Loss: 0.2926163193046142\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1493] Loss: 0.29264401389356887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1494] Loss: 0.292629169837122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1495] Loss: 0.2926143964053871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1496] Loss: 0.2926368251740146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1497] Loss: 0.29260533719085796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1498] Loss: 0.29270566977442647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1499] Loss: 0.292672685274606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1500] Loss: 0.2926681289035072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1501] Loss: 0.29264629974634837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1502] Loss: 0.29264019754235493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1503] Loss: 0.2926077329023517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1504] Loss: 0.29258620467528845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1505] Loss: 0.29257055285904143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1506] Loss: 0.2925544131959226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1507] Loss: 0.29255801002696213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1508] Loss: 0.29255205573156773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1509] Loss: 0.2925590307738734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1510] Loss: 0.2926113632495368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1511] Loss: 0.2926315671799999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1512] Loss: 0.2926310835071141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1513] Loss: 0.29262567489369223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1514] Loss: 0.2926313352221862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1515] Loss: 0.2926169247598362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1516] Loss: 0.2926229973590828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1517] Loss: 0.2926042706290842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1518] Loss: 0.2926620269321522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1519] Loss: 0.29278403819757526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1520] Loss: 0.29278573364092797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1521] Loss: 0.2928047848110363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1522] Loss: 0.2927769369342397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1523] Loss: 0.2927736445097291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 13, Batch 1524] Loss: 0.2928015211768862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 0] Loss: 0.29278680399439194\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1] Loss: 0.2928047238194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 2] Loss: 0.29279319597757975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 3] Loss: 0.29276498134051554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 4] Loss: 0.29277003890232955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 5] Loss: 0.29275689877813954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 6] Loss: 0.2927314670855072\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 7] Loss: 0.2927267864891073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 8] Loss: 0.2927081173408838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 9] Loss: 0.2927001466203193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 10] Loss: 0.292710689690199\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 11] Loss: 0.2927171390526829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 12] Loss: 0.2927171258310435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 13] Loss: 0.2927090805292266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 14] Loss: 0.29272520771308974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 15] Loss: 0.29269574212120714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 16] Loss: 0.2926982579747621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 17] Loss: 0.29268349809279537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 18] Loss: 0.2926755394172926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 19] Loss: 0.29268015632305344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 20] Loss: 0.2927073936650525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 21] Loss: 0.2926857656000719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 22] Loss: 0.2926611638359527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 23] Loss: 0.29264638000072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 24] Loss: 0.29261911304280697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 25] Loss: 0.29263496817960066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 26] Loss: 0.2926798220617361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 27] Loss: 0.29267115651714143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 28] Loss: 0.29265574288342916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 29] Loss: 0.29264358276103464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 30] Loss: 0.2926685794883845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 31] Loss: 0.29265922529077437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 32] Loss: 0.292663285566209\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 33] Loss: 0.2926673083918719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 34] Loss: 0.2926373497772458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 35] Loss: 0.2926085687882068\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 36] Loss: 0.29258687827141283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 37] Loss: 0.29260615315907973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 38] Loss: 0.29262568508866327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 39] Loss: 0.29262892387012995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 40] Loss: 0.29265551618691277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 41] Loss: 0.29264745311847995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 42] Loss: 0.29262067587960905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 43] Loss: 0.2926109273152947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 44] Loss: 0.29260412781537404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 45] Loss: 0.2925843672699474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 46] Loss: 0.2925604382168002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 47] Loss: 0.2925421653302688\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 48] Loss: 0.2925126793540089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 49] Loss: 0.2924913306281572\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 50] Loss: 0.29248298836418823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 51] Loss: 0.2925078837068256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 52] Loss: 0.29248352036939435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 53] Loss: 0.29246243849401554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 54] Loss: 0.29248458468263605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 55] Loss: 0.2924828808033854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 56] Loss: 0.29248086594673817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 57] Loss: 0.2925527402186483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 58] Loss: 0.2925611283056778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 59] Loss: 0.2925539124995557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 60] Loss: 0.29258150263083454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 61] Loss: 0.29259788690714617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 62] Loss: 0.29258068356422784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 63] Loss: 0.2925632953675256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 64] Loss: 0.29256854048819875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 65] Loss: 0.2925544416398874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 66] Loss: 0.2925456181202417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 67] Loss: 0.2925343898266157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 68] Loss: 0.29252786915153917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 69] Loss: 0.2925307289972215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 70] Loss: 0.2925455394392079\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 71] Loss: 0.2925639556194624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 72] Loss: 0.2925805667518255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 73] Loss: 0.29257090719134077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 74] Loss: 0.29256264764551676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 75] Loss: 0.2925767980419353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 76] Loss: 0.29258776519865914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 77] Loss: 0.2926316463840583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 78] Loss: 0.29260979622129335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 79] Loss: 0.2926189572567553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 80] Loss: 0.29261676896559563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 81] Loss: 0.2926508353292035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 82] Loss: 0.29263635074892574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 83] Loss: 0.2926192617652273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 84] Loss: 0.2926034227225807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 85] Loss: 0.29260115568378403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 86] Loss: 0.29259802584889405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 87] Loss: 0.2925748737659685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 88] Loss: 0.29260136159722827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 89] Loss: 0.2925943941911646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 90] Loss: 0.29259109550194096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 91] Loss: 0.2926325730695198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 92] Loss: 0.2926806584410026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 93] Loss: 0.292725229060305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 94] Loss: 0.2927217061320941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 95] Loss: 0.2927270668882257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 96] Loss: 0.29272670457911326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 97] Loss: 0.2927245194494281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 98] Loss: 0.29272006211831536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 99] Loss: 0.29272215044403177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 100] Loss: 0.29272252876519\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 101] Loss: 0.29271682596773274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 102] Loss: 0.2926990921006907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 103] Loss: 0.29267732724162715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 104] Loss: 0.292663323152573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 105] Loss: 0.292653938665265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 106] Loss: 0.29263826030465867\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 107] Loss: 0.29262999293473374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 108] Loss: 0.29264743175095137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 109] Loss: 0.29264048823329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 110] Loss: 0.29265080915762476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 111] Loss: 0.2926616361181639\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 112] Loss: 0.292665202107592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 113] Loss: 0.29267366954063057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 114] Loss: 0.2926879189830507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 115] Loss: 0.2927142858901174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 116] Loss: 0.29270327016003483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 117] Loss: 0.2926849472586543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 118] Loss: 0.292664287300352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 119] Loss: 0.2926746357710733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 120] Loss: 0.2926829377382586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 121] Loss: 0.29269561336341016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 122] Loss: 0.29268135346720336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 123] Loss: 0.29269791161177167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 124] Loss: 0.292734745307058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 125] Loss: 0.29272008266455685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 126] Loss: 0.29271116483764353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 127] Loss: 0.29271609377929036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 128] Loss: 0.2927652647077951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 129] Loss: 0.2927503275234913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 130] Loss: 0.2927470816031517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 131] Loss: 0.29274366511085237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 132] Loss: 0.29273734735637563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 133] Loss: 0.29272084803886916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 134] Loss: 0.2927201148998701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 135] Loss: 0.2927161646561389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 136] Loss: 0.2927467968680181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 137] Loss: 0.29272722666521245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 138] Loss: 0.2927231929052341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 139] Loss: 0.29274248009213544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 140] Loss: 0.2927464184552168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 141] Loss: 0.29272589441974284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 142] Loss: 0.2927115826577985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 143] Loss: 0.29271371553569275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 144] Loss: 0.2926978729336876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 145] Loss: 0.2926856257869169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 146] Loss: 0.2926792362667566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 147] Loss: 0.2926915751896066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 148] Loss: 0.2926922465960853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 149] Loss: 0.29266733666291267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 150] Loss: 0.2926483162636276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 151] Loss: 0.2926487190242746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 152] Loss: 0.29263129234385526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 153] Loss: 0.29267007388083904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 154] Loss: 0.29268233679446015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 155] Loss: 0.2927467950618267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 156] Loss: 0.29276897271710994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 157] Loss: 0.2927408332847683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 158] Loss: 0.29276744951639255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 159] Loss: 0.2927597353778729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 160] Loss: 0.2927586006567918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 161] Loss: 0.2927348548796533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 162] Loss: 0.29275583134318417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 163] Loss: 0.29272573683071573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 164] Loss: 0.2927105815839817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 165] Loss: 0.29273167002481965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 166] Loss: 0.2927537338682932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 167] Loss: 0.29273162852033985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 168] Loss: 0.2927505762281936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 169] Loss: 0.2927629207445268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 170] Loss: 0.29275325828210313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 171] Loss: 0.29274232505900655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 172] Loss: 0.2927578266776025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 173] Loss: 0.292741007756991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 174] Loss: 0.2927399493241675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 175] Loss: 0.2927471832142017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 176] Loss: 0.29276969502905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 177] Loss: 0.2927524516042987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 178] Loss: 0.2927823293649483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 179] Loss: 0.2927992784702877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 180] Loss: 0.29285300351590277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 181] Loss: 0.29285765177504397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 182] Loss: 0.2928651393502025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 183] Loss: 0.29284317030857115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 184] Loss: 0.29282172256939765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 185] Loss: 0.2928267374575423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 186] Loss: 0.29284000975954516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 187] Loss: 0.2928188944502892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 188] Loss: 0.292813743573597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 189] Loss: 0.2927844061899781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 190] Loss: 0.29276524892072403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 191] Loss: 0.2927488007808221\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 192] Loss: 0.2927408873364128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 193] Loss: 0.2927320495724166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 194] Loss: 0.2927128269126389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 195] Loss: 0.2927034282757766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 196] Loss: 0.2926841187026595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 197] Loss: 0.2926776843454549\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 198] Loss: 0.29266980131882353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 199] Loss: 0.2926854058273887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 200] Loss: 0.29270670922551484\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 201] Loss: 0.292712548192652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 202] Loss: 0.2927533531931538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 203] Loss: 0.2927525308587677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 204] Loss: 0.29273521087698223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 205] Loss: 0.29271891134953687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 206] Loss: 0.292711475645005\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 207] Loss: 0.2927048904085322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 208] Loss: 0.29268920147714966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 209] Loss: 0.29266578117314596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 210] Loss: 0.2926820714295525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 211] Loss: 0.29266446714826777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 212] Loss: 0.2926387088788242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 213] Loss: 0.2926206337152192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 214] Loss: 0.2926020952717195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 215] Loss: 0.29260697679242886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 216] Loss: 0.2925801066936451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 217] Loss: 0.29255426476422713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 218] Loss: 0.29259248600342413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 219] Loss: 0.29257572808214144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 220] Loss: 0.29259059290775496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 221] Loss: 0.29261424344214665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 222] Loss: 0.2925890053205047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 223] Loss: 0.29258432079032276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 224] Loss: 0.292567461298687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 225] Loss: 0.2925588877199666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 226] Loss: 0.2926062746134066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 227] Loss: 0.2925743874479456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 228] Loss: 0.2925657997181497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 229] Loss: 0.29255209268930216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 230] Loss: 0.292530983966801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 231] Loss: 0.29253776258177444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 232] Loss: 0.29256913434023424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 233] Loss: 0.2926008791583867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 234] Loss: 0.2925778315528382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 235] Loss: 0.29257656639503643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 236] Loss: 0.29255433499935474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 237] Loss: 0.29255053076707177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 238] Loss: 0.2925375610326452\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 239] Loss: 0.29255673000639004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 240] Loss: 0.2925746692690025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 241] Loss: 0.2925693109100026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 242] Loss: 0.2925447507839164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 243] Loss: 0.29254282937135634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 244] Loss: 0.2925509738139802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 245] Loss: 0.29253431726423346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 246] Loss: 0.29257411151499313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 247] Loss: 0.29257006819167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 248] Loss: 0.29260310631209546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 249] Loss: 0.29266663575439256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 250] Loss: 0.2926674623062894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 251] Loss: 0.2926766149842923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 252] Loss: 0.2926435914704682\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 253] Loss: 0.29263002169802643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 254] Loss: 0.292643108554648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 255] Loss: 0.29264422581186444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 256] Loss: 0.2926287501557683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 257] Loss: 0.2926123099593957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 258] Loss: 0.29261121000902834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 259] Loss: 0.2925978343784903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 260] Loss: 0.29259429511875734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 261] Loss: 0.2925950370757103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 262] Loss: 0.29261156980142256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 263] Loss: 0.2926255913533929\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 264] Loss: 0.2926010810100052\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 265] Loss: 0.29258516522932726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 266] Loss: 0.29256098506970146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 267] Loss: 0.29253519924415505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 268] Loss: 0.29253124516751805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 269] Loss: 0.29253282634942956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 270] Loss: 0.29250592331271047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 271] Loss: 0.29252336101335547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 272] Loss: 0.29251612358594475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 273] Loss: 0.2925262467189219\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 274] Loss: 0.2925049658343582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 275] Loss: 0.29249344516890574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 276] Loss: 0.29251425841004897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 277] Loss: 0.2925029298149129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 278] Loss: 0.2925021150715681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 279] Loss: 0.29249242333514697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 280] Loss: 0.2924893074701814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 281] Loss: 0.2924672735181907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 282] Loss: 0.29245831876740813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 283] Loss: 0.2924658614007815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 284] Loss: 0.292443399691464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 285] Loss: 0.2924246082099908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 286] Loss: 0.29243221657040946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 287] Loss: 0.29247270898431754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 288] Loss: 0.2925377490926028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 289] Loss: 0.29252869912999724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 290] Loss: 0.2925073838189606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 291] Loss: 0.2925034397615092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 292] Loss: 0.292490543447947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 293] Loss: 0.29246788674530455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 294] Loss: 0.29246182502188356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 295] Loss: 0.29246527797576033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 296] Loss: 0.2924432261771037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 297] Loss: 0.2924282588122951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 298] Loss: 0.2924315182431826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 299] Loss: 0.29244224059351503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 300] Loss: 0.2924415528413092\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 301] Loss: 0.2924363730606938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 302] Loss: 0.292416846909867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 303] Loss: 0.2924303936202308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 304] Loss: 0.2924389546982458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 305] Loss: 0.29243580491398463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 306] Loss: 0.29244179881083704\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 307] Loss: 0.29241719612574174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 308] Loss: 0.2924203367081449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 309] Loss: 0.2924167462900923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 310] Loss: 0.2924149199319854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 311] Loss: 0.2924090808049854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 312] Loss: 0.2923826513262775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 313] Loss: 0.29236855391952465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 314] Loss: 0.2923739809472562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 315] Loss: 0.2923701341375825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 316] Loss: 0.2923705846645348\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 317] Loss: 0.2923399051262097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 318] Loss: 0.2923544915507722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 319] Loss: 0.2923444954322382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 320] Loss: 0.2923344860331025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 321] Loss: 0.2923315050965291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 322] Loss: 0.29231137348095426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 323] Loss: 0.29234418414716556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 324] Loss: 0.29231391112521943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 325] Loss: 0.292299929434126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 326] Loss: 0.29233721175370714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 327] Loss: 0.29232137231547783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 328] Loss: 0.29230682984651035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 329] Loss: 0.292331647741403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 330] Loss: 0.29231224118619575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 331] Loss: 0.2922793898152908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 332] Loss: 0.29227817403675704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 333] Loss: 0.29225970197911766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 334] Loss: 0.2922716923851881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 335] Loss: 0.2922811640701713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 336] Loss: 0.2922572772472036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 337] Loss: 0.29223709210489335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 338] Loss: 0.29223211172295827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 339] Loss: 0.2922112782343059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 340] Loss: 0.2922652035731628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 341] Loss: 0.29224847931231934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 342] Loss: 0.2922980684798031\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 343] Loss: 0.2922949297107703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 344] Loss: 0.29229581616975114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 345] Loss: 0.29228320829685983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 346] Loss: 0.29226633267264024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 347] Loss: 0.2922829405474889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 348] Loss: 0.2922680661355174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 349] Loss: 0.29227768248126307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 350] Loss: 0.29227063367425055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 351] Loss: 0.29227993413436093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 352] Loss: 0.29230056734409227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 353] Loss: 0.29229543884180803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 354] Loss: 0.2922816460163376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 355] Loss: 0.29226586092465606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 356] Loss: 0.29226113147508115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 357] Loss: 0.2922508032531729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 358] Loss: 0.2922257441313215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 359] Loss: 0.29221298127124323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 360] Loss: 0.29218389292057867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 361] Loss: 0.2921535671138132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 362] Loss: 0.2921553040164182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 363] Loss: 0.29214707606830587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 364] Loss: 0.2921344182149953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 365] Loss: 0.2921387713771579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 366] Loss: 0.29213421432697223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 367] Loss: 0.2921187392398784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 368] Loss: 0.29208930495924096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 369] Loss: 0.2921210661078665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 370] Loss: 0.2920998162201736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 371] Loss: 0.2920838721094211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 372] Loss: 0.29208961902933694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 373] Loss: 0.29208995288050166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 374] Loss: 0.29206838527074425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 375] Loss: 0.2920957538499346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 376] Loss: 0.2920962453828015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 377] Loss: 0.29212373634167155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 378] Loss: 0.2921235855296427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 379] Loss: 0.2921184081610685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 380] Loss: 0.29215031236295075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 381] Loss: 0.29213931943721944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 382] Loss: 0.2921231686884889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 383] Loss: 0.29210333635627433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 384] Loss: 0.292114597951801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 385] Loss: 0.29212597589249967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 386] Loss: 0.29212060842483134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 387] Loss: 0.2921204831323591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 388] Loss: 0.29215111193071447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 389] Loss: 0.2921645119050857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 390] Loss: 0.29216268254686706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 391] Loss: 0.2921514470374882\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 392] Loss: 0.29214300798932447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 393] Loss: 0.29212100104269206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 394] Loss: 0.2921375768690599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 395] Loss: 0.29212718357106754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 396] Loss: 0.2921070490274053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 397] Loss: 0.2921098009766397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 398] Loss: 0.29208981294294506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 399] Loss: 0.292098960044992\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 400] Loss: 0.29214111065708553\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 401] Loss: 0.2921220812134518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 402] Loss: 0.2921321951569604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 403] Loss: 0.2921285471661379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 404] Loss: 0.29210965790956234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 405] Loss: 0.2921342123884421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 406] Loss: 0.29212598678671825\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8865000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 407] Loss: 0.2921021718111305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 408] Loss: 0.2920846275993261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 409] Loss: 0.2920825875342479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 410] Loss: 0.2920834406122256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 411] Loss: 0.2920741841329855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 412] Loss: 0.2920500986954758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 413] Loss: 0.2920325112625837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 414] Loss: 0.2920407406318424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 415] Loss: 0.29202858867916276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 416] Loss: 0.29202691618152304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 417] Loss: 0.2920105612678888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 418] Loss: 0.2920313942373833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 419] Loss: 0.2920922699802157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 420] Loss: 0.292106895504415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 421] Loss: 0.292085706348376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 422] Loss: 0.2920849481796183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 423] Loss: 0.29207501317888507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 424] Loss: 0.29206144733834183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 425] Loss: 0.2920534109273937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 426] Loss: 0.2920499451137397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 427] Loss: 0.29204729186614864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 428] Loss: 0.2920450433282313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 429] Loss: 0.2920269295618916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 430] Loss: 0.29201989188504857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 431] Loss: 0.2919999496806373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 432] Loss: 0.29197769810581015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 433] Loss: 0.2919746059467833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 434] Loss: 0.29196870220184035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 435] Loss: 0.2919572851102896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 436] Loss: 0.2919326866180494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 437] Loss: 0.2919620597473844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 438] Loss: 0.2919379247926414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 439] Loss: 0.2919237668411353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 440] Loss: 0.29193294240302864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 441] Loss: 0.2919457892327589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 442] Loss: 0.29196797406361596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 443] Loss: 0.2919570236176517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 444] Loss: 0.2919436417546253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 445] Loss: 0.2919246665199233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 446] Loss: 0.2919029143813042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 447] Loss: 0.2919183565193781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 448] Loss: 0.29196945331101387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 449] Loss: 0.29195318501761464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 450] Loss: 0.29197620999002566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 451] Loss: 0.29199196884036976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 452] Loss: 0.29197931963605006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 453] Loss: 0.29197025896211043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 454] Loss: 0.2919683344246941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 455] Loss: 0.29200292353866664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 456] Loss: 0.29197739457409905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 457] Loss: 0.2919658292642134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 458] Loss: 0.2919590186555659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 459] Loss: 0.2920000704541361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 460] Loss: 0.29200725384812426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 461] Loss: 0.29199239234437185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 462] Loss: 0.29201857001498394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 463] Loss: 0.2920498591031142\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 464] Loss: 0.29202897404480277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 465] Loss: 0.29202888542218364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 466] Loss: 0.2920194322791537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 467] Loss: 0.2920382146629045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 468] Loss: 0.29203669211336086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 469] Loss: 0.2920505256107727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 470] Loss: 0.2920375576805996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 471] Loss: 0.292033195464968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 472] Loss: 0.29205681925356725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 473] Loss: 0.292039523585206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 474] Loss: 0.2920855338326381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 475] Loss: 0.29207696582968923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 476] Loss: 0.2920803502746286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 477] Loss: 0.2920615257716857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 478] Loss: 0.29205385217646856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 479] Loss: 0.2920305732731711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 480] Loss: 0.29202272147166863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 481] Loss: 0.29200956008879697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 482] Loss: 0.2920086237016974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 483] Loss: 0.2920137695293091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 484] Loss: 0.2919920925450173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 485] Loss: 0.29198674355438237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 486] Loss: 0.2919698181614345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 487] Loss: 0.2919424586371606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 488] Loss: 0.2919384325196202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 489] Loss: 0.2919455285671813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 490] Loss: 0.29195662046936127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 491] Loss: 0.2919343004957398\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 492] Loss: 0.2919376998397782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 493] Loss: 0.29193471557374207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 494] Loss: 0.2919104501779171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 495] Loss: 0.29193254016871156\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 496] Loss: 0.2919622353747082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 497] Loss: 0.2919318129769662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 498] Loss: 0.29194638411499346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 499] Loss: 0.29195441151970547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 500] Loss: 0.29192234931162037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 501] Loss: 0.2919156958793531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 502] Loss: 0.2918929309149809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 503] Loss: 0.2918855332382414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 504] Loss: 0.29188098719281164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 505] Loss: 0.29190616331091435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 506] Loss: 0.2919247428466291\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 507] Loss: 0.29190179686434736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 508] Loss: 0.2919297406646531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 509] Loss: 0.29192958053975376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 510] Loss: 0.29190226791145946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 511] Loss: 0.2919171872274409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 512] Loss: 0.29190181315157226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 513] Loss: 0.2919192479767771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 514] Loss: 0.2919430232019626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 515] Loss: 0.29192092666084035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 516] Loss: 0.29191304066302537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 517] Loss: 0.29190195381474615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 518] Loss: 0.2919045287874072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 519] Loss: 0.29193428385747067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 520] Loss: 0.29194393950322317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 521] Loss: 0.291939842084506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 522] Loss: 0.29192075071958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 523] Loss: 0.2919045868933417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 524] Loss: 0.2919217996701119\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 525] Loss: 0.29192209811571684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 526] Loss: 0.2919647618250497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 527] Loss: 0.29196613808861493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 528] Loss: 0.2919589541498113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 529] Loss: 0.29195560271185844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 530] Loss: 0.29198335187952473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 531] Loss: 0.29205427351083024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 532] Loss: 0.2920554645862068\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 533] Loss: 0.29205031872951237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 534] Loss: 0.29203554328493375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 535] Loss: 0.29203178910580696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 536] Loss: 0.29203703385248136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 537] Loss: 0.2920160685285063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 538] Loss: 0.2920010607753083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 539] Loss: 0.29200126121664643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 540] Loss: 0.29197742411110217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 541] Loss: 0.29197287827959223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 542] Loss: 0.29196951772495083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 543] Loss: 0.2919684460386073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 544] Loss: 0.29200738375648205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 545] Loss: 0.2920508838460657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 546] Loss: 0.2920315911570706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 547] Loss: 0.29202043650196785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 548] Loss: 0.29203445485458124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 549] Loss: 0.2920805899339993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 550] Loss: 0.2920943607135407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 551] Loss: 0.2921113756586459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 552] Loss: 0.29209436881433704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 553] Loss: 0.2920715991468152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 554] Loss: 0.29207395996849667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 555] Loss: 0.29205601641670204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 556] Loss: 0.29206261335974476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 557] Loss: 0.2920632130347116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 558] Loss: 0.292047766337274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 559] Loss: 0.29204089107310444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 560] Loss: 0.2920464531841088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 561] Loss: 0.2920531166276375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 562] Loss: 0.292039496988949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 563] Loss: 0.29202972290845525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 564] Loss: 0.2920291368491676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 565] Loss: 0.29204676945221136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 566] Loss: 0.29212244058014164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 567] Loss: 0.2921060503135301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 568] Loss: 0.29210793495549464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 569] Loss: 0.292197527678064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 570] Loss: 0.2921703411854485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 571] Loss: 0.2921491132950214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 572] Loss: 0.2921589404820627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 573] Loss: 0.2921520414017476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 574] Loss: 0.29215074462844326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 575] Loss: 0.2921628980231339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 576] Loss: 0.2921884096684243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 577] Loss: 0.2922237248392217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 578] Loss: 0.2922396298822266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 579] Loss: 0.29224944699322314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 580] Loss: 0.29223548890611206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 581] Loss: 0.2922558544193595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 582] Loss: 0.2922657513381461\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 583] Loss: 0.29229806485973847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 584] Loss: 0.2922728639349803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 585] Loss: 0.29226119111286486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 586] Loss: 0.29228192798334973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 587] Loss: 0.2922577176458266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 588] Loss: 0.2922421238818123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 589] Loss: 0.29225088547712347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 590] Loss: 0.29224948910362436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 591] Loss: 0.2922504071916269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 592] Loss: 0.29224761372232394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 593] Loss: 0.2922654987399747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 594] Loss: 0.2922424959058647\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 595] Loss: 0.2922340473040426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 596] Loss: 0.2922276996678523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 597] Loss: 0.2922079086969498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 598] Loss: 0.2921864077711558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 599] Loss: 0.2921879426533067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 600] Loss: 0.2921730612099304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 601] Loss: 0.2921533748616543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 602] Loss: 0.2922084215064449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 603] Loss: 0.29220117048924094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 604] Loss: 0.2922102588482322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 605] Loss: 0.29224053433693165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 606] Loss: 0.29225249683998394\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 607] Loss: 0.2922276799786326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 608] Loss: 0.29220492652209323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 609] Loss: 0.2922101765526525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 610] Loss: 0.292196216991506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 611] Loss: 0.29221608223437445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 612] Loss: 0.2921979582536784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 613] Loss: 0.292183085590752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 614] Loss: 0.29219413523609866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 615] Loss: 0.29218317323909787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 616] Loss: 0.2921717687841766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 617] Loss: 0.292146320532479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 618] Loss: 0.29215319514714105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 619] Loss: 0.2921347013633895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 620] Loss: 0.29212993541961274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 621] Loss: 0.29213049311685624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 622] Loss: 0.29211060446461734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 623] Loss: 0.29209576023564554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 624] Loss: 0.2921072380582054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 625] Loss: 0.2921169466860149\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 626] Loss: 0.29209961500054865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 627] Loss: 0.29208511909610174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 628] Loss: 0.2920629809918493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 629] Loss: 0.29209215423687274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 630] Loss: 0.2920780968588524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 631] Loss: 0.292051312344444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 632] Loss: 0.2920338005631591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 633] Loss: 0.29206107679630466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 634] Loss: 0.29205134102313723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 635] Loss: 0.29205722683185453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 636] Loss: 0.29203239095244676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 637] Loss: 0.2920461489519617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 638] Loss: 0.29203908355898583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 639] Loss: 0.2920114662988376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 640] Loss: 0.2920372518925819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 641] Loss: 0.2920144519098368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 642] Loss: 0.2919933670119537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 643] Loss: 0.29199627705270015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 644] Loss: 0.29200884020546103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 645] Loss: 0.29200099557195996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 646] Loss: 0.29201590209346096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 647] Loss: 0.2920195107394954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 648] Loss: 0.2920110620139491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 649] Loss: 0.2920088425128415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 650] Loss: 0.29198088972567626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 651] Loss: 0.2919624026554418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 652] Loss: 0.29195525484830165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 653] Loss: 0.29199296434889654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 654] Loss: 0.2919880127384409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 655] Loss: 0.29196076549534444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 656] Loss: 0.2919556018240433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 657] Loss: 0.2919494781954329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 658] Loss: 0.2919432768193012\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 659] Loss: 0.29196031844899223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 660] Loss: 0.2919519346888466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 661] Loss: 0.29197218531688657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 662] Loss: 0.2920338049425129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 663] Loss: 0.29209753583546716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 664] Loss: 0.2921159376345772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 665] Loss: 0.29210859533487515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 666] Loss: 0.2921042814961007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 667] Loss: 0.29209162639847913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 668] Loss: 0.2921334386354418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 669] Loss: 0.2922481181323616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 670] Loss: 0.2922302711135532\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 671] Loss: 0.2922064292038417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 672] Loss: 0.2922056821870349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 673] Loss: 0.2922381456362317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 674] Loss: 0.2922555226356579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 675] Loss: 0.2922416326115279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 676] Loss: 0.2922274792769638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 677] Loss: 0.2922082435366094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 678] Loss: 0.29219780449045824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 679] Loss: 0.29222776770156184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 680] Loss: 0.292225547093087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 681] Loss: 0.2922291113283604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 682] Loss: 0.2922074920858573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 683] Loss: 0.2922002063637094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 684] Loss: 0.29220138596699713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 685] Loss: 0.2921808743580165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 686] Loss: 0.292187943422826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 687] Loss: 0.2921990799711838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 688] Loss: 0.2921984459802269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 689] Loss: 0.2921747644069464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 690] Loss: 0.2921838569296311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 691] Loss: 0.29217908956657246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 692] Loss: 0.2921591294956555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 693] Loss: 0.29219344647630885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 694] Loss: 0.29223427775133903\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 695] Loss: 0.2922250162025527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 696] Loss: 0.29222144608341566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 697] Loss: 0.2921921791206032\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 698] Loss: 0.29217245243955886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 699] Loss: 0.29216960278356957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 700] Loss: 0.29217205936742935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 701] Loss: 0.29215802421235315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 702] Loss: 0.2921639373203981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 703] Loss: 0.2921939185445415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 704] Loss: 0.2921901223319456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 705] Loss: 0.29217587713480875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 706] Loss: 0.2921534486628774\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 707] Loss: 0.2921729824727281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 708] Loss: 0.29221213969078796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 709] Loss: 0.2922011538891057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 710] Loss: 0.2921868979788545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 711] Loss: 0.2921660780816911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 712] Loss: 0.2921931939053171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 713] Loss: 0.29217823929800163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 714] Loss: 0.2921579668782832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 715] Loss: 0.2921549065533626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 716] Loss: 0.2921360307444894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 717] Loss: 0.2921171591169279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 718] Loss: 0.29212633325707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 719] Loss: 0.29210804481007924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 720] Loss: 0.29210066622067093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 721] Loss: 0.29209374284105016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 722] Loss: 0.2920682348580904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 723] Loss: 0.2920952899959053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 724] Loss: 0.29211445132218283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 725] Loss: 0.29210854424627997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 726] Loss: 0.29208701599720827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 727] Loss: 0.292115681410727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 728] Loss: 0.2921013796319015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 729] Loss: 0.2921777902802088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 730] Loss: 0.29215583749306506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 731] Loss: 0.29218806299320105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 732] Loss: 0.2921783105553864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 733] Loss: 0.2921721093120914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 734] Loss: 0.29217940645950014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 735] Loss: 0.2921654089848957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 736] Loss: 0.29216325316425995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 737] Loss: 0.29214776280517113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 738] Loss: 0.29214705718736006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 739] Loss: 0.29212807666402413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 740] Loss: 0.29214760520943084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 741] Loss: 0.29212693488819336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 742] Loss: 0.29212103632827835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 743] Loss: 0.29210512471036904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 744] Loss: 0.29209032876351404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 745] Loss: 0.292077545200785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 746] Loss: 0.2920692068754869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 747] Loss: 0.29205892390542404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 748] Loss: 0.2920421558623772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 749] Loss: 0.29203364079734306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 750] Loss: 0.29201457021095356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 751] Loss: 0.2919934794432999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 752] Loss: 0.29205306876100934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 753] Loss: 0.292061067181944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 754] Loss: 0.2920311984723981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 755] Loss: 0.2920125758908961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 756] Loss: 0.2919931022042237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 757] Loss: 0.2920461897583329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 758] Loss: 0.2920372100702154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 759] Loss: 0.2920300741980262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 760] Loss: 0.292010547526121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 761] Loss: 0.291984128233267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 762] Loss: 0.29197300252660435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 763] Loss: 0.29195917734632376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 764] Loss: 0.29195833387254433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 765] Loss: 0.291994646107968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 766] Loss: 0.2920186050662997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 767] Loss: 0.29201493928324584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 768] Loss: 0.29201878850125335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 769] Loss: 0.2920058743609224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 770] Loss: 0.29204032333094276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 771] Loss: 0.2920299802163218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 772] Loss: 0.2920357467373664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 773] Loss: 0.2920442736790617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 774] Loss: 0.29203733281526767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 775] Loss: 0.2920713833662013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 776] Loss: 0.2920650160643597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 777] Loss: 0.2920508628333805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 778] Loss: 0.29206286988763264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 779] Loss: 0.2920657630900345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 780] Loss: 0.29205681977380404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 781] Loss: 0.2920325137024671\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 782] Loss: 0.2920270120294111\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 783] Loss: 0.29206966642126786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 784] Loss: 0.2920589295258822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 785] Loss: 0.2920630932677277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 786] Loss: 0.29204477920485106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 787] Loss: 0.2920281718317522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 788] Loss: 0.29201505879936357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 789] Loss: 0.2919967848350841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 790] Loss: 0.29199885473834836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 791] Loss: 0.2920075001656542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 792] Loss: 0.29199068587967353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 793] Loss: 0.2920616584420031\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 794] Loss: 0.29206626433898064\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 795] Loss: 0.2920614078338157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 796] Loss: 0.2920517285980588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 797] Loss: 0.29203510270154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 798] Loss: 0.2920380530493294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 799] Loss: 0.2920321676569951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 800] Loss: 0.29202553713295754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 801] Loss: 0.2920419487782393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 802] Loss: 0.29208045103799885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 803] Loss: 0.29205727184698455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 804] Loss: 0.2920776770687359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 805] Loss: 0.292063093641098\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 806] Loss: 0.2920541754754932\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 807] Loss: 0.2920286970904463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 808] Loss: 0.2920752126266434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 809] Loss: 0.2920618766577761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 810] Loss: 0.2920655246330378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 811] Loss: 0.2920559478134495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 812] Loss: 0.29207755865474433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 813] Loss: 0.2920503165436985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 814] Loss: 0.2920254599747842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 815] Loss: 0.29201515380578863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 816] Loss: 0.29202622255209887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 817] Loss: 0.29201689278232623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 818] Loss: 0.29200886117080765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 819] Loss: 0.29200015175039107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 820] Loss: 0.29198638560413365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 821] Loss: 0.2919587146678262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 822] Loss: 0.2919652754923193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 823] Loss: 0.2919810549260461\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 824] Loss: 0.29199363774544485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 825] Loss: 0.291978658828346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 826] Loss: 0.2919684452845895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 827] Loss: 0.29197622931647665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 828] Loss: 0.2919606146514717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 829] Loss: 0.2919561704873591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 830] Loss: 0.29194115291864003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 831] Loss: 0.2919271336774278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 832] Loss: 0.29191250536578445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 833] Loss: 0.2919166728888593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 834] Loss: 0.29189993532073155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 835] Loss: 0.2919029459593788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 836] Loss: 0.2918776835374993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 837] Loss: 0.2918821853352845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 838] Loss: 0.2918770323106776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 839] Loss: 0.2918781646741444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 840] Loss: 0.2918713657758867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 841] Loss: 0.2918547467621862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 842] Loss: 0.2918411512139965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 843] Loss: 0.29184894023839525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 844] Loss: 0.2918342176531144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 845] Loss: 0.29183126713983953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 846] Loss: 0.2918643937182519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 847] Loss: 0.2918644528122293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 848] Loss: 0.29184000002201227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 849] Loss: 0.2918528623016302\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 850] Loss: 0.2918488689319447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 851] Loss: 0.2918800011217131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 852] Loss: 0.291866345739423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 853] Loss: 0.2918567978870821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 854] Loss: 0.2918907156872705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 855] Loss: 0.2918718070735177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 856] Loss: 0.2918739350918059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 857] Loss: 0.2918786547383587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 858] Loss: 0.2918762112930835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 859] Loss: 0.2918635386933977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 860] Loss: 0.2918578373258873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 861] Loss: 0.29187203559632474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 862] Loss: 0.2918939404773253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 863] Loss: 0.2918946430147054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 864] Loss: 0.29188998025550394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 865] Loss: 0.2918986628131791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 866] Loss: 0.291912802412513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 867] Loss: 0.2919211670953253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 868] Loss: 0.29189873342838984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 869] Loss: 0.2918932822729273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 870] Loss: 0.29188970634814054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 871] Loss: 0.29187238732143755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 872] Loss: 0.29190352948568843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 873] Loss: 0.29187667804388445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 874] Loss: 0.29184889327053154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 875] Loss: 0.29182907275139347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 876] Loss: 0.29182621676129056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 877] Loss: 0.2918221484281965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 878] Loss: 0.29184191585843644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 879] Loss: 0.2918568253025716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 880] Loss: 0.29184733341458025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 881] Loss: 0.2918444158906681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 882] Loss: 0.2918397315968855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 883] Loss: 0.2918234777872526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 884] Loss: 0.29181776229179524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 885] Loss: 0.2918046069990598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 886] Loss: 0.29180616858421843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 887] Loss: 0.29182893305854724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 888] Loss: 0.29183966883372975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 889] Loss: 0.2918441738732765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 890] Loss: 0.2918314122479561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 891] Loss: 0.29183342410830077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 892] Loss: 0.29180430298018256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 893] Loss: 0.29177836223084824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 894] Loss: 0.2918065546768335\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 895] Loss: 0.2917962515185746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 896] Loss: 0.2917985329309889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 897] Loss: 0.2917928796962705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 898] Loss: 0.29178867177566575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 899] Loss: 0.2917678479839213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 900] Loss: 0.291774415791972\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 901] Loss: 0.2917631155326153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 902] Loss: 0.2917994646648603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 903] Loss: 0.2917926215299728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 904] Loss: 0.291779584062791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 905] Loss: 0.29176180552904096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 906] Loss: 0.29175794705559127\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8904\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 907] Loss: 0.29175998857343277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 908] Loss: 0.29173752569235617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 909] Loss: 0.2917690652596532\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 910] Loss: 0.2917552459386738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 911] Loss: 0.29177706048334967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 912] Loss: 0.29177797683762535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 913] Loss: 0.29177649396675803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 914] Loss: 0.29176337339535713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 915] Loss: 0.29174179124105165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 916] Loss: 0.2917366133559925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 917] Loss: 0.29175151970515883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 918] Loss: 0.29172655380917023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 919] Loss: 0.29172866154034016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 920] Loss: 0.29170932673868416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 921] Loss: 0.29170393541204653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 922] Loss: 0.2916996364456662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 923] Loss: 0.2916833809960446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 924] Loss: 0.29167875128795934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 925] Loss: 0.29167759033540885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 926] Loss: 0.2916720428125605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 927] Loss: 0.29166204722216754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 928] Loss: 0.29165044998627826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 929] Loss: 0.2916185631552512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 930] Loss: 0.29162291944769353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 931] Loss: 0.29165145779097995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 932] Loss: 0.2916613024009636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 933] Loss: 0.2916659949804292\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 934] Loss: 0.29164771364570125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 935] Loss: 0.29163244146095907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 936] Loss: 0.29161116407391796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 937] Loss: 0.291618287964884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 938] Loss: 0.29159076820137864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 939] Loss: 0.29158724986815404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 940] Loss: 0.2915653356738182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 941] Loss: 0.2915547697647618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 942] Loss: 0.29156628091624637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 943] Loss: 0.291556550064406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 944] Loss: 0.2915493695192131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 945] Loss: 0.29154509187210353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 946] Loss: 0.2915503777570979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 947] Loss: 0.2915774132147563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 948] Loss: 0.29155855232445876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 949] Loss: 0.2915393632076668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 950] Loss: 0.2915290112377821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 951] Loss: 0.29153077911533753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 952] Loss: 0.2915286627626257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 953] Loss: 0.2915284430225476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 954] Loss: 0.2915175444357851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 955] Loss: 0.2914990192715158\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 956] Loss: 0.29148684278682113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 957] Loss: 0.291468955952541\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 958] Loss: 0.29145547538873207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 959] Loss: 0.29146228822781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 960] Loss: 0.29145947098731995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 961] Loss: 0.29145852988269055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 962] Loss: 0.29147569242475796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 963] Loss: 0.29146988863865964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 964] Loss: 0.2914610193781887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 965] Loss: 0.29145819953413254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 966] Loss: 0.29147546367492644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 967] Loss: 0.2914523815752528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 968] Loss: 0.2914839678212028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 969] Loss: 0.29147476992345206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 970] Loss: 0.29146129860577646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 971] Loss: 0.2914647787404472\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 972] Loss: 0.29146220752537394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 973] Loss: 0.29145052860399795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 974] Loss: 0.2914706299313057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 975] Loss: 0.2915023086642612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 976] Loss: 0.29148382753062124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 977] Loss: 0.29146555133956187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 978] Loss: 0.2914545521504569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 979] Loss: 0.2914805521576088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 980] Loss: 0.2914718822771585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 981] Loss: 0.2914721341587818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 982] Loss: 0.2914623684767395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 983] Loss: 0.2914439354067672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 984] Loss: 0.29143491944345246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 985] Loss: 0.29141260238606376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 986] Loss: 0.2913896284724945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 987] Loss: 0.29137390250625733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 988] Loss: 0.2913593129428046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 989] Loss: 0.2913648342300377\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 990] Loss: 0.29134788558402364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 991] Loss: 0.29134986465920376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 992] Loss: 0.2913700620710421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 993] Loss: 0.29135568282497587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 994] Loss: 0.2913422644293885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 995] Loss: 0.29135002979564734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 996] Loss: 0.2913581710169571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 997] Loss: 0.2913521595038705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 998] Loss: 0.29136090310852164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 999] Loss: 0.2913665662823948\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1000] Loss: 0.2913819179327409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1001] Loss: 0.2913907860566388\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1002] Loss: 0.2913708024496459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1003] Loss: 0.2913623288416567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1004] Loss: 0.2913625752137469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1005] Loss: 0.29135921595172143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1006] Loss: 0.2913747041611718\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1007] Loss: 0.2913592520023829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1008] Loss: 0.291359166928736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1009] Loss: 0.2913420896196889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1010] Loss: 0.29133080133328043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1011] Loss: 0.2913030854133009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1012] Loss: 0.29129561273960586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1013] Loss: 0.29129221401353966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1014] Loss: 0.2913064068160508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1015] Loss: 0.29130866285177726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1016] Loss: 0.2913412066998235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1017] Loss: 0.2913370346273343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1018] Loss: 0.29134180938960397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1019] Loss: 0.29135163295841265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1020] Loss: 0.29134730650498175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1021] Loss: 0.29134390166342394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1022] Loss: 0.29132139120914136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1023] Loss: 0.2913288824452832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1024] Loss: 0.2913516939642942\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1025] Loss: 0.29134734056241224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1026] Loss: 0.2913561242810044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1027] Loss: 0.2913696106258603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1028] Loss: 0.2913885970365779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1029] Loss: 0.29137538663900714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1030] Loss: 0.2913698329099438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1031] Loss: 0.2914470236632401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1032] Loss: 0.2914819804227869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1033] Loss: 0.29147102549885356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1034] Loss: 0.2914700389675496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1035] Loss: 0.29146067181505464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1036] Loss: 0.29145766284080127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1037] Loss: 0.2914490229405768\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1038] Loss: 0.2914389894206427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1039] Loss: 0.2914459982394487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1040] Loss: 0.29142556472328385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1041] Loss: 0.29141274111182536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1042] Loss: 0.29139573334983637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1043] Loss: 0.29139300010227864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1044] Loss: 0.2913966196813425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1045] Loss: 0.29139238626119635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1046] Loss: 0.29137410195835195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1047] Loss: 0.29137509833532066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1048] Loss: 0.2913485137962434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1049] Loss: 0.29132306974064165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1050] Loss: 0.29130674534593953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1051] Loss: 0.291294075230649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1052] Loss: 0.2913025068850205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1053] Loss: 0.29131274794295603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1054] Loss: 0.29130576571700023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1055] Loss: 0.2912883632822887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1056] Loss: 0.29128684555774903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1057] Loss: 0.29131227337819854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1058] Loss: 0.29135594553448646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1059] Loss: 0.2913473522416083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1060] Loss: 0.2913394751007702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1061] Loss: 0.29133507839777306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1062] Loss: 0.2913185262700458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1063] Loss: 0.2913134716509457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1064] Loss: 0.2913326925874789\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1065] Loss: 0.2913300125968939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1066] Loss: 0.2913585663717846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1067] Loss: 0.2913573288255499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1068] Loss: 0.2913415983598042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1069] Loss: 0.2913307129498519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1070] Loss: 0.2913131262912088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1071] Loss: 0.29130029227005483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1072] Loss: 0.29132928874526076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1073] Loss: 0.29131320398612714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1074] Loss: 0.2913233027244599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1075] Loss: 0.2913114589700057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1076] Loss: 0.29128919849687834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1077] Loss: 0.29129240152930425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1078] Loss: 0.2912920272487004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1079] Loss: 0.29132114115771157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1080] Loss: 0.2913105292239256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1081] Loss: 0.2913141733330312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1082] Loss: 0.2913055485416807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1083] Loss: 0.2912964542196816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1084] Loss: 0.29129170939411386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1085] Loss: 0.2912832259913922\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1086] Loss: 0.29127829558000873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1087] Loss: 0.2912759852246284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1088] Loss: 0.29126007126269027\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1089] Loss: 0.2912813799968093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1090] Loss: 0.2912687337027296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1091] Loss: 0.2913029136614666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1092] Loss: 0.29130211912165005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1093] Loss: 0.29128712301365883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1094] Loss: 0.2912681387561985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1095] Loss: 0.29127652120461717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1096] Loss: 0.2912670965041608\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1097] Loss: 0.2912690118711454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1098] Loss: 0.2912565197562054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1099] Loss: 0.2912621282389057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1100] Loss: 0.29127440175366454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1101] Loss: 0.29124906278417767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1102] Loss: 0.2912339023511758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1103] Loss: 0.29122108057820323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1104] Loss: 0.29120226028971846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1105] Loss: 0.2911813378903187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1106] Loss: 0.2911738043697803\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1107] Loss: 0.29118083025851504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1108] Loss: 0.29116344475177663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1109] Loss: 0.2911714247662133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1110] Loss: 0.29119617055576763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1111] Loss: 0.2911752667531367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1112] Loss: 0.291159390126458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1113] Loss: 0.2911573804296826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1114] Loss: 0.2911516076796411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1115] Loss: 0.2911377003212329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1116] Loss: 0.2911471784840161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1117] Loss: 0.2911519753388726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1118] Loss: 0.2911447957591923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1119] Loss: 0.29111830685168943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1120] Loss: 0.2911139712419183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1121] Loss: 0.29109158161346144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1122] Loss: 0.2910914189346497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1123] Loss: 0.2910910896011405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1124] Loss: 0.29108917769000775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1125] Loss: 0.2910925843017666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1126] Loss: 0.29109017912382534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1127] Loss: 0.29109768289788973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1128] Loss: 0.29113230357705006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1129] Loss: 0.2911125383642012\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1130] Loss: 0.29108784458525866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1131] Loss: 0.2910800339072044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1132] Loss: 0.29109740841288906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1133] Loss: 0.29109243024089054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1134] Loss: 0.2910920379659928\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1135] Loss: 0.29106833621612227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1136] Loss: 0.29107056509395834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1137] Loss: 0.2910987363264904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1138] Loss: 0.29109867603476736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1139] Loss: 0.29113852166263543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1140] Loss: 0.2911315588855958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1141] Loss: 0.2911614099637934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1142] Loss: 0.2911518778133593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1143] Loss: 0.29114517173543264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1144] Loss: 0.29117083191842824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1145] Loss: 0.29117609648270665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1146] Loss: 0.29114831291255394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1147] Loss: 0.29116356947007377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1148] Loss: 0.29117222262687087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1149] Loss: 0.291153886270607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1150] Loss: 0.2911350019034474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1151] Loss: 0.2911283020480229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1152] Loss: 0.2911144133461366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1153] Loss: 0.2910974877886474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1154] Loss: 0.29108083676014923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1155] Loss: 0.2911048878503257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1156] Loss: 0.29108476944071493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1157] Loss: 0.2910761077067945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1158] Loss: 0.2910733780291152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1159] Loss: 0.29108259185680285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1160] Loss: 0.29107469613493764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1161] Loss: 0.29105335172849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1162] Loss: 0.2910508881523883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1163] Loss: 0.29102895087255826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1164] Loss: 0.29102030770376147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1165] Loss: 0.29102730717273945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1166] Loss: 0.29102228340667036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1167] Loss: 0.29100495303027624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1168] Loss: 0.2910149887442162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1169] Loss: 0.29099947782048735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1170] Loss: 0.29100500348204184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1171] Loss: 0.291006678057552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1172] Loss: 0.29099208539452043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1173] Loss: 0.2910304091384877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1174] Loss: 0.2910133042902037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1175] Loss: 0.29098623255178063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1176] Loss: 0.2909851554720229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1177] Loss: 0.29097853526800904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1178] Loss: 0.29096085871698507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1179] Loss: 0.2909590389998463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1180] Loss: 0.29095237031425397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1181] Loss: 0.2909560677661484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1182] Loss: 0.2909510396978956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1183] Loss: 0.29095611271457905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1184] Loss: 0.2909933075314941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1185] Loss: 0.29096715974283743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1186] Loss: 0.29094749229439926\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1187] Loss: 0.29093880841313596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1188] Loss: 0.2909303862925301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1189] Loss: 0.29094959803862536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1190] Loss: 0.2909374992018623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1191] Loss: 0.29091944283724747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1192] Loss: 0.29093040849415874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1193] Loss: 0.2909258174578098\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1194] Loss: 0.29091766110899553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1195] Loss: 0.2909077689010747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1196] Loss: 0.2908919046787123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1197] Loss: 0.2909198170604447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1198] Loss: 0.2909258210404034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1199] Loss: 0.29090976978626387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1200] Loss: 0.2908976167195296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1201] Loss: 0.29091882036402494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1202] Loss: 0.29093325252245106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1203] Loss: 0.2909175506799343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1204] Loss: 0.29093281496818924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1205] Loss: 0.2909148295223713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1206] Loss: 0.2909087378155645\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1207] Loss: 0.2908933772755237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1208] Loss: 0.2909146591043629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1209] Loss: 0.29089070640724696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1210] Loss: 0.29089348410434057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1211] Loss: 0.29087396414760297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1212] Loss: 0.2908685558575097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1213] Loss: 0.2908545903062227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1214] Loss: 0.29084066015039134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1215] Loss: 0.29084010734726073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1216] Loss: 0.29084649627625475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1217] Loss: 0.2908460607801114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1218] Loss: 0.2908650926688326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1219] Loss: 0.29085934084516946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1220] Loss: 0.29085370933284355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1221] Loss: 0.2908918448709086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1222] Loss: 0.2908846072030879\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1223] Loss: 0.2908668230937035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1224] Loss: 0.290875062228373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1225] Loss: 0.2908689416008569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1226] Loss: 0.29085676189068804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1227] Loss: 0.2909043048760566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1228] Loss: 0.2908930318771157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1229] Loss: 0.29086686206797413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1230] Loss: 0.2909123574253235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1231] Loss: 0.29090943390571156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1232] Loss: 0.29089267092580373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1233] Loss: 0.29090218640981835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1234] Loss: 0.2908905192188993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1235] Loss: 0.290906340043387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1236] Loss: 0.2909131412292841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1237] Loss: 0.2908969770007311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1238] Loss: 0.29088273700837236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1239] Loss: 0.29086948318327194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1240] Loss: 0.290842254606127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1241] Loss: 0.29082079530229454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1242] Loss: 0.29079293935711126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1243] Loss: 0.29078942571321703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1244] Loss: 0.29077685527713637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1245] Loss: 0.29077286919127204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1246] Loss: 0.29076340758736363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1247] Loss: 0.29076136583606965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1248] Loss: 0.29078214250917933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1249] Loss: 0.29079198499549025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1250] Loss: 0.29078959039050434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1251] Loss: 0.2907826594355205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1252] Loss: 0.29077553751619717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1253] Loss: 0.29078700623911524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1254] Loss: 0.29077164941210215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1255] Loss: 0.2907705847939261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1256] Loss: 0.2907442371152547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1257] Loss: 0.2907212131187498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1258] Loss: 0.2907136552786642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1259] Loss: 0.29070488950311085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1260] Loss: 0.290697016201807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1261] Loss: 0.2906746678277161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1262] Loss: 0.29066414260152923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1263] Loss: 0.29066889238935933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1264] Loss: 0.2906567341835218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1265] Loss: 0.2907042278624747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1266] Loss: 0.29069678896131956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1267] Loss: 0.29072913703987113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1268] Loss: 0.2907131784768204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1269] Loss: 0.29071942007392726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1270] Loss: 0.29073955906708066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1271] Loss: 0.29072419386695914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1272] Loss: 0.29072011317296736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1273] Loss: 0.2907169529850656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1274] Loss: 0.29070130971114144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1275] Loss: 0.2906907237146247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1276] Loss: 0.29067193007690745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1277] Loss: 0.2907223450166501\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1278] Loss: 0.2907482054972283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1279] Loss: 0.29073237825376025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1280] Loss: 0.2907186340538122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1281] Loss: 0.2907100660088038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1282] Loss: 0.29070607176177043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1283] Loss: 0.2907244507824221\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1284] Loss: 0.2907594649134039\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1285] Loss: 0.2907627286848822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1286] Loss: 0.290750575239638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1287] Loss: 0.2907787703383392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1288] Loss: 0.2907566188631642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1289] Loss: 0.29075829475464243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1290] Loss: 0.29075072198077156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1291] Loss: 0.290772290310739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1292] Loss: 0.2907578169520885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1293] Loss: 0.2907440904053921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1294] Loss: 0.2907326536751968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1295] Loss: 0.29073738420687123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1296] Loss: 0.2907306746388301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1297] Loss: 0.29071184687993745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1298] Loss: 0.2907352193627679\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1299] Loss: 0.29073022644939633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1300] Loss: 0.29073155423206926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1301] Loss: 0.2907200657060947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1302] Loss: 0.29072679078657915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1303] Loss: 0.29074798370090876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1304] Loss: 0.2907497869631554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1305] Loss: 0.29074868934380044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1306] Loss: 0.2907391077698026\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1307] Loss: 0.29072357083262385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1308] Loss: 0.2907651914755396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1309] Loss: 0.290748970692767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1310] Loss: 0.29078208633889063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1311] Loss: 0.29078427960333103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1312] Loss: 0.2907885011510358\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1313] Loss: 0.29080660395388597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1314] Loss: 0.2907894665528959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1315] Loss: 0.2908071286101573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1316] Loss: 0.2908128915130167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1317] Loss: 0.2908009323062796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1318] Loss: 0.2907918507425984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1319] Loss: 0.290768330381635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1320] Loss: 0.29074856155399365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1321] Loss: 0.290748660252736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1322] Loss: 0.2907361067943019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1323] Loss: 0.2907190856790729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1324] Loss: 0.29070099155879403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1325] Loss: 0.2907050533275399\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1326] Loss: 0.2907143409337947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1327] Loss: 0.29073406590489614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1328] Loss: 0.2907272734066137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1329] Loss: 0.2907114549748728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1330] Loss: 0.29073313609518187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1331] Loss: 0.290752486783059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1332] Loss: 0.2907536940257161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1333] Loss: 0.29073161642856027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1334] Loss: 0.2907194395211491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1335] Loss: 0.29072287872654273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1336] Loss: 0.290728382675007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1337] Loss: 0.2907127880614624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1338] Loss: 0.29069983124829196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1339] Loss: 0.29068441097698516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1340] Loss: 0.2906738776525717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1341] Loss: 0.2906702342430644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1342] Loss: 0.2906571589515497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1343] Loss: 0.29070296765383485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1344] Loss: 0.2906847207925063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1345] Loss: 0.2906743409652864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1346] Loss: 0.29064928757184744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1347] Loss: 0.290639865744005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1348] Loss: 0.29066139915473804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1349] Loss: 0.2906864195129264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1350] Loss: 0.2907006368671511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1351] Loss: 0.29068105982422926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1352] Loss: 0.29068157807786915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1353] Loss: 0.2906711261227295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1354] Loss: 0.2906678711380602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1355] Loss: 0.2906770354989391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1356] Loss: 0.2906982771719307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1357] Loss: 0.29071692258151577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1358] Loss: 0.2906990664370441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1359] Loss: 0.2906794022895678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1360] Loss: 0.2906579451082058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1361] Loss: 0.2906761877978213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1362] Loss: 0.2906828739715195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1363] Loss: 0.29068637827097105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1364] Loss: 0.2907119206619704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1365] Loss: 0.29070173583065856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1366] Loss: 0.2907115889808285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1367] Loss: 0.2906948896426996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1368] Loss: 0.29068505335921146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1369] Loss: 0.29074897945310524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1370] Loss: 0.290746187517009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1371] Loss: 0.29073497845808494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1372] Loss: 0.29071555311890257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1373] Loss: 0.29074551044186614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1374] Loss: 0.29074593916413677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1375] Loss: 0.2907506401639387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1376] Loss: 0.2907486762479995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1377] Loss: 0.29073466862924796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1378] Loss: 0.2907467467624883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1379] Loss: 0.2907671631877838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1380] Loss: 0.29076495857741125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1381] Loss: 0.29076699828634356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1382] Loss: 0.2907439103591437\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1383] Loss: 0.29074988368122484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1384] Loss: 0.2907505834751356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1385] Loss: 0.29078356383796683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1386] Loss: 0.29077594613442526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1387] Loss: 0.29078080967383346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1388] Loss: 0.29076874326428165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1389] Loss: 0.29077843519127794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1390] Loss: 0.2907632194683404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1391] Loss: 0.290795286103681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1392] Loss: 0.29077850388671744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1393] Loss: 0.2907685067775285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1394] Loss: 0.29076638522907733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1395] Loss: 0.29075307191930244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1396] Loss: 0.2907694018367708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1397] Loss: 0.2907546008303085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1398] Loss: 0.29073643137103133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1399] Loss: 0.2907213097871193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1400] Loss: 0.29071194371078074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1401] Loss: 0.2906951147868132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1402] Loss: 0.2907196543956735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1403] Loss: 0.29071983987965816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1404] Loss: 0.2907118541802672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1405] Loss: 0.29070697536071144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1406] Loss: 0.29068333640100635\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8951\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1407] Loss: 0.2906689738059753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1408] Loss: 0.2906743503176545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1409] Loss: 0.29069351057496184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1410] Loss: 0.29070340525718785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1411] Loss: 0.29068144949938723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1412] Loss: 0.29067133063659123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1413] Loss: 0.2906555888204814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1414] Loss: 0.29064310987611824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1415] Loss: 0.290628859127925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1416] Loss: 0.2906538073646532\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1417] Loss: 0.2906609939042324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1418] Loss: 0.29065306867591456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1419] Loss: 0.2906351077132619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1420] Loss: 0.2906224549599909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1421] Loss: 0.2906136507234829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1422] Loss: 0.29059945127483344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1423] Loss: 0.290587039320995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1424] Loss: 0.29060131888219787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1425] Loss: 0.29058605872114446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1426] Loss: 0.2905891629329959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1427] Loss: 0.29060577557116896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1428] Loss: 0.2905943345743482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1429] Loss: 0.29061649542189005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1430] Loss: 0.2906390972915678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1431] Loss: 0.29063507895634555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1432] Loss: 0.2906216260011617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1433] Loss: 0.29061560024082755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1434] Loss: 0.2906135650323688\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1435] Loss: 0.2906034494311132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1436] Loss: 0.2906226547794297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1437] Loss: 0.2906091313223556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1438] Loss: 0.2905910759244436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1439] Loss: 0.29057765837025984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1440] Loss: 0.2905889016364138\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1441] Loss: 0.2905829452292602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1442] Loss: 0.29059413584640936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1443] Loss: 0.29060053238168293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1444] Loss: 0.29059075745592733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1445] Loss: 0.2906062203125944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1446] Loss: 0.29062351492846017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1447] Loss: 0.2906427149463557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1448] Loss: 0.29061878055737095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1449] Loss: 0.2906533871685507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1450] Loss: 0.29062486292215434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1451] Loss: 0.2906210968019616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1452] Loss: 0.29065195912732433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1453] Loss: 0.2906530039403716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1454] Loss: 0.2906484536620335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1455] Loss: 0.29065034769327436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1456] Loss: 0.29062663624439317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1457] Loss: 0.2906033579137776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1458] Loss: 0.290595834294755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1459] Loss: 0.29057879596435743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1460] Loss: 0.29058657551663825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1461] Loss: 0.2905832111456837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1462] Loss: 0.29056429085678503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1463] Loss: 0.290550576213902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1464] Loss: 0.29053450193941666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1465] Loss: 0.29053253570285736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1466] Loss: 0.2905640221768595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1467] Loss: 0.2905679079207659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1468] Loss: 0.2905443902173623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1469] Loss: 0.29055086569786703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1470] Loss: 0.2906063534491754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1471] Loss: 0.29061648315801025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1472] Loss: 0.29064920730125765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1473] Loss: 0.2906282695549802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1474] Loss: 0.2906069914035303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1475] Loss: 0.2906213760336608\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1476] Loss: 0.29061696132522735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1477] Loss: 0.2906264171626161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1478] Loss: 0.29062921983884343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1479] Loss: 0.29062603150819266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1480] Loss: 0.2906333995534248\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1481] Loss: 0.2906507288696836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1482] Loss: 0.2906283968202873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1483] Loss: 0.2906419306857805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1484] Loss: 0.29064333692403405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1485] Loss: 0.2906383539242842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1486] Loss: 0.2906298913113707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1487] Loss: 0.2906305055217279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1488] Loss: 0.2906411238045157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1489] Loss: 0.2906325213750628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1490] Loss: 0.2906128307127921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1491] Loss: 0.29059772083922264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1492] Loss: 0.290577899339101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1493] Loss: 0.29057284000854494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1494] Loss: 0.29056261902109787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1495] Loss: 0.29056901353287445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1496] Loss: 0.2905698516600579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1497] Loss: 0.29059584949801515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1498] Loss: 0.2906330598830533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1499] Loss: 0.2906444422814339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1500] Loss: 0.2906649482169773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1501] Loss: 0.29065654703708715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1502] Loss: 0.29065783094819225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1503] Loss: 0.2906466020272653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1504] Loss: 0.29062883659328564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1505] Loss: 0.2906439333536515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1506] Loss: 0.290628799556737\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1507] Loss: 0.29065034283196195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1508] Loss: 0.29067563847261996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1509] Loss: 0.2906833333274382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1510] Loss: 0.29069309744942273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1511] Loss: 0.29071112571599106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1512] Loss: 0.2907132036642143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1513] Loss: 0.2907118824732683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1514] Loss: 0.29069471710484796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1515] Loss: 0.29070508258708516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1516] Loss: 0.2906828346016012\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1517] Loss: 0.29066753523014155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1518] Loss: 0.2907692671505252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1519] Loss: 0.2907662315894479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1520] Loss: 0.29076883731530984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1521] Loss: 0.2907916441718273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1522] Loss: 0.29088532387549065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1523] Loss: 0.2909318735467524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 14, Batch 1524] Loss: 0.2909115041668591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 0] Loss: 0.2909051963595033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1] Loss: 0.2908998798823876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 2] Loss: 0.2908948283604408\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 3] Loss: 0.2908727871172231\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 4] Loss: 0.290885300108731\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 5] Loss: 0.29087650435674384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 6] Loss: 0.29086615812886335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 7] Loss: 0.290847165973827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 8] Loss: 0.2908519386842761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 9] Loss: 0.29083099128425255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 10] Loss: 0.2908356428341659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 11] Loss: 0.2908358045860631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 12] Loss: 0.2908306440467355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 13] Loss: 0.29082341290513547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 14] Loss: 0.290816077880048\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 15] Loss: 0.29087846946817797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 16] Loss: 0.29087618550099337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 17] Loss: 0.29085866676289107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 18] Loss: 0.29084072540250555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 19] Loss: 0.29082103395847486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 20] Loss: 0.29082212692506965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 21] Loss: 0.29081016938663906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 22] Loss: 0.29080357150440594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 23] Loss: 0.29081139871154826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 24] Loss: 0.29078709174674694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 25] Loss: 0.2907785140041279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 26] Loss: 0.29077681773952435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 27] Loss: 0.2907991958152426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 28] Loss: 0.29077775618957674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 29] Loss: 0.2907692327197353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 30] Loss: 0.2907880776107701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 31] Loss: 0.2907847393269539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 32] Loss: 0.2907680677950102\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 33] Loss: 0.2907826963724007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 34] Loss: 0.2907979099392922\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 35] Loss: 0.2908049283387379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 36] Loss: 0.29079432764295937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 37] Loss: 0.29079087510601853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 38] Loss: 0.29078027873172807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 39] Loss: 0.29079688671076015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 40] Loss: 0.2907900083703098\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 41] Loss: 0.29082366734287973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 42] Loss: 0.29081396212063765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 43] Loss: 0.29082344499826585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 44] Loss: 0.2908318208176682\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 45] Loss: 0.2908143527509337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 46] Loss: 0.29081170934626865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 47] Loss: 0.2908081342697579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 48] Loss: 0.29079764608408426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 49] Loss: 0.29082370417216624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 50] Loss: 0.2908614653923987\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 51] Loss: 0.29087959276372644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 52] Loss: 0.29089290845645854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 53] Loss: 0.29088222465668323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 54] Loss: 0.2908725844876264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 55] Loss: 0.29087636926080973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 56] Loss: 0.2908638341584325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 57] Loss: 0.29085858900115036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 58] Loss: 0.29084909914192913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 59] Loss: 0.29084759128444565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 60] Loss: 0.29083231763215733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 61] Loss: 0.2908940100333076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 62] Loss: 0.2909036893121155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 63] Loss: 0.2908832703494132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 64] Loss: 0.2908763538295561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 65] Loss: 0.29088106923624535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 66] Loss: 0.29085501213321735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 67] Loss: 0.29086668362240803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 68] Loss: 0.29087959397312846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 69] Loss: 0.2908688769062298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 70] Loss: 0.2908500132241779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 71] Loss: 0.2908745754411837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 72] Loss: 0.2908968766245582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 73] Loss: 0.2908731572805526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 74] Loss: 0.29085549355726875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 75] Loss: 0.29084544449928446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 76] Loss: 0.2908612556354745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 77] Loss: 0.29085116751944073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 78] Loss: 0.2908687366477063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 79] Loss: 0.29090047876007885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 80] Loss: 0.2908826310768143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 81] Loss: 0.2908727659606466\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 82] Loss: 0.29086311185798747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 83] Loss: 0.2908728695921305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 84] Loss: 0.2908686673334439\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 85] Loss: 0.29088023869172075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 86] Loss: 0.29087997879663785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 87] Loss: 0.2908789369934911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 88] Loss: 0.29086290337106663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 89] Loss: 0.2908598085047979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 90] Loss: 0.2908520236331124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 91] Loss: 0.2908836076077885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 92] Loss: 0.2908808144313743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 93] Loss: 0.2908842361595061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 94] Loss: 0.2908752805758379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 95] Loss: 0.2908710192384383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 96] Loss: 0.29085054250599157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 97] Loss: 0.29084116446106056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 98] Loss: 0.2908579637263917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 99] Loss: 0.2908456753792193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 100] Loss: 0.2908568255265528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 101] Loss: 0.2908821047913718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 102] Loss: 0.2908724550012093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 103] Loss: 0.2908638249810196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 104] Loss: 0.29089209701960445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 105] Loss: 0.2908944661167833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 106] Loss: 0.29088362295266385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 107] Loss: 0.29089792689974564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 108] Loss: 0.29088865004877096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 109] Loss: 0.2908826405778942\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 110] Loss: 0.290919674355609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 111] Loss: 0.29096065234292173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 112] Loss: 0.29094911402450807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 113] Loss: 0.2909557793304357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 114] Loss: 0.29095988804370915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 115] Loss: 0.29095477838533956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 116] Loss: 0.29097748907126925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 117] Loss: 0.29099403146576847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 118] Loss: 0.2910214184156821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 119] Loss: 0.2910181575610373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 120] Loss: 0.2910308572663432\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 121] Loss: 0.2910063471368403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 122] Loss: 0.29103155546438875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 123] Loss: 0.2910641357783783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 124] Loss: 0.29108869423608735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 125] Loss: 0.2910850753576006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 126] Loss: 0.29108489482244443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 127] Loss: 0.2910819888043761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 128] Loss: 0.2910627927284959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 129] Loss: 0.2910530544506887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 130] Loss: 0.2910607566247063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 131] Loss: 0.29111385789456296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 132] Loss: 0.29111307748219517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 133] Loss: 0.29111016157898734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 134] Loss: 0.2911030520988412\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 135] Loss: 0.2910929184577528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 136] Loss: 0.29109949160841797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 137] Loss: 0.29110435484222363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 138] Loss: 0.2910875916071899\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 139] Loss: 0.2910679944357947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 140] Loss: 0.29104782644102406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 141] Loss: 0.29103453165610693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 142] Loss: 0.2910318765678884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 143] Loss: 0.29111970122363723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 144] Loss: 0.2911082104314838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 145] Loss: 0.2910998406280876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 146] Loss: 0.2911255496333178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 147] Loss: 0.29112739094082296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 148] Loss: 0.2911149131107297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 149] Loss: 0.2911061191583005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 150] Loss: 0.291093112345176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 151] Loss: 0.2910694579769172\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 152] Loss: 0.2910614930557272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 153] Loss: 0.2910740218091376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 154] Loss: 0.29105599125277454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 155] Loss: 0.2910385274292955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 156] Loss: 0.2910257334583305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 157] Loss: 0.29100900215332803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 158] Loss: 0.2909896457642049\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 159] Loss: 0.29100327944254445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 160] Loss: 0.2909973206874797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 161] Loss: 0.29099899231468307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 162] Loss: 0.2909951312713989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 163] Loss: 0.2910154253611999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 164] Loss: 0.29099919878804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 165] Loss: 0.29100457559852316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 166] Loss: 0.2909896313443669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 167] Loss: 0.29097929716500603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 168] Loss: 0.29096691582675277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 169] Loss: 0.2909478448845024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 170] Loss: 0.29093769730782476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 171] Loss: 0.29096189013702267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 172] Loss: 0.29097478331336313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 173] Loss: 0.2909942966618526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 174] Loss: 0.2909866651476638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 175] Loss: 0.29098785623019136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 176] Loss: 0.29102676257246457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 177] Loss: 0.2910061053921202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 178] Loss: 0.2910140393391179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 179] Loss: 0.2910188260155162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 180] Loss: 0.2910363133671956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 181] Loss: 0.2910356042729236\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 182] Loss: 0.2910322335913254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 183] Loss: 0.2910233091256962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 184] Loss: 0.2910288447434935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 185] Loss: 0.2910074632836793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 186] Loss: 0.2910051234228746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 187] Loss: 0.2909969568970232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 188] Loss: 0.29097660472540215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 189] Loss: 0.2910418923257759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 190] Loss: 0.29106719626659927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 191] Loss: 0.2910520541478693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 192] Loss: 0.29103038276231447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 193] Loss: 0.291053862154911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 194] Loss: 0.29105524445395503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 195] Loss: 0.2910460201245199\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 196] Loss: 0.29103124779701506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 197] Loss: 0.29101383022404514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 198] Loss: 0.29098870754947165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 199] Loss: 0.2909846124237479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 200] Loss: 0.2909847992431858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 201] Loss: 0.29098173941363764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 202] Loss: 0.29098014670396305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 203] Loss: 0.29096247217801624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 204] Loss: 0.29095891981475164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 205] Loss: 0.29095322954292874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 206] Loss: 0.29093490650559856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 207] Loss: 0.2909582901931181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 208] Loss: 0.2909631059782491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 209] Loss: 0.29096586389500667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 210] Loss: 0.290955521452739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 211] Loss: 0.2909323165989778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 212] Loss: 0.2909151157288145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 213] Loss: 0.29090398464712863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 214] Loss: 0.2909044707857333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 215] Loss: 0.29089938192874976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 216] Loss: 0.29094898945548786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 217] Loss: 0.29094700627927345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 218] Loss: 0.29094231101981566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 219] Loss: 0.29095329778533396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 220] Loss: 0.29093378239349293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 221] Loss: 0.2909109662743631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 222] Loss: 0.29091253976540216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 223] Loss: 0.2909359475154818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 224] Loss: 0.2909430747862178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 225] Loss: 0.290948627687815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 226] Loss: 0.29093908516748535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 227] Loss: 0.29091362163236495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 228] Loss: 0.2909063823876198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 229] Loss: 0.29089970135384263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 230] Loss: 0.290893239161581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 231] Loss: 0.29088234099240506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 232] Loss: 0.29086748020314496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 233] Loss: 0.2908959941411723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 234] Loss: 0.29089186306704173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 235] Loss: 0.29089746882796363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 236] Loss: 0.29088220359992745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 237] Loss: 0.29086778862161217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 238] Loss: 0.29084819800948847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 239] Loss: 0.2908456413574704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 240] Loss: 0.2908563121195892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 241] Loss: 0.2908378562804337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 242] Loss: 0.290819950114639\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 243] Loss: 0.29081048291879164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 244] Loss: 0.2908011704772362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 245] Loss: 0.2908167698346576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 246] Loss: 0.2908100223686369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 247] Loss: 0.2908005853285022\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 248] Loss: 0.29080935130033114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 249] Loss: 0.2907959168663487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 250] Loss: 0.2908162446259468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 251] Loss: 0.2908082398308034\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 252] Loss: 0.29079666625749984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 253] Loss: 0.2908262453219056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 254] Loss: 0.29085631235480186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 255] Loss: 0.29092227583366725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 256] Loss: 0.29090785547814707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 257] Loss: 0.2909018795852645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 258] Loss: 0.29089503841889364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 259] Loss: 0.29087574496583474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 260] Loss: 0.2909056833562285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 261] Loss: 0.29093335406582244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 262] Loss: 0.29092530804017236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 263] Loss: 0.29090737721954946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 264] Loss: 0.2909027692913359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 265] Loss: 0.290890397191917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 266] Loss: 0.2909041298910004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 267] Loss: 0.2909032451287639\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 268] Loss: 0.2909218670156148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 269] Loss: 0.2909087243142018\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 270] Loss: 0.29090835862569936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 271] Loss: 0.29089419425436486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 272] Loss: 0.2908822418106989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 273] Loss: 0.29090207747300434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 274] Loss: 0.290891223090878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 275] Loss: 0.2909055294106851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 276] Loss: 0.2908844551120591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 277] Loss: 0.29085955041880185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 278] Loss: 0.29087670081384454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 279] Loss: 0.29090125609204837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 280] Loss: 0.2909087218136727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 281] Loss: 0.2908999799094069\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 282] Loss: 0.2908858272624873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 283] Loss: 0.2908758819687748\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 284] Loss: 0.2908932495360764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 285] Loss: 0.2908990860268106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 286] Loss: 0.29088478388023054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 287] Loss: 0.2908574837234389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 288] Loss: 0.2908671820139608\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 289] Loss: 0.29084831594908367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 290] Loss: 0.29084461116233157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 291] Loss: 0.2908386734263994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 292] Loss: 0.2908513761785623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 293] Loss: 0.29085218752363035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 294] Loss: 0.29084878345767384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 295] Loss: 0.2908492611769401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 296] Loss: 0.290838633999492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 297] Loss: 0.2908352088812281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 298] Loss: 0.2908289941551409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 299] Loss: 0.2908267459155544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 300] Loss: 0.2908334448293905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 301] Loss: 0.2908591555308971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 302] Loss: 0.290861021735788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 303] Loss: 0.29084711822313325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 304] Loss: 0.2908515667882247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 305] Loss: 0.290823238036986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 306] Loss: 0.29081641639005407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 307] Loss: 0.29079529734604953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 308] Loss: 0.290793952658983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 309] Loss: 0.2908214193200656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 310] Loss: 0.2908292899535403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 311] Loss: 0.2908137868176251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 312] Loss: 0.29081601861242506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 313] Loss: 0.2907911255236278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 314] Loss: 0.2907880254771597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 315] Loss: 0.29077531560720254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 316] Loss: 0.290771251591298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 317] Loss: 0.29085184596934216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 318] Loss: 0.29083890716798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 319] Loss: 0.29084417186203526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 320] Loss: 0.2908279595826225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 321] Loss: 0.2908253915727086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 322] Loss: 0.290802881292374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 323] Loss: 0.2907867327240572\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 324] Loss: 0.2907883473929892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 325] Loss: 0.2907787434386337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 326] Loss: 0.290783010303164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 327] Loss: 0.2908075164684308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 328] Loss: 0.2908242601051994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 329] Loss: 0.290832566143072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 330] Loss: 0.2908209913181809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 331] Loss: 0.29083631341074445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 332] Loss: 0.2908282199915746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 333] Loss: 0.29087124050120716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 334] Loss: 0.29087865456792616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 335] Loss: 0.2908647263971685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 336] Loss: 0.29083904885354955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 337] Loss: 0.2908379542087136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 338] Loss: 0.29083041834861945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 339] Loss: 0.29081002128758554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 340] Loss: 0.29079994371245105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 341] Loss: 0.29078013214438636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 342] Loss: 0.29080687335364125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 343] Loss: 0.29080063898341374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 344] Loss: 0.29080126085704533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 345] Loss: 0.29078635779646156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 346] Loss: 0.29080480272139764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 347] Loss: 0.2907938320291181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 348] Loss: 0.29079748998044513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 349] Loss: 0.2908108014777473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 350] Loss: 0.29081275673721035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 351] Loss: 0.2908527378486863\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 352] Loss: 0.2908319770229996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 353] Loss: 0.29082193544091745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 354] Loss: 0.290837989430029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 355] Loss: 0.29082568984326895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 356] Loss: 0.2908265823853336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 357] Loss: 0.29084336225929247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 358] Loss: 0.2908653219768161\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 359] Loss: 0.2908939818731076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 360] Loss: 0.29089253788958663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 361] Loss: 0.2909075114864363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 362] Loss: 0.29091053797420746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 363] Loss: 0.29090986756992177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 364] Loss: 0.2908971181133751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 365] Loss: 0.29091456150571926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 366] Loss: 0.29089707490957056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 367] Loss: 0.29088364418302654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 368] Loss: 0.2908720891049685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 369] Loss: 0.29087516321047685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 370] Loss: 0.2908679514461301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 371] Loss: 0.2908942435729744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 372] Loss: 0.2909236216108437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 373] Loss: 0.29092541451945886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 374] Loss: 0.29090743357292825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 375] Loss: 0.2909152225256339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 376] Loss: 0.2909203848758225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 377] Loss: 0.2909215913415402\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 378] Loss: 0.29091502427141885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 379] Loss: 0.2909111731794793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 380] Loss: 0.29091235872637483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 381] Loss: 0.29090256304472567\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8842000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 382] Loss: 0.29089485188088127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 383] Loss: 0.2908851631580629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 384] Loss: 0.2908785334764526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 385] Loss: 0.2908540500273264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 386] Loss: 0.2908815566013894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 387] Loss: 0.29086950199779266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 388] Loss: 0.2908742655502199\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 389] Loss: 0.29096602676777017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 390] Loss: 0.29095216362124376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 391] Loss: 0.29096519728212705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 392] Loss: 0.29098545795160563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 393] Loss: 0.2910071811267429\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 394] Loss: 0.291018350301336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 395] Loss: 0.2910039709189573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 396] Loss: 0.29099957099808993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 397] Loss: 0.2909829356171283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 398] Loss: 0.2909760988958832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 399] Loss: 0.2909896265720471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 400] Loss: 0.29099810304134116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 401] Loss: 0.2910029819610707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 402] Loss: 0.29098132868559545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 403] Loss: 0.2909721625558117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 404] Loss: 0.2909991290756424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 405] Loss: 0.2910137345477057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 406] Loss: 0.29102321344790033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 407] Loss: 0.2910668243604994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 408] Loss: 0.291050527728333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 409] Loss: 0.2910764832492709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 410] Loss: 0.2910665035275638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 411] Loss: 0.2910675338054636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 412] Loss: 0.29107465341490546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 413] Loss: 0.29106862174775316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 414] Loss: 0.2910861665898142\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 415] Loss: 0.29106825241945367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 416] Loss: 0.29106202471172443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 417] Loss: 0.29105740245230033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 418] Loss: 0.29106183855720913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 419] Loss: 0.2910519588457395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 420] Loss: 0.29104652874709214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 421] Loss: 0.29106646487756893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 422] Loss: 0.2910597734801792\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 423] Loss: 0.29106668648002576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 424] Loss: 0.29108173707764073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 425] Loss: 0.2910825514595208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 426] Loss: 0.29109242831185744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 427] Loss: 0.29108754547487153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 428] Loss: 0.2910808879024836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 429] Loss: 0.2910624376662119\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 430] Loss: 0.2910837811685127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 431] Loss: 0.29107283294263825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 432] Loss: 0.29107858549564763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 433] Loss: 0.2910664390985838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 434] Loss: 0.2910678203624488\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 435] Loss: 0.2910559005568221\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 436] Loss: 0.29104019307957774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 437] Loss: 0.29104145059048336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 438] Loss: 0.2910534902199544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 439] Loss: 0.2910412381980848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 440] Loss: 0.29103953204309524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 441] Loss: 0.2910210485854925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 442] Loss: 0.2910209906739637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 443] Loss: 0.2910204439270471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 444] Loss: 0.2910065976468595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 445] Loss: 0.29101063463252597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 446] Loss: 0.29100715698101165\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 447] Loss: 0.2910033493441647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 448] Loss: 0.29099265414802195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 449] Loss: 0.29099713350383155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 450] Loss: 0.2909748788853782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 451] Loss: 0.29095710498934974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 452] Loss: 0.2909513622830641\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 453] Loss: 0.29094251794940135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 454] Loss: 0.29092817912734115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 455] Loss: 0.2909579191220803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 456] Loss: 0.2909591142125077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 457] Loss: 0.2909444905932814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 458] Loss: 0.2909482251691343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 459] Loss: 0.2909352950357995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 460] Loss: 0.29092558692347736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 461] Loss: 0.2909250977470901\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 462] Loss: 0.29096693513959654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 463] Loss: 0.29096699884334415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 464] Loss: 0.2909699299652452\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 465] Loss: 0.290959988930709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 466] Loss: 0.29097320040724317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 467] Loss: 0.2909655747477979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 468] Loss: 0.2909479854761095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 469] Loss: 0.2909209824788112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 470] Loss: 0.29090211728520976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 471] Loss: 0.29090354548604447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 472] Loss: 0.2909030700475346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 473] Loss: 0.2908810804620243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 474] Loss: 0.29087040320587415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 475] Loss: 0.2909010474846789\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 476] Loss: 0.29090969746365497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 477] Loss: 0.2909287511787106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 478] Loss: 0.29096322004501834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 479] Loss: 0.2909405833032708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 480] Loss: 0.2909249246313616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 481] Loss: 0.29092495417763403\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 482] Loss: 0.2909100164528968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 483] Loss: 0.29090090033196087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 484] Loss: 0.2908763132211619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 485] Loss: 0.29085221682835477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 486] Loss: 0.2908905661857234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 487] Loss: 0.2908768701227273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 488] Loss: 0.290873318297681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 489] Loss: 0.29087334755247185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 490] Loss: 0.2908900073815583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 491] Loss: 0.290869671051775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 492] Loss: 0.2908747680990404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 493] Loss: 0.2908852766534851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 494] Loss: 0.29095169094808737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 495] Loss: 0.2909436579543503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 496] Loss: 0.2909174429481329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 497] Loss: 0.290898024621886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 498] Loss: 0.29092558753364894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 499] Loss: 0.2909076915828373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 500] Loss: 0.2909091482177524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 501] Loss: 0.2909039595008278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 502] Loss: 0.29088335122286585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 503] Loss: 0.290863739536869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 504] Loss: 0.2908530580544783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 505] Loss: 0.29084872461649086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 506] Loss: 0.29083428183470106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 507] Loss: 0.2908236756379921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 508] Loss: 0.2908129703004177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 509] Loss: 0.2908211202694197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 510] Loss: 0.29082016886233697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 511] Loss: 0.29080591455345706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 512] Loss: 0.2907886883263159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 513] Loss: 0.2907796812558781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 514] Loss: 0.2907622384389701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 515] Loss: 0.2907552971620829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 516] Loss: 0.29075846101103336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 517] Loss: 0.29077006952283835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 518] Loss: 0.29075875450232364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 519] Loss: 0.2908038778223911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 520] Loss: 0.2907984023616267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 521] Loss: 0.2907973205193213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 522] Loss: 0.2907738308906116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 523] Loss: 0.290752327138034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 524] Loss: 0.2907351872376907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 525] Loss: 0.29072278948961555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 526] Loss: 0.2907283644736355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 527] Loss: 0.29071205772475056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 528] Loss: 0.2907212148724553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 529] Loss: 0.29071759579298406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 530] Loss: 0.29073428278320407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 531] Loss: 0.2907169523297344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 532] Loss: 0.29074741498191525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 533] Loss: 0.29073062539100647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 534] Loss: 0.29073236310689626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 535] Loss: 0.2907233562446094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 536] Loss: 0.2907089241645598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 537] Loss: 0.2907035822715335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 538] Loss: 0.2906970600285902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 539] Loss: 0.29068044910486135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 540] Loss: 0.29065534579476304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 541] Loss: 0.29064727826287035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 542] Loss: 0.29063721977336765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 543] Loss: 0.2906274689254436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 544] Loss: 0.29062097480941324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 545] Loss: 0.29063397270364405\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 546] Loss: 0.2906285224958229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 547] Loss: 0.2906195110245377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 548] Loss: 0.29062834771734963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 549] Loss: 0.29061103732484633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 550] Loss: 0.29058811427475917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 551] Loss: 0.2905935631207132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 552] Loss: 0.29058125596703177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 553] Loss: 0.2905699350737481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 554] Loss: 0.2905671224654348\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 555] Loss: 0.29055974850049443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 556] Loss: 0.29056158479405186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 557] Loss: 0.2905482391670658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 558] Loss: 0.2905634772820559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 559] Loss: 0.2905564104753538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 560] Loss: 0.2905479761910759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 561] Loss: 0.2905436720074775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 562] Loss: 0.2905428823301274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 563] Loss: 0.29055008329892673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 564] Loss: 0.29055646617049286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 565] Loss: 0.2905412733918949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 566] Loss: 0.29054468022984803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 567] Loss: 0.2905482657920809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 568] Loss: 0.2905572187546759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 569] Loss: 0.29054267437543346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 570] Loss: 0.2905374409078242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 571] Loss: 0.2905212660355237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 572] Loss: 0.290518230309317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 573] Loss: 0.29050574381389593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 574] Loss: 0.29055079141273016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 575] Loss: 0.2905347062437326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 576] Loss: 0.29053662026463745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 577] Loss: 0.2905151305041459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 578] Loss: 0.29051688100999956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 579] Loss: 0.2904936698500391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 580] Loss: 0.29050013627039223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 581] Loss: 0.2904913839501966\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 582] Loss: 0.2904815496474875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 583] Loss: 0.29046536030005526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 584] Loss: 0.2904933259501392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 585] Loss: 0.29047473409167157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 586] Loss: 0.2904634443112853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 587] Loss: 0.29047206579273505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 588] Loss: 0.29044789262660586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 589] Loss: 0.29045445262979164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 590] Loss: 0.29045222969532886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 591] Loss: 0.2904514533120174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 592] Loss: 0.29043441667383435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 593] Loss: 0.290448131635522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 594] Loss: 0.29044627695042113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 595] Loss: 0.2904622234898374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 596] Loss: 0.29048739321268696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 597] Loss: 0.2904936910977659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 598] Loss: 0.2904778084207492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 599] Loss: 0.290471201455931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 600] Loss: 0.2904751729011681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 601] Loss: 0.2904566428552157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 602] Loss: 0.29044269787419946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 603] Loss: 0.29045448595302087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 604] Loss: 0.29043845725445083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 605] Loss: 0.29043427174214537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 606] Loss: 0.29042261308333084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 607] Loss: 0.29044358600739273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 608] Loss: 0.2904597939158245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 609] Loss: 0.2904648351063052\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 610] Loss: 0.29043672897627765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 611] Loss: 0.290447878725433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 612] Loss: 0.29047629543678766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 613] Loss: 0.2904676114964982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 614] Loss: 0.2904799517329157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 615] Loss: 0.29048839521475034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 616] Loss: 0.29049347685410265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 617] Loss: 0.2905045884254453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 618] Loss: 0.29051106015835737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 619] Loss: 0.29052749129166183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 620] Loss: 0.29052669805456494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 621] Loss: 0.29052502977424577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 622] Loss: 0.29053673943158015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 623] Loss: 0.2905192272562498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 624] Loss: 0.2905206552818273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 625] Loss: 0.2905141882806239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 626] Loss: 0.29052831287347386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 627] Loss: 0.29053257441940245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 628] Loss: 0.2905464829063128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 629] Loss: 0.29054093468066766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 630] Loss: 0.29056792034750634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 631] Loss: 0.2905983800492226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 632] Loss: 0.2905803065366909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 633] Loss: 0.2905685575456637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 634] Loss: 0.29055800057879033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 635] Loss: 0.29054755237312263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 636] Loss: 0.2905535045708891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 637] Loss: 0.2905426143294236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 638] Loss: 0.29052443258720567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 639] Loss: 0.2905071774852386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 640] Loss: 0.29050764980394456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 641] Loss: 0.290492643545711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 642] Loss: 0.2905324708771378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 643] Loss: 0.2905196097863041\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 644] Loss: 0.2905060826027216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 645] Loss: 0.29051074569586255\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 646] Loss: 0.29053478141286215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 647] Loss: 0.2905282226179457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 648] Loss: 0.29052953185296443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 649] Loss: 0.2905275028319244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 650] Loss: 0.29052359080561485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 651] Loss: 0.2905185528400407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 652] Loss: 0.2905142407623846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 653] Loss: 0.29049375783177006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 654] Loss: 0.2904871160055067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 655] Loss: 0.29046792559258167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 656] Loss: 0.2904558385352781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 657] Loss: 0.2904613715750311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 658] Loss: 0.29045771982806134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 659] Loss: 0.29044534441707776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 660] Loss: 0.29046660362819327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 661] Loss: 0.290453490191033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 662] Loss: 0.290439905051796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 663] Loss: 0.2904693888925123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 664] Loss: 0.2904596074499129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 665] Loss: 0.2904696793933374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 666] Loss: 0.2904626927589186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 667] Loss: 0.2904441999677217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 668] Loss: 0.2904363360177747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 669] Loss: 0.2904469334838481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 670] Loss: 0.29044372740867846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 671] Loss: 0.29042484663887874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 672] Loss: 0.29040406811065894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 673] Loss: 0.2904220089593979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 674] Loss: 0.2904098141338377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 675] Loss: 0.2904004278003882\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 676] Loss: 0.2903875611958668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 677] Loss: 0.2903969837955406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 678] Loss: 0.290377056199198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 679] Loss: 0.29044965131257455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 680] Loss: 0.29047309491424317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 681] Loss: 0.29047479665625126\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 682] Loss: 0.29047579461831163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 683] Loss: 0.2904806365057652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 684] Loss: 0.2904810469381346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 685] Loss: 0.29050508768918426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 686] Loss: 0.29049176028074125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 687] Loss: 0.29048886538346536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 688] Loss: 0.2904686462084799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 689] Loss: 0.29045705618631584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 690] Loss: 0.29045255890992505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 691] Loss: 0.2904391468997546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 692] Loss: 0.290437719587925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 693] Loss: 0.29042036886543526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 694] Loss: 0.2904048793359297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 695] Loss: 0.29038531167664516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 696] Loss: 0.2903703070057985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 697] Loss: 0.2903730781641168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 698] Loss: 0.29035315156138825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 699] Loss: 0.2903648084032914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 700] Loss: 0.29036010966519826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 701] Loss: 0.29034552448327877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 702] Loss: 0.2903490735697355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 703] Loss: 0.29035128431895324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 704] Loss: 0.2903441085991687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 705] Loss: 0.29036020612528735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 706] Loss: 0.2903654193919516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 707] Loss: 0.29035817592364405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 708] Loss: 0.2903529661242366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 709] Loss: 0.2903621783915016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 710] Loss: 0.29035208524480943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 711] Loss: 0.2903505122456076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 712] Loss: 0.2903343277513459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 713] Loss: 0.2903117778920353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 714] Loss: 0.29030413613061284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 715] Loss: 0.2903058017268285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 716] Loss: 0.2903207522846451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 717] Loss: 0.29030908786249243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 718] Loss: 0.2902978137738665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 719] Loss: 0.29029156047091853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 720] Loss: 0.29027794942179386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 721] Loss: 0.29028290090663744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 722] Loss: 0.2902658115241795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 723] Loss: 0.29025770015063496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 724] Loss: 0.29027649412138795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 725] Loss: 0.2902568790242478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 726] Loss: 0.29026713507710145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 727] Loss: 0.2902576917938855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 728] Loss: 0.29024146050241545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 729] Loss: 0.2902511167463242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 730] Loss: 0.2902359494205542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 731] Loss: 0.29025320375032404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 732] Loss: 0.29024559039385256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 733] Loss: 0.29022644899003264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 734] Loss: 0.29022502328157923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 735] Loss: 0.29021670869114086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 736] Loss: 0.29024040602127965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 737] Loss: 0.29026995568913005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 738] Loss: 0.29027775750607143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 739] Loss: 0.29028773584244555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 740] Loss: 0.29034469330629875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 741] Loss: 0.29032376421007966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 742] Loss: 0.29031647474805056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 743] Loss: 0.2903034227441283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 744] Loss: 0.29029868229544875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 745] Loss: 0.2902899128938783\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 746] Loss: 0.2903004356459791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 747] Loss: 0.290337344512452\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 748] Loss: 0.29039522376842797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 749] Loss: 0.2903969634962183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 750] Loss: 0.2903863881593359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 751] Loss: 0.2904032357410887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 752] Loss: 0.29042076573756553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 753] Loss: 0.2904187227396552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 754] Loss: 0.2904104573844774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 755] Loss: 0.29039221651651964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 756] Loss: 0.29039453714863606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 757] Loss: 0.2903976337805211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 758] Loss: 0.29045197494082575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 759] Loss: 0.2904715368059366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 760] Loss: 0.29045069713927896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 761] Loss: 0.2904962986532557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 762] Loss: 0.29051262893465346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 763] Loss: 0.2905013635314643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 764] Loss: 0.2905583442131036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 765] Loss: 0.2905891673202911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 766] Loss: 0.2905732478072208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 767] Loss: 0.2905614660583997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 768] Loss: 0.29053734048346525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 769] Loss: 0.2905465617339842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 770] Loss: 0.29055198390654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 771] Loss: 0.29057665034585184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 772] Loss: 0.2905756169721463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 773] Loss: 0.29060146276981447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 774] Loss: 0.2905980977843333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 775] Loss: 0.2906088644240391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 776] Loss: 0.2906086700935167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 777] Loss: 0.290618154142756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 778] Loss: 0.29061664595612674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 779] Loss: 0.29062853464334476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 780] Loss: 0.29063290846977563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 781] Loss: 0.29062013729250286\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 782] Loss: 0.29062778429589126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 783] Loss: 0.2906102783637509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 784] Loss: 0.2906056075950447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 785] Loss: 0.2905901558369239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 786] Loss: 0.29059041035987565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 787] Loss: 0.2906021188895392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 788] Loss: 0.2906361639204583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 789] Loss: 0.29068993329461357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 790] Loss: 0.29068436700890593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 791] Loss: 0.29067107287416377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 792] Loss: 0.2906983679016991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 793] Loss: 0.29075175711107315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 794] Loss: 0.2907597262780677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 795] Loss: 0.29077005270853046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 796] Loss: 0.29076610012962606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 797] Loss: 0.29075344999858443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 798] Loss: 0.29074426789751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 799] Loss: 0.2907431257497296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 800] Loss: 0.29073794480554777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 801] Loss: 0.2907299608033756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 802] Loss: 0.2907221320330056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 803] Loss: 0.2907285952484376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 804] Loss: 0.29073859515563644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 805] Loss: 0.29072931244704064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 806] Loss: 0.29071809602169635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 807] Loss: 0.29071553415866724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 808] Loss: 0.2906980392950537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 809] Loss: 0.2907389907776489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 810] Loss: 0.29072689563522064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 811] Loss: 0.2907312425612774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 812] Loss: 0.2907236225764308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 813] Loss: 0.29071112239633284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 814] Loss: 0.2907078218064851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 815] Loss: 0.29070847143408596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 816] Loss: 0.2907097728281768\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 817] Loss: 0.290692090078141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 818] Loss: 0.2906985821705254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 819] Loss: 0.29069375788715934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 820] Loss: 0.2906933816083623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 821] Loss: 0.2907179810500423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 822] Loss: 0.290710751467235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 823] Loss: 0.2907037684786811\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 824] Loss: 0.29072591021422284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 825] Loss: 0.2907083620570375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 826] Loss: 0.29071626399156114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 827] Loss: 0.29070741832882563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 828] Loss: 0.2906821351679162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 829] Loss: 0.2906808308782606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 830] Loss: 0.29067770570384327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 831] Loss: 0.29072704583312997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 832] Loss: 0.29071154252930564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 833] Loss: 0.2907047952465492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 834] Loss: 0.2907128866007523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 835] Loss: 0.2907062654525024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 836] Loss: 0.2907211717747702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 837] Loss: 0.29071115010060206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 838] Loss: 0.290695310025248\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 839] Loss: 0.2907033617889991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 840] Loss: 0.2907021619595288\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 841] Loss: 0.2906975075034558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 842] Loss: 0.29072308256699175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 843] Loss: 0.2907505137404588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 844] Loss: 0.29076117694259945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 845] Loss: 0.2907643545104319\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 846] Loss: 0.2907646653763184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 847] Loss: 0.2907527118214676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 848] Loss: 0.2907648195741794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 849] Loss: 0.29076337802938323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 850] Loss: 0.290745918878927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 851] Loss: 0.2907524062715283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 852] Loss: 0.2907648545424291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 853] Loss: 0.29076979143786186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 854] Loss: 0.2907983720296427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 855] Loss: 0.29078172750749065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 856] Loss: 0.2907671521687217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 857] Loss: 0.2907550366540675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 858] Loss: 0.29078476044531065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 859] Loss: 0.2907684550619847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 860] Loss: 0.2907735868097933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 861] Loss: 0.2907519225804659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 862] Loss: 0.2907346616736513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 863] Loss: 0.2907257781208236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 864] Loss: 0.2907333219922703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 865] Loss: 0.29072099855289296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 866] Loss: 0.29071079529101923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 867] Loss: 0.29070992832115644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 868] Loss: 0.2907338393321059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 869] Loss: 0.2907412546447771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 870] Loss: 0.29074274145891654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 871] Loss: 0.2907234452622734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 872] Loss: 0.2907163508721591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 873] Loss: 0.29072315166518486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 874] Loss: 0.2907232672878685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 875] Loss: 0.2907225539191917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 876] Loss: 0.2907164639739842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 877] Loss: 0.2907165951826923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 878] Loss: 0.29070281329816594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 879] Loss: 0.29069700451105995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 880] Loss: 0.290701571784037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 881] Loss: 0.29070418700838757\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8957\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 882] Loss: 0.29070499706973935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 883] Loss: 0.2907074128031051\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 884] Loss: 0.2907039977102834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 885] Loss: 0.2907423777909821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 886] Loss: 0.29073752520368107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 887] Loss: 0.2907525375708754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 888] Loss: 0.2907415566682206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 889] Loss: 0.2907245351157698\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 890] Loss: 0.2907210356484744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 891] Loss: 0.2907302875512531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 892] Loss: 0.2907438130416431\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 893] Loss: 0.29075931668549365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 894] Loss: 0.2907691006867019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 895] Loss: 0.2907718533992382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 896] Loss: 0.29077757189523634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 897] Loss: 0.29080117888878987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 898] Loss: 0.290793632378224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 899] Loss: 0.29080404311353814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 900] Loss: 0.2908133418281768\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 901] Loss: 0.29080404277434935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 902] Loss: 0.29078499549889236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 903] Loss: 0.2907790367517878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 904] Loss: 0.29081813847066607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 905] Loss: 0.29081649965948025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 906] Loss: 0.29080972699970603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 907] Loss: 0.29080003602783006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 908] Loss: 0.29078511988847233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 909] Loss: 0.29078435167575906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 910] Loss: 0.2908038902573576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 911] Loss: 0.29081255890662205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 912] Loss: 0.2908045449820284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 913] Loss: 0.29082419983427077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 914] Loss: 0.2908155515476562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 915] Loss: 0.29081569474319235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 916] Loss: 0.290818124269803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 917] Loss: 0.2908082143082312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 918] Loss: 0.290791246228044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 919] Loss: 0.29077187132224264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 920] Loss: 0.29076723237445606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 921] Loss: 0.2907626439845928\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 922] Loss: 0.29077473060090336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 923] Loss: 0.2907596642172624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 924] Loss: 0.2907425506543612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 925] Loss: 0.2907512431071647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 926] Loss: 0.2907379933809672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 927] Loss: 0.2907281827638794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 928] Loss: 0.2907045009303237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 929] Loss: 0.2907138685493745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 930] Loss: 0.2907188359769987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 931] Loss: 0.2907046695578455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 932] Loss: 0.29072101720477217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 933] Loss: 0.2907613515884371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 934] Loss: 0.29074180303396246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 935] Loss: 0.29073927450787945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 936] Loss: 0.29075622174613674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 937] Loss: 0.29077591629979893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 938] Loss: 0.2907607274312377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 939] Loss: 0.2907703393119903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 940] Loss: 0.2907778298974072\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 941] Loss: 0.2908018213102694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 942] Loss: 0.29078908238400714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 943] Loss: 0.2908040706847443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 944] Loss: 0.2908657078053985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 945] Loss: 0.2908561324527823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 946] Loss: 0.2908733343862531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 947] Loss: 0.29088215685635543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 948] Loss: 0.29086738274964036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 949] Loss: 0.29084664671415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 950] Loss: 0.29084650088784086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 951] Loss: 0.2908329426492092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 952] Loss: 0.2908302636971811\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 953] Loss: 0.29083552937331103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 954] Loss: 0.2908532020775156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 955] Loss: 0.2908353456902608\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 956] Loss: 0.2908492549101009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 957] Loss: 0.29084626880375686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 958] Loss: 0.29082111741172306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 959] Loss: 0.2908427020697059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 960] Loss: 0.2908526074492952\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 961] Loss: 0.29089851875859685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 962] Loss: 0.2909008465499811\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 963] Loss: 0.29088139743156244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 964] Loss: 0.2908705461849281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 965] Loss: 0.2908574000595438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 966] Loss: 0.29090703736219253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 967] Loss: 0.29089293739340544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 968] Loss: 0.29089616899064974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 969] Loss: 0.2909046568623597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 970] Loss: 0.2909135575791142\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 971] Loss: 0.290903387101461\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 972] Loss: 0.2908841539048581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 973] Loss: 0.29087878787909804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 974] Loss: 0.29085743459137076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 975] Loss: 0.29086238237671064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 976] Loss: 0.2908530463709415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 977] Loss: 0.29085552828433875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 978] Loss: 0.2908612760700239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 979] Loss: 0.2908687874443456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 980] Loss: 0.2908641628171642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 981] Loss: 0.2908470998536035\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 982] Loss: 0.29084456371049483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 983] Loss: 0.2908522712009827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 984] Loss: 0.29085152085458316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 985] Loss: 0.29084290926632045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 986] Loss: 0.29086264604782675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 987] Loss: 0.2908859237430368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 988] Loss: 0.2908765048487112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 989] Loss: 0.29086615658542714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 990] Loss: 0.2908968758339908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 991] Loss: 0.2908995749517339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 992] Loss: 0.2909115153878297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 993] Loss: 0.29091230971622134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 994] Loss: 0.29090551179294205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 995] Loss: 0.29091061289815173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 996] Loss: 0.29091352791401764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 997] Loss: 0.2909086304963728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 998] Loss: 0.2908919907878954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 999] Loss: 0.29090659530023605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1000] Loss: 0.29094157616830413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1001] Loss: 0.29095036817029446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1002] Loss: 0.2909525657429984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1003] Loss: 0.2909526336801193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1004] Loss: 0.29095267076589554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1005] Loss: 0.2909348612950332\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1006] Loss: 0.29094125780065555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1007] Loss: 0.2909342710520773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1008] Loss: 0.2909756026308039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1009] Loss: 0.2909614298147728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1010] Loss: 0.29097442880557767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1011] Loss: 0.2909627474357591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1012] Loss: 0.29097963126369364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1013] Loss: 0.2909955074808923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1014] Loss: 0.2909744086862818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1015] Loss: 0.2909659665011055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1016] Loss: 0.29095878073968934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1017] Loss: 0.29096504847269045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1018] Loss: 0.2909699431714339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1019] Loss: 0.29097855315457577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1020] Loss: 0.29097264760682634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1021] Loss: 0.29098538668068963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1022] Loss: 0.2909854204648038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1023] Loss: 0.2909833594236877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1024] Loss: 0.29098921829911295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1025] Loss: 0.29102329778759173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1026] Loss: 0.29102150360417467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1027] Loss: 0.2910105033376023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1028] Loss: 0.29100474938845033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1029] Loss: 0.2910045393867422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1030] Loss: 0.29101076833386985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1031] Loss: 0.2909976432648763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1032] Loss: 0.2909849906612362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1033] Loss: 0.2909709531609363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1034] Loss: 0.29097963797554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1035] Loss: 0.29097623241379866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1036] Loss: 0.2909751340653059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1037] Loss: 0.2909822093943181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1038] Loss: 0.2909917209829234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1039] Loss: 0.29098487136220186\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1040] Loss: 0.2909721202868753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1041] Loss: 0.2909631338639336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1042] Loss: 0.29095959669541155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1043] Loss: 0.29095615803099706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1044] Loss: 0.29094767350478173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1045] Loss: 0.2909384577774238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1046] Loss: 0.29092486931912526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1047] Loss: 0.2909304214888816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1048] Loss: 0.2909445309767394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1049] Loss: 0.29095988454440497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1050] Loss: 0.2909651973681157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1051] Loss: 0.29095154075915874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1052] Loss: 0.290973129394856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1053] Loss: 0.29098680487039397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1054] Loss: 0.2909769434370195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1055] Loss: 0.2909600396797877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1056] Loss: 0.29097529732200983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1057] Loss: 0.2910045197653656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1058] Loss: 0.29099987980899555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1059] Loss: 0.2909888445275976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1060] Loss: 0.29103444252240424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1061] Loss: 0.29106938658337306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1062] Loss: 0.2910735547757971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1063] Loss: 0.2910570062398327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1064] Loss: 0.29104411734104196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1065] Loss: 0.29105607109872444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1066] Loss: 0.29105424927601187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1067] Loss: 0.29106150978312706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1068] Loss: 0.2910488229463628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1069] Loss: 0.29103482797628705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1070] Loss: 0.2910156659820515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1071] Loss: 0.2910347645818337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1072] Loss: 0.29103307499993014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1073] Loss: 0.2910321193868178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1074] Loss: 0.2910234949139958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1075] Loss: 0.2910201412938908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1076] Loss: 0.2910013751370152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1077] Loss: 0.2910039389904983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1078] Loss: 0.29103856613728046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1079] Loss: 0.29104028574472285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1080] Loss: 0.2910232391966314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1081] Loss: 0.291026548639928\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1082] Loss: 0.29102053029505076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1083] Loss: 0.2910771015661243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1084] Loss: 0.2910791933789048\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1085] Loss: 0.2910646496367379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1086] Loss: 0.29107764087850174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1087] Loss: 0.2911192183291356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1088] Loss: 0.29111657803086755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1089] Loss: 0.29111032473463005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1090] Loss: 0.29110642017328303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1091] Loss: 0.29109780173891997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1092] Loss: 0.29108879186642334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1093] Loss: 0.2911018935800399\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1094] Loss: 0.2911011480647427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1095] Loss: 0.2911126122310356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1096] Loss: 0.29110144022270906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1097] Loss: 0.2911050713760933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1098] Loss: 0.291094833721748\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1099] Loss: 0.29109671538752735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1100] Loss: 0.29108530610688715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1101] Loss: 0.29109489258855886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1102] Loss: 0.2910784921307536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1103] Loss: 0.2910670201585276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1104] Loss: 0.29107177029297626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1105] Loss: 0.29105981778236034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1106] Loss: 0.2910959184056209\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1107] Loss: 0.29111878982792816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1108] Loss: 0.2911068061746548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1109] Loss: 0.2910947272051943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1110] Loss: 0.29108928299433245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1111] Loss: 0.2910889919700901\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1112] Loss: 0.29111254205812065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1113] Loss: 0.29109615661638116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1114] Loss: 0.2910945662759024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1115] Loss: 0.29110378228046924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1116] Loss: 0.29108432919638155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1117] Loss: 0.2910830888735582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1118] Loss: 0.2910902884375908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1119] Loss: 0.2911055959549706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1120] Loss: 0.29109725200057507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1121] Loss: 0.2911005081661525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1122] Loss: 0.2910993980340535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1123] Loss: 0.2911222120341121\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1124] Loss: 0.2911162608897881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1125] Loss: 0.291129827763078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1126] Loss: 0.291129965730831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1127] Loss: 0.2911836016363991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1128] Loss: 0.291170561672957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1129] Loss: 0.29117989821423595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1130] Loss: 0.29121220821610516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1131] Loss: 0.2912191714831365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1132] Loss: 0.2912377025076517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1133] Loss: 0.29124056423987565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1134] Loss: 0.29123265355925854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1135] Loss: 0.29124188286952807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1136] Loss: 0.29126352753129425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1137] Loss: 0.2912514978470773\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1138] Loss: 0.291255528716399\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1139] Loss: 0.2912611992765384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1140] Loss: 0.29127054820687737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1141] Loss: 0.29126922330875304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1142] Loss: 0.29125478681703254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1143] Loss: 0.29129489118997043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1144] Loss: 0.29129243688296574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1145] Loss: 0.29128046319636697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1146] Loss: 0.2912710097253139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1147] Loss: 0.2912728330608042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1148] Loss: 0.29126858587137144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1149] Loss: 0.2912569022990745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1150] Loss: 0.2912520177632638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1151] Loss: 0.29127130526490785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1152] Loss: 0.2912809490776169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1153] Loss: 0.29127606458704636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1154] Loss: 0.2912820118048691\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1155] Loss: 0.2912665306396804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1156] Loss: 0.29126207828615114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1157] Loss: 0.29126967307168705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1158] Loss: 0.2912713672700946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1159] Loss: 0.2912659929978921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1160] Loss: 0.2912547953066189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1161] Loss: 0.2912677258924252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1162] Loss: 0.29129132615842235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1163] Loss: 0.2913075654347441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1164] Loss: 0.291297829926662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1165] Loss: 0.29131495337010993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1166] Loss: 0.29131553612052447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1167] Loss: 0.291339529776795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1168] Loss: 0.2913287964560054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1169] Loss: 0.2913108889536146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1170] Loss: 0.2913148931027374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1171] Loss: 0.29131539481718605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1172] Loss: 0.2913231148369258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1173] Loss: 0.2913336921510817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1174] Loss: 0.29133002903911437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1175] Loss: 0.29131921733548993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1176] Loss: 0.2913173881726591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1177] Loss: 0.2913014400613617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1178] Loss: 0.29134486474022253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1179] Loss: 0.29133510503527155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1180] Loss: 0.29133625066034835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1181] Loss: 0.29131666544616164\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1182] Loss: 0.2913508288713815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1183] Loss: 0.2913618051395875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1184] Loss: 0.2913430339742667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1185] Loss: 0.2913458907290286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1186] Loss: 0.29134153391486495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1187] Loss: 0.2913252782041489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1188] Loss: 0.29134405249912465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1189] Loss: 0.29133225358005127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1190] Loss: 0.29132610784630025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1191] Loss: 0.29130691787833224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1192] Loss: 0.2912858499838035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1193] Loss: 0.2912750824965502\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1194] Loss: 0.2912650585224434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1195] Loss: 0.29128310592782747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1196] Loss: 0.2913014327295077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1197] Loss: 0.29129158050621246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1198] Loss: 0.29130772995978194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1199] Loss: 0.2913040225514987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1200] Loss: 0.2913011686715916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1201] Loss: 0.29127985619244195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1202] Loss: 0.2912765518873862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1203] Loss: 0.29127717623555727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1204] Loss: 0.2912643369813285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1205] Loss: 0.2912535164085225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1206] Loss: 0.2912565628355785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1207] Loss: 0.29124821264305634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1208] Loss: 0.2912454094411151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1209] Loss: 0.2912364772497014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1210] Loss: 0.2912347030173038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1211] Loss: 0.29121606813220835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1212] Loss: 0.29123205264846364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1213] Loss: 0.2912546385910042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1214] Loss: 0.2912470158526895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1215] Loss: 0.2912587295646342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1216] Loss: 0.29125936536582997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1217] Loss: 0.29125083716310324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1218] Loss: 0.2912312927098431\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1219] Loss: 0.2912213044201664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1220] Loss: 0.2912324457707482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1221] Loss: 0.29122224548415904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1222] Loss: 0.2912243554291514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1223] Loss: 0.2912277145487959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1224] Loss: 0.29123089283457076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1225] Loss: 0.2912260676172408\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1226] Loss: 0.2912136395458699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1227] Loss: 0.29120914093712985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1228] Loss: 0.29121586807942856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1229] Loss: 0.29121273651496815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1230] Loss: 0.2912063334339251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1231] Loss: 0.2912063498818939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1232] Loss: 0.2912174503963274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1233] Loss: 0.2912271842928569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1234] Loss: 0.2912579231138373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1235] Loss: 0.29124997782587736\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1236] Loss: 0.29123015014468184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1237] Loss: 0.2912594585894926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1238] Loss: 0.2912880195038539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1239] Loss: 0.2912734522704905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1240] Loss: 0.2912743202976159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1241] Loss: 0.29127422087344357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1242] Loss: 0.2912543026465167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1243] Loss: 0.29125729240254317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1244] Loss: 0.2912603610498656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1245] Loss: 0.29125304876354474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1246] Loss: 0.2912826058348717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1247] Loss: 0.2912858072489759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1248] Loss: 0.29127306461772795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1249] Loss: 0.29125785460568143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1250] Loss: 0.29124017320741324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1251] Loss: 0.2912700812183988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1252] Loss: 0.2912739578846966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1253] Loss: 0.29125530009521017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1254] Loss: 0.29124051891519176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1255] Loss: 0.2912320572528201\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1256] Loss: 0.2912334446615232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1257] Loss: 0.29122030281493144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1258] Loss: 0.2911995502713733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1259] Loss: 0.29122320128732104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1260] Loss: 0.2912290376757884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1261] Loss: 0.29120759029591986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1262] Loss: 0.29118973403654036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1263] Loss: 0.2911746359205784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1264] Loss: 0.2911722731421367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1265] Loss: 0.29116405921206756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1266] Loss: 0.29114913498769396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1267] Loss: 0.2911594287043074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1268] Loss: 0.29113612985858756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1269] Loss: 0.2911353354569743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1270] Loss: 0.2911390814955343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1271] Loss: 0.2911633132791314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1272] Loss: 0.2911655989899105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1273] Loss: 0.29118128216869515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1274] Loss: 0.29119683442206323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1275] Loss: 0.29119040850568784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1276] Loss: 0.29119083893162084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1277] Loss: 0.29118321858563356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1278] Loss: 0.2911775331277711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1279] Loss: 0.29117468113783157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1280] Loss: 0.2911592664958888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1281] Loss: 0.29117427871079876\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1282] Loss: 0.2912184001701725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1283] Loss: 0.2912321429840671\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1284] Loss: 0.2912378027639373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1285] Loss: 0.2912457179460112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1286] Loss: 0.2912533226057589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1287] Loss: 0.29123805741152875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1288] Loss: 0.29124155538185903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1289] Loss: 0.2912311047622519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1290] Loss: 0.29125043479840695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1291] Loss: 0.29127118420831993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1292] Loss: 0.2912667118965128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1293] Loss: 0.29128198300681707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1294] Loss: 0.29129404276868526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1295] Loss: 0.291284298705824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1296] Loss: 0.2912940996414499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1297] Loss: 0.29129289349444354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1298] Loss: 0.2912998351043524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1299] Loss: 0.2913040893432411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1300] Loss: 0.29129523890293435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1301] Loss: 0.29128559878942223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1302] Loss: 0.29128592204936377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1303] Loss: 0.2912971327002208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1304] Loss: 0.2913090847010993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1305] Loss: 0.29131969318294726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1306] Loss: 0.2913406951198116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1307] Loss: 0.29135271388683914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1308] Loss: 0.2913458396786637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1309] Loss: 0.2913261182232398\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1310] Loss: 0.2913340715305569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1311] Loss: 0.2913658854169637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1312] Loss: 0.2913906848103148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1313] Loss: 0.2914039272538501\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1314] Loss: 0.29140458288071924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1315] Loss: 0.2913904890347174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1316] Loss: 0.2913905655870993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1317] Loss: 0.2913756726262346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1318] Loss: 0.29135891549792714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1319] Loss: 0.291361342635477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1320] Loss: 0.29134335592097416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1321] Loss: 0.2913531080107503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1322] Loss: 0.29135466558686696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1323] Loss: 0.2913451738077383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1324] Loss: 0.2913379950239781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1325] Loss: 0.2913389513572046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1326] Loss: 0.291343135818366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1327] Loss: 0.2913384194286713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1328] Loss: 0.2913217989496377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1329] Loss: 0.2913175532210305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1330] Loss: 0.2913119415302017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1331] Loss: 0.29133183598133816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1332] Loss: 0.29133434916311296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1333] Loss: 0.29132191779787453\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1334] Loss: 0.2913190662359117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1335] Loss: 0.29132373118695104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1336] Loss: 0.2913047980226813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1337] Loss: 0.29130230865856027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1338] Loss: 0.29127978263986337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1339] Loss: 0.29126698579687016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1340] Loss: 0.2912759398970853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1341] Loss: 0.2912678934555576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1342] Loss: 0.2912903024518438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1343] Loss: 0.29127086370702926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1344] Loss: 0.29126766499627293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1345] Loss: 0.29125094713982896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1346] Loss: 0.2912512713120455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1347] Loss: 0.29126328317022876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1348] Loss: 0.29125264492409303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1349] Loss: 0.29126050288482963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1350] Loss: 0.2912513945833131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1351] Loss: 0.29127409112606606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1352] Loss: 0.2912674423924187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1353] Loss: 0.29127733268621747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1354] Loss: 0.29127959453618907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1355] Loss: 0.29127302972926733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1356] Loss: 0.2912692292854887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1357] Loss: 0.2912682256820812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1358] Loss: 0.29126398740587334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1359] Loss: 0.291262665291146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1360] Loss: 0.2912422630102073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1361] Loss: 0.29126238099240986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1362] Loss: 0.2912750815538442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1363] Loss: 0.291268957941583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1364] Loss: 0.291314072724936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1365] Loss: 0.291301751779014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1366] Loss: 0.29133677826072935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1367] Loss: 0.29133854192311964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1368] Loss: 0.29138810331359155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1369] Loss: 0.29136669344637267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1370] Loss: 0.29141363549907096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1371] Loss: 0.2914275357748527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1372] Loss: 0.2914104434223249\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1373] Loss: 0.2913982001096292\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1374] Loss: 0.2914020237830783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1375] Loss: 0.2913954100550348\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1376] Loss: 0.291414129261214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1377] Loss: 0.291394165669138\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1378] Loss: 0.2913979120849801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1379] Loss: 0.29139093871766863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1380] Loss: 0.29137606574801933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1381] Loss: 0.29136765031720346\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8914000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1382] Loss: 0.29136537426660136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1383] Loss: 0.2913590017844773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1384] Loss: 0.291363450877289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1385] Loss: 0.291348810235389\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1386] Loss: 0.29136533015363153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1387] Loss: 0.29136276030748587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1388] Loss: 0.2913690916867182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1389] Loss: 0.29134942843802153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1390] Loss: 0.2913558456997132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1391] Loss: 0.29133356615798467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1392] Loss: 0.29132378867364794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1393] Loss: 0.29132755990820575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1394] Loss: 0.2913206026734198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1395] Loss: 0.2913156845353242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1396] Loss: 0.291305637637859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1397] Loss: 0.29130503988196915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1398] Loss: 0.29129727269167793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1399] Loss: 0.2912890562715988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1400] Loss: 0.29127541182071237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1401] Loss: 0.29128092652887333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1402] Loss: 0.2912598302774916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1403] Loss: 0.291241315447903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1404] Loss: 0.29126931822748114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1405] Loss: 0.2912667087046559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1406] Loss: 0.2912616331391111\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1407] Loss: 0.29125394804337773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1408] Loss: 0.2912429969350935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1409] Loss: 0.2912497218252252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1410] Loss: 0.2912801011674857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1411] Loss: 0.29126540635264986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1412] Loss: 0.29125520589813403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1413] Loss: 0.2912605053172065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1414] Loss: 0.29125437174979596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1415] Loss: 0.2912339293027392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1416] Loss: 0.2912484429180523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1417] Loss: 0.29122602594530966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1418] Loss: 0.2912186216546499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1419] Loss: 0.2912211486158289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1420] Loss: 0.2912183934760865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1421] Loss: 0.2912007895241749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1422] Loss: 0.2911871845857235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1423] Loss: 0.29120879477407174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1424] Loss: 0.2912051525878918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1425] Loss: 0.29122282079672734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1426] Loss: 0.2912321348614676\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1427] Loss: 0.29122418372615855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1428] Loss: 0.2912273948141773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1429] Loss: 0.2912200777831065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1430] Loss: 0.2912131411417578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1431] Loss: 0.29121068797519084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1432] Loss: 0.2911980718780034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1433] Loss: 0.29119521389717123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1434] Loss: 0.29117578526402127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1435] Loss: 0.2911835413321252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1436] Loss: 0.29118338372030167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1437] Loss: 0.2911857554479895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1438] Loss: 0.29120313318075725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1439] Loss: 0.29117956595646055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1440] Loss: 0.2911661396041578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1441] Loss: 0.2911798772399538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1442] Loss: 0.2912300866503368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1443] Loss: 0.2912335743371105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1444] Loss: 0.29123133619333186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1445] Loss: 0.291228918762856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1446] Loss: 0.29124775063883396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1447] Loss: 0.2912287663529112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1448] Loss: 0.2912577178964353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1449] Loss: 0.29124808176523836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1450] Loss: 0.29124849517978607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1451] Loss: 0.2912415029398301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1452] Loss: 0.2912527068351384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1453] Loss: 0.2912515083726778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1454] Loss: 0.29126833310748595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1455] Loss: 0.2912667232517861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1456] Loss: 0.2912501709744229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1457] Loss: 0.2912430005197097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1458] Loss: 0.29123122422473957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1459] Loss: 0.2912291606352913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1460] Loss: 0.2912418989124113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1461] Loss: 0.2912514332844755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1462] Loss: 0.2912547949142057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1463] Loss: 0.29125548215841557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1464] Loss: 0.291245022925205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1465] Loss: 0.2912284268719289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1466] Loss: 0.29122378181804714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1467] Loss: 0.2912163180578387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1468] Loss: 0.2912059524961465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1469] Loss: 0.29124459090929233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1470] Loss: 0.29123497437091483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1471] Loss: 0.29122390766272505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1472] Loss: 0.29123213051154956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1473] Loss: 0.2912142111917874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1474] Loss: 0.2912305255170829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1475] Loss: 0.2912596615163457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1476] Loss: 0.29126173776280373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1477] Loss: 0.2912443519122649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1478] Loss: 0.2912588302303203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1479] Loss: 0.29125659674093257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1480] Loss: 0.29125750702938863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1481] Loss: 0.2912453862234382\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1482] Loss: 0.29123391981589236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1483] Loss: 0.2912330071217874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1484] Loss: 0.2912228741746082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1485] Loss: 0.2912127602151925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1486] Loss: 0.2912040612024807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1487] Loss: 0.29121432141241854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1488] Loss: 0.2912354010677143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1489] Loss: 0.29124811730067535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1490] Loss: 0.2912409278955699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1491] Loss: 0.2912402490302945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1492] Loss: 0.29122306370643264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1493] Loss: 0.2912158828086045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1494] Loss: 0.2912266000051567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1495] Loss: 0.2912303674171178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1496] Loss: 0.2912203988947952\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1497] Loss: 0.2912090282756057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1498] Loss: 0.29121365055587306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1499] Loss: 0.2912135502606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1500] Loss: 0.29122393654553186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1501] Loss: 0.2912228304981408\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1502] Loss: 0.29120630731675884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1503] Loss: 0.2912344863771197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1504] Loss: 0.2912380654329711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1505] Loss: 0.2912546208015043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1506] Loss: 0.2912403510949988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1507] Loss: 0.2912229749710723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1508] Loss: 0.29122033534819664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1509] Loss: 0.29122547011649846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1510] Loss: 0.29122710682794467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1511] Loss: 0.2912110104074112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1512] Loss: 0.2912367659580555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1513] Loss: 0.2912338318006077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1514] Loss: 0.2912831372606505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1515] Loss: 0.29127166546681377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1516] Loss: 0.29133505010267885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1517] Loss: 0.29134574028369475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1518] Loss: 0.2913644241520099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1519] Loss: 0.291361379251577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1520] Loss: 0.2913512361564019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1521] Loss: 0.29134739149627153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1522] Loss: 0.2913520480806647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1523] Loss: 0.2913571861063764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 15, Batch 1524] Loss: 0.2913463214393591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 0] Loss: 0.2913488361872468\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1] Loss: 0.29135113080082475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 2] Loss: 0.29133627385697664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 3] Loss: 0.2913266259978986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 4] Loss: 0.29134842826662033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 5] Loss: 0.2913489333523916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 6] Loss: 0.29136733451978053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 7] Loss: 0.2913649497367279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 8] Loss: 0.29137232992421613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 9] Loss: 0.2913806681444525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 10] Loss: 0.29137800060770086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 11] Loss: 0.2914020733349595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 12] Loss: 0.2914028891672555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 13] Loss: 0.2914002490211396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 14] Loss: 0.29139805502212585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 15] Loss: 0.29140355039396076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 16] Loss: 0.2913934400036968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 17] Loss: 0.291379070086732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 18] Loss: 0.2913697133570321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 19] Loss: 0.2913704909224084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 20] Loss: 0.29134788301453785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 21] Loss: 0.2913343563571023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 22] Loss: 0.2913354311723173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 23] Loss: 0.291333205919521\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 24] Loss: 0.2913275936561363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 25] Loss: 0.29131758750224385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 26] Loss: 0.29130942176931196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 27] Loss: 0.29130988121901896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 28] Loss: 0.29131679944949895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 29] Loss: 0.2913422091365192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 30] Loss: 0.2913358329845547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 31] Loss: 0.29134515723872867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 32] Loss: 0.29133450547962897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 33] Loss: 0.2913678470831774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 34] Loss: 0.29138848830149616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 35] Loss: 0.29137965202583144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 36] Loss: 0.29139391983096513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 37] Loss: 0.2913799424720853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 38] Loss: 0.2913602948481677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 39] Loss: 0.2913727690372454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 40] Loss: 0.2913943900923951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 41] Loss: 0.2914393752048491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 42] Loss: 0.29145305472483596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 43] Loss: 0.2914540817868654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 44] Loss: 0.2914373277989477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 45] Loss: 0.291432236795343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 46] Loss: 0.29143677862850903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 47] Loss: 0.2914244428579351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 48] Loss: 0.29142143857230446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 49] Loss: 0.29143036546074996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 50] Loss: 0.2914227437157064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 51] Loss: 0.29141398847286226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 52] Loss: 0.2914205720257089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 53] Loss: 0.2914049030455961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 54] Loss: 0.29141463909724613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 55] Loss: 0.29140375536299595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 56] Loss: 0.2914049582175911\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 57] Loss: 0.2913824388973125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 58] Loss: 0.29138887050198004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 59] Loss: 0.2913901677253174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 60] Loss: 0.29140658166739103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 61] Loss: 0.2914097537069075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 62] Loss: 0.29141140095683216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 63] Loss: 0.2914040364186683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 64] Loss: 0.2913900822136486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 65] Loss: 0.29140203654094643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 66] Loss: 0.2913895042006692\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 67] Loss: 0.2913977970194057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 68] Loss: 0.29139425091321713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 69] Loss: 0.29137193845208575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 70] Loss: 0.2913747930394943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 71] Loss: 0.29138868667238665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 72] Loss: 0.29138969481710797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 73] Loss: 0.2914070212960172\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 74] Loss: 0.29138713387047305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 75] Loss: 0.29142240874687464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 76] Loss: 0.2914319390150468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 77] Loss: 0.2914329046394407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 78] Loss: 0.29141187893916826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 79] Loss: 0.2913940391763366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 80] Loss: 0.2913793711404652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 81] Loss: 0.291372825575917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 82] Loss: 0.2913621679280275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 83] Loss: 0.29134913786517486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 84] Loss: 0.29133336380227426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 85] Loss: 0.2913269298358993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 86] Loss: 0.2913066702749079\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 87] Loss: 0.29130785609272664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 88] Loss: 0.2912976404803908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 89] Loss: 0.2913108075980419\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 90] Loss: 0.2913118659413274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 91] Loss: 0.2913065009902487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 92] Loss: 0.29130018000646535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 93] Loss: 0.2912854436175258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 94] Loss: 0.2912961901284997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 95] Loss: 0.29130211542945084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 96] Loss: 0.2912987313903413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 97] Loss: 0.2913052003018057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 98] Loss: 0.29129910266366815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 99] Loss: 0.2913216112386266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 100] Loss: 0.29131059197355375\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 101] Loss: 0.2913061585688285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 102] Loss: 0.291304754209113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 103] Loss: 0.2913127408687599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 104] Loss: 0.2913238932092114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 105] Loss: 0.2913191056086405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 106] Loss: 0.2913043051084246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 107] Loss: 0.29129685541204997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 108] Loss: 0.2912904772012508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 109] Loss: 0.2913019815465886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 110] Loss: 0.2912881992089143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 111] Loss: 0.2913054963819322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 112] Loss: 0.29130690069212195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 113] Loss: 0.29129246013232185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 114] Loss: 0.29129164675955754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 115] Loss: 0.29127293490315936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 116] Loss: 0.29126811032715955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 117] Loss: 0.29126307994965484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 118] Loss: 0.29124508082141154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 119] Loss: 0.29122501763753234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 120] Loss: 0.2912092507902796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 121] Loss: 0.29123066835179395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 122] Loss: 0.2912198641121683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 123] Loss: 0.2912184377162716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 124] Loss: 0.2911976439881761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 125] Loss: 0.2911846723600402\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 126] Loss: 0.29117896398480186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 127] Loss: 0.2911799041730311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 128] Loss: 0.2911800611734062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 129] Loss: 0.2911680723535062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 130] Loss: 0.29116621752031085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 131] Loss: 0.29116769047970464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 132] Loss: 0.2911727337269668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 133] Loss: 0.29117947591396015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 134] Loss: 0.2911795264326565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 135] Loss: 0.29117563401613833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 136] Loss: 0.29117419020741514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 137] Loss: 0.2912077098106373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 138] Loss: 0.2912439109183078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 139] Loss: 0.2912336068873112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 140] Loss: 0.29123889656279056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 141] Loss: 0.29122898453293355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 142] Loss: 0.29122599436821983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 143] Loss: 0.29121953091631475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 144] Loss: 0.29123400548962874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 145] Loss: 0.29125911089660855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 146] Loss: 0.2912717441975198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 147] Loss: 0.2912816057579846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 148] Loss: 0.29127032635397754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 149] Loss: 0.29127697413376397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 150] Loss: 0.2912815210401948\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 151] Loss: 0.2912740667503673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 152] Loss: 0.291266255923368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 153] Loss: 0.2912764008711662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 154] Loss: 0.29126516595588586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 155] Loss: 0.291263548025601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 156] Loss: 0.29125013375736325\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 157] Loss: 0.29123733827247145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 158] Loss: 0.2912641128369567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 159] Loss: 0.29125512912364954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 160] Loss: 0.29127237467412537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 161] Loss: 0.2912720833854402\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 162] Loss: 0.29128731416757103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 163] Loss: 0.29129101064690116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 164] Loss: 0.29129118628688666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 165] Loss: 0.29127874990643093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 166] Loss: 0.29127739749680004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 167] Loss: 0.29125479880930005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 168] Loss: 0.2912662616684596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 169] Loss: 0.2912513990065578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 170] Loss: 0.2912611512918179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 171] Loss: 0.29127571961617343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 172] Loss: 0.2912822347380252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 173] Loss: 0.29127718536828906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 174] Loss: 0.291314371188035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 175] Loss: 0.2912995105197597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 176] Loss: 0.2913087089705392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 177] Loss: 0.2913350969216511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 178] Loss: 0.29131513926023056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 179] Loss: 0.2913254202079429\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 180] Loss: 0.2913248690487392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 181] Loss: 0.29131965832547096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 182] Loss: 0.2913216746723657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 183] Loss: 0.2913275024790547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 184] Loss: 0.2913239766880654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 185] Loss: 0.29130507716978926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 186] Loss: 0.29129551565341466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 187] Loss: 0.2912831090425013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 188] Loss: 0.291280203802156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 189] Loss: 0.2912642828140744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 190] Loss: 0.2913003714325183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 191] Loss: 0.29128103455107246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 192] Loss: 0.2912623779536015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 193] Loss: 0.2912630433399006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 194] Loss: 0.29126084675657976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 195] Loss: 0.2912925759354323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 196] Loss: 0.29127859023314207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 197] Loss: 0.2912742982085557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 198] Loss: 0.29125801980675675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 199] Loss: 0.2912614177709146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 200] Loss: 0.29125829882014875\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 201] Loss: 0.29125163394885234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 202] Loss: 0.29124026996129093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 203] Loss: 0.2912295591209688\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 204] Loss: 0.2912446221772847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 205] Loss: 0.2912213558161801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 206] Loss: 0.291242696853589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 207] Loss: 0.29123159689459094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 208] Loss: 0.29122877111372675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 209] Loss: 0.2912270403838493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 210] Loss: 0.29122032846146445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 211] Loss: 0.29120652173064865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 212] Loss: 0.29119853869453194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 213] Loss: 0.29120613757475533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 214] Loss: 0.29120544991923175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 215] Loss: 0.29119236336137433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 216] Loss: 0.29119589381001915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 217] Loss: 0.2911937434689449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 218] Loss: 0.2911938279879108\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 219] Loss: 0.29118151746933746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 220] Loss: 0.2911754081000858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 221] Loss: 0.29118725793929773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 222] Loss: 0.29118785011355686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 223] Loss: 0.29117527874632015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 224] Loss: 0.29117301496874815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 225] Loss: 0.2911595341215743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 226] Loss: 0.29116232024662564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 227] Loss: 0.2911586288630651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 228] Loss: 0.29113896870320893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 229] Loss: 0.2911236237540622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 230] Loss: 0.2911374292520682\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 231] Loss: 0.29112734896752274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 232] Loss: 0.2911492381429633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 233] Loss: 0.2911313836033581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 234] Loss: 0.2911790111519715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 235] Loss: 0.29116626571593823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 236] Loss: 0.2911842191589278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 237] Loss: 0.2911620103271467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 238] Loss: 0.29114387788535195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 239] Loss: 0.29117407025023023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 240] Loss: 0.29115339186187433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 241] Loss: 0.29115848976506714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 242] Loss: 0.2911412195422893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 243] Loss: 0.29112531033927486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 244] Loss: 0.29111888588057283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 245] Loss: 0.2911011164779829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 246] Loss: 0.2910875869531491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 247] Loss: 0.2911053681869085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 248] Loss: 0.29109877197673356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 249] Loss: 0.2910934836572571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 250] Loss: 0.29108947671359053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 251] Loss: 0.29108147172906734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 252] Loss: 0.29106790640789726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 253] Loss: 0.2910693708686727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 254] Loss: 0.2910491876495983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 255] Loss: 0.29104003565702985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 256] Loss: 0.29106814332988756\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 257] Loss: 0.2910703257296034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 258] Loss: 0.29107253606655326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 259] Loss: 0.2910685387092612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 260] Loss: 0.29105320238983107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 261] Loss: 0.2910892225714021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 262] Loss: 0.2910810863598983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 263] Loss: 0.29106231443433495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 264] Loss: 0.2910637192179469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 265] Loss: 0.29104924407074034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 266] Loss: 0.291039351787964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 267] Loss: 0.2910632176532125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 268] Loss: 0.2910570850649062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 269] Loss: 0.2910330356428147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 270] Loss: 0.29104240480226784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 271] Loss: 0.2910491668005191\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 272] Loss: 0.2910371687391027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 273] Loss: 0.29101699836596445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 274] Loss: 0.29101054985378755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 275] Loss: 0.29099321271433254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 276] Loss: 0.2909905297588567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 277] Loss: 0.2910162511876126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 278] Loss: 0.29102233727217974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 279] Loss: 0.2910196793084594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 280] Loss: 0.2910241522495721\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 281] Loss: 0.2910298449891358\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 282] Loss: 0.291027700615638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 283] Loss: 0.29105114792724524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 284] Loss: 0.291037129120709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 285] Loss: 0.2910468160873032\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 286] Loss: 0.29103621311526057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 287] Loss: 0.291027056573447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 288] Loss: 0.2910201700488602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 289] Loss: 0.29101375007450275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 290] Loss: 0.2909990584626833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 291] Loss: 0.291002935123515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 292] Loss: 0.29099372850747396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 293] Loss: 0.29100503760695295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 294] Loss: 0.2909857403624707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 295] Loss: 0.29097721477915234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 296] Loss: 0.29098214230111197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 297] Loss: 0.29097324639465677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 298] Loss: 0.2909604856922132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 299] Loss: 0.2909394168495747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 300] Loss: 0.2909275236407905\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 301] Loss: 0.2909442358978586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 302] Loss: 0.29094492476597167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 303] Loss: 0.29094637074735674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 304] Loss: 0.29093852152178395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 305] Loss: 0.2909433007078669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 306] Loss: 0.2909258012855454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 307] Loss: 0.2909114871878008\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 308] Loss: 0.2909135106506864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 309] Loss: 0.2909033926168777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 310] Loss: 0.29089265541975806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 311] Loss: 0.2908859519306066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 312] Loss: 0.2908819620658539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 313] Loss: 0.2908675229745244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 314] Loss: 0.2908761891567823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 315] Loss: 0.29088428308222497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 316] Loss: 0.29087869722769016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 317] Loss: 0.290894475218859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 318] Loss: 0.2908808855491644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 319] Loss: 0.2908759278997886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 320] Loss: 0.2908995853059045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 321] Loss: 0.29088534309258995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 322] Loss: 0.2908692882696622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 323] Loss: 0.29086050954909015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 324] Loss: 0.29084863035663944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 325] Loss: 0.29088172190945594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 326] Loss: 0.2908631414226167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 327] Loss: 0.2908460883603815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 328] Loss: 0.2908258639051087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 329] Loss: 0.2908371469917801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 330] Loss: 0.2908314630715545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 331] Loss: 0.29081139582810384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 332] Loss: 0.29079873917862764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 333] Loss: 0.2907954561766799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 334] Loss: 0.2907822687752244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 335] Loss: 0.29078350986318685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 336] Loss: 0.2907680693792619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 337] Loss: 0.290765406127781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 338] Loss: 0.29075970428935977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 339] Loss: 0.2907731532480981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 340] Loss: 0.2907955698731798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 341] Loss: 0.2908057221408832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 342] Loss: 0.2907924609372842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 343] Loss: 0.29077325117991043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 344] Loss: 0.2907654583632368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 345] Loss: 0.2907658627458914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 346] Loss: 0.2907555118558775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 347] Loss: 0.2907468460785171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 348] Loss: 0.2907629773844052\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 349] Loss: 0.29076184211320405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 350] Loss: 0.2907444994545504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 351] Loss: 0.29075588917174544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 352] Loss: 0.29076457426039826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 353] Loss: 0.2907492592063991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 354] Loss: 0.29076477132854506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 355] Loss: 0.2907600310315427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 356] Loss: 0.29079069831547655\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8855\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 357] Loss: 0.2907852164235015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 358] Loss: 0.2907806089845511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 359] Loss: 0.2907709151922697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 360] Loss: 0.29077159282621556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 361] Loss: 0.2908136496164003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 362] Loss: 0.2908200047121817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 363] Loss: 0.2908053800607077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 364] Loss: 0.29079458198149394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 365] Loss: 0.290783899785514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 366] Loss: 0.29077975253872523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 367] Loss: 0.2907706198801073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 368] Loss: 0.2908194484744246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 369] Loss: 0.290820825494558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 370] Loss: 0.29082162726750765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 371] Loss: 0.290832427231663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 372] Loss: 0.2908154351076428\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 373] Loss: 0.2908328766843574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 374] Loss: 0.290841484662098\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 375] Loss: 0.2908912494039053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 376] Loss: 0.29087465097322684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 377] Loss: 0.29085726079950996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 378] Loss: 0.290858374916977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 379] Loss: 0.2908552580034687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 380] Loss: 0.29086633396391165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 381] Loss: 0.2908554202629636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 382] Loss: 0.2908388990234344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 383] Loss: 0.29082339400531904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 384] Loss: 0.2908075516211741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 385] Loss: 0.2908077444943745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 386] Loss: 0.2907999634016518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 387] Loss: 0.2908029870536183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 388] Loss: 0.2907939686324749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 389] Loss: 0.29080197665639546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 390] Loss: 0.2908337772966963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 391] Loss: 0.2908345258575177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 392] Loss: 0.2908722162570914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 393] Loss: 0.2908705635155258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 394] Loss: 0.2908716728950443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 395] Loss: 0.29089134479335954\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 396] Loss: 0.29089886385636277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 397] Loss: 0.2908793449338609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 398] Loss: 0.2908702217562418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 399] Loss: 0.29086323207469106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 400] Loss: 0.29086833923875566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 401] Loss: 0.2908669877196386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 402] Loss: 0.29085127030489233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 403] Loss: 0.29085378938717016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 404] Loss: 0.29084254059375286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 405] Loss: 0.29082955925057385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 406] Loss: 0.2908293688737083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 407] Loss: 0.2908411416076155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 408] Loss: 0.2908361146945417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 409] Loss: 0.2908540981138772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 410] Loss: 0.2908449678673506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 411] Loss: 0.29083329260422564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 412] Loss: 0.2908408085929527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 413] Loss: 0.29082236390387034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 414] Loss: 0.29082269772095326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 415] Loss: 0.2908112661472705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 416] Loss: 0.290801750468171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 417] Loss: 0.2907855873629435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 418] Loss: 0.29079254246016406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 419] Loss: 0.2908056255968288\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 420] Loss: 0.29078769523960035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 421] Loss: 0.2908022291962226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 422] Loss: 0.290802249339736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 423] Loss: 0.29079867487292066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 424] Loss: 0.2908304628342536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 425] Loss: 0.2908612404649254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 426] Loss: 0.29084690858471507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 427] Loss: 0.2908379208235046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 428] Loss: 0.29082529948786745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 429] Loss: 0.2908266333527342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 430] Loss: 0.2908317796442434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 431] Loss: 0.29082981953412956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 432] Loss: 0.29081446717939496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 433] Loss: 0.2908049056955626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 434] Loss: 0.29080253210210805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 435] Loss: 0.29079204954961335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 436] Loss: 0.29078954427558745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 437] Loss: 0.29078276079801985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 438] Loss: 0.29078861423047475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 439] Loss: 0.29076655381374705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 440] Loss: 0.2907879007196109\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 441] Loss: 0.2907784024718663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 442] Loss: 0.2907916130230905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 443] Loss: 0.2907929907637988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 444] Loss: 0.29077744817160966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 445] Loss: 0.29076128938003243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 446] Loss: 0.2907510851424903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 447] Loss: 0.2907541153895777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 448] Loss: 0.2907605753537979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 449] Loss: 0.29075676251427257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 450] Loss: 0.2907424208539703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 451] Loss: 0.2907416050703967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 452] Loss: 0.2907337969475871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 453] Loss: 0.29077594523314426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 454] Loss: 0.2907680680416272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 455] Loss: 0.29077081617472383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 456] Loss: 0.2907617094992207\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 457] Loss: 0.29078460790036903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 458] Loss: 0.290778442213703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 459] Loss: 0.2907680361281486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 460] Loss: 0.29075249918832996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 461] Loss: 0.2907844028661934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 462] Loss: 0.2907650397449307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 463] Loss: 0.2907855487405342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 464] Loss: 0.290771912042901\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 465] Loss: 0.2907598716663303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 466] Loss: 0.29075312701923806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 467] Loss: 0.29074452568506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 468] Loss: 0.29073171000154574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 469] Loss: 0.29073858953507115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 470] Loss: 0.2907433606831437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 471] Loss: 0.2907779224688798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 472] Loss: 0.29075940092507463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 473] Loss: 0.2907446506918313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 474] Loss: 0.2907457060899695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 475] Loss: 0.2907397742024981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 476] Loss: 0.2907324615192598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 477] Loss: 0.2907329458624087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 478] Loss: 0.29073383129190894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 479] Loss: 0.29075143730007763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 480] Loss: 0.2907470827927063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 481] Loss: 0.2907393216152567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 482] Loss: 0.2907407172728522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 483] Loss: 0.29073699587460033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 484] Loss: 0.29073739146411587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 485] Loss: 0.29074214847172347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 486] Loss: 0.29077223702869104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 487] Loss: 0.2907805200097649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 488] Loss: 0.2908188547708189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 489] Loss: 0.2908416226763763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 490] Loss: 0.29083968026879603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 491] Loss: 0.2908317971748063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 492] Loss: 0.2908280066089739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 493] Loss: 0.2908241284648229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 494] Loss: 0.2908303748311121\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 495] Loss: 0.29082752235612497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 496] Loss: 0.2908347446192085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 497] Loss: 0.29082901039626546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 498] Loss: 0.2908254221377655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 499] Loss: 0.2908215659544218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 500] Loss: 0.2908150348041792\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 501] Loss: 0.29080621169529747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 502] Loss: 0.290799978928185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 503] Loss: 0.29080444863985044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 504] Loss: 0.2908058256680718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 505] Loss: 0.29080570768082387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 506] Loss: 0.29079687967047263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 507] Loss: 0.2907834696022649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 508] Loss: 0.2907683674763592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 509] Loss: 0.2907683018105684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 510] Loss: 0.29076154601698956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 511] Loss: 0.2907693291282823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 512] Loss: 0.2908058332616286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 513] Loss: 0.2908157504285763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 514] Loss: 0.2908065751706367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 515] Loss: 0.29082011672751074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 516] Loss: 0.2908135415601929\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 517] Loss: 0.29084355374125376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 518] Loss: 0.2908297403347667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 519] Loss: 0.2908292872717894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 520] Loss: 0.2908167908741937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 521] Loss: 0.2908080342044014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 522] Loss: 0.290791267888865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 523] Loss: 0.2907876343889039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 524] Loss: 0.2907761933919852\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 525] Loss: 0.29076772814090707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 526] Loss: 0.29075254140017864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 527] Loss: 0.2907474887883945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 528] Loss: 0.29073899475266257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 529] Loss: 0.29074259932467345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 530] Loss: 0.2907357084831988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 531] Loss: 0.2907228177850708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 532] Loss: 0.2907272387625875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 533] Loss: 0.29073186703626774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 534] Loss: 0.29074608951904013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 535] Loss: 0.2907530895184171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 536] Loss: 0.2907567761073445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 537] Loss: 0.2907650190305701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 538] Loss: 0.2907602415008171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 539] Loss: 0.2907668178345591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 540] Loss: 0.29075983455370985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 541] Loss: 0.2907440556513637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 542] Loss: 0.2907606334688648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 543] Loss: 0.2907634054850495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 544] Loss: 0.2907701264365174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 545] Loss: 0.2907829677176703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 546] Loss: 0.2907813202488651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 547] Loss: 0.2907766029796034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 548] Loss: 0.29077919347952125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 549] Loss: 0.29078977330189487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 550] Loss: 0.29081788482687226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 551] Loss: 0.2908394322872758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 552] Loss: 0.29084441518224524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 553] Loss: 0.29082731219731883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 554] Loss: 0.29084845715162927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 555] Loss: 0.2908443214686722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 556] Loss: 0.29083379695406836\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 557] Loss: 0.2908566020804658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 558] Loss: 0.29087218001491016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 559] Loss: 0.2908673317790468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 560] Loss: 0.2908574091807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 561] Loss: 0.2908469772497152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 562] Loss: 0.2908603410779865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 563] Loss: 0.2908578259574167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 564] Loss: 0.29086069372681256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 565] Loss: 0.29085803336044724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 566] Loss: 0.2908359009198267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 567] Loss: 0.29082407161809776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 568] Loss: 0.29080628647995294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 569] Loss: 0.29080091635007255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 570] Loss: 0.2907949814466521\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 571] Loss: 0.2907798411063896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 572] Loss: 0.29077272842528645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 573] Loss: 0.29075860564124867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 574] Loss: 0.2907450477651963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 575] Loss: 0.2907596587065469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 576] Loss: 0.2907554217375774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 577] Loss: 0.2907625267987565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 578] Loss: 0.29075781624964586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 579] Loss: 0.290752873776679\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 580] Loss: 0.2907405617599469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 581] Loss: 0.29074647935513737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 582] Loss: 0.29073068938810054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 583] Loss: 0.290731051019592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 584] Loss: 0.29072872750052925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 585] Loss: 0.2907108455165103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 586] Loss: 0.2906938517082786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 587] Loss: 0.2907028195765551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 588] Loss: 0.29073291468619383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 589] Loss: 0.29074560281578915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 590] Loss: 0.29074086667553667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 591] Loss: 0.29073208901417447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 592] Loss: 0.29072222589744035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 593] Loss: 0.2907141606962474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 594] Loss: 0.29070838495923734\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 595] Loss: 0.29069569434175074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 596] Loss: 0.2906864832598361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 597] Loss: 0.2906783795708095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 598] Loss: 0.2906698737410865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 599] Loss: 0.29065879001297446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 600] Loss: 0.2906534187533936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 601] Loss: 0.29065158156719745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 602] Loss: 0.29065085793755063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 603] Loss: 0.2906305321454421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 604] Loss: 0.29062468073983194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 605] Loss: 0.2906282239361451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 606] Loss: 0.29064472490539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 607] Loss: 0.29067478259304164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 608] Loss: 0.29067102731944144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 609] Loss: 0.2906627159631378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 610] Loss: 0.2906531164446499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 611] Loss: 0.29064727740651725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 612] Loss: 0.29063078792752567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 613] Loss: 0.290645523227371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 614] Loss: 0.2906476369096902\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 615] Loss: 0.29064507685548274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 616] Loss: 0.2906552118847188\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 617] Loss: 0.29063995962566014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 618] Loss: 0.2906314785655504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 619] Loss: 0.2906255555167086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 620] Loss: 0.290622092821219\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 621] Loss: 0.2906023543717131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 622] Loss: 0.2905950489246767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 623] Loss: 0.29057710313785096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 624] Loss: 0.2905848684087388\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 625] Loss: 0.29057245242505686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 626] Loss: 0.29056551594871627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 627] Loss: 0.29060467975378046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 628] Loss: 0.2905931664486112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 629] Loss: 0.2905752862972675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 630] Loss: 0.29057025686744836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 631] Loss: 0.29055911220142194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 632] Loss: 0.2905630546278007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 633] Loss: 0.29055231071911075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 634] Loss: 0.2905365051074053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 635] Loss: 0.29054171329969825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 636] Loss: 0.29052323510824823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 637] Loss: 0.2905072895735746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 638] Loss: 0.2905412700301779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 639] Loss: 0.29053017748245635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 640] Loss: 0.2905153632091666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 641] Loss: 0.29050819809102596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 642] Loss: 0.29051100040282674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 643] Loss: 0.29050379204950105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 644] Loss: 0.2905108588449417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 645] Loss: 0.2904992047720137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 646] Loss: 0.29048525106712386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 647] Loss: 0.290481569035834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 648] Loss: 0.29046887192252424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 649] Loss: 0.29045620127524957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 650] Loss: 0.2904536924647731\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 651] Loss: 0.29044388122696896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 652] Loss: 0.29043091143859495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 653] Loss: 0.29045703924149063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 654] Loss: 0.29044511478007634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 655] Loss: 0.290454560209978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 656] Loss: 0.2904559242027978\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 657] Loss: 0.2904447718692377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 658] Loss: 0.29042822731092405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 659] Loss: 0.2904172635727278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 660] Loss: 0.2904094081613283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 661] Loss: 0.2904178718028599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 662] Loss: 0.2904000358248269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 663] Loss: 0.2903993775719415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 664] Loss: 0.2903848437301663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 665] Loss: 0.29036794632073265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 666] Loss: 0.2903670274198128\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 667] Loss: 0.2903748502783231\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 668] Loss: 0.29035964532760344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 669] Loss: 0.29037850733109266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 670] Loss: 0.2903992565919019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 671] Loss: 0.2904141885661099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 672] Loss: 0.29043647362640473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 673] Loss: 0.29044234206155173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 674] Loss: 0.2904550829510185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 675] Loss: 0.29044346927025955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 676] Loss: 0.29044017243230247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 677] Loss: 0.29043338499429255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 678] Loss: 0.2904299748074489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 679] Loss: 0.2904242558897009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 680] Loss: 0.29042485664705286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 681] Loss: 0.29043969382969265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 682] Loss: 0.29043952744085294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 683] Loss: 0.2904344651356073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 684] Loss: 0.2904385101117981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 685] Loss: 0.29045306418587513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 686] Loss: 0.29045202618353233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 687] Loss: 0.2904545650529494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 688] Loss: 0.2904497192145685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 689] Loss: 0.29046547153192986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 690] Loss: 0.2904554024031911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 691] Loss: 0.2904737601700554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 692] Loss: 0.29046505767671427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 693] Loss: 0.2904604588296173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 694] Loss: 0.2904592995113571\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 695] Loss: 0.2904453542965608\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 696] Loss: 0.2904235935433919\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 697] Loss: 0.29043671187430503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 698] Loss: 0.2904219372211421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 699] Loss: 0.2904123644507613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 700] Loss: 0.2904546921823974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 701] Loss: 0.2904655884780336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 702] Loss: 0.2904755805916135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 703] Loss: 0.2904954455126377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 704] Loss: 0.2904988167042847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 705] Loss: 0.2904872837704299\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 706] Loss: 0.29047514409403147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 707] Loss: 0.2904762011131032\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 708] Loss: 0.2904850898729651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 709] Loss: 0.29049250181641123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 710] Loss: 0.29047890624521894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 711] Loss: 0.2904761242585759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 712] Loss: 0.2904779270939106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 713] Loss: 0.2904855518029891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 714] Loss: 0.2904662939461307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 715] Loss: 0.29044828708066356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 716] Loss: 0.2904282231029282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 717] Loss: 0.2904256875240502\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 718] Loss: 0.2904063031650739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 719] Loss: 0.2904068394058021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 720] Loss: 0.29040938737130506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 721] Loss: 0.2903927416005828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 722] Loss: 0.29038711629367725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 723] Loss: 0.2903967751643081\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 724] Loss: 0.29038601366514094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 725] Loss: 0.29038250928116244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 726] Loss: 0.29037368962287047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 727] Loss: 0.290357358037076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 728] Loss: 0.2903690067629563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 729] Loss: 0.29038446265936246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 730] Loss: 0.2903716089442561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 731] Loss: 0.29039809266395744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 732] Loss: 0.2904017305959058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 733] Loss: 0.29039493878866995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 734] Loss: 0.29042648990244585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 735] Loss: 0.2904166102790187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 736] Loss: 0.29040664880103023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 737] Loss: 0.29041682650176615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 738] Loss: 0.29041888678623007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 739] Loss: 0.2904093523367709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 740] Loss: 0.2904286222746261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 741] Loss: 0.2904158739213691\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 742] Loss: 0.29041262954146446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 743] Loss: 0.29042541673655614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 744] Loss: 0.290435036112236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 745] Loss: 0.2904480201142291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 746] Loss: 0.2904518975155676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 747] Loss: 0.29044559228339817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 748] Loss: 0.2904309992360554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 749] Loss: 0.2904766464983577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 750] Loss: 0.2904612646481947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 751] Loss: 0.29046268262633135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 752] Loss: 0.2904559501942793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 753] Loss: 0.2904437089658435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 754] Loss: 0.2904529059321781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 755] Loss: 0.29046157635409725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 756] Loss: 0.2904747315789609\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 757] Loss: 0.29047930323887694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 758] Loss: 0.29050340540582975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 759] Loss: 0.2904907988526547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 760] Loss: 0.2904843308467837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 761] Loss: 0.29048116561292386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 762] Loss: 0.2904813455533756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 763] Loss: 0.2904719663851586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 764] Loss: 0.2904663176620442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 765] Loss: 0.29049099365326336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 766] Loss: 0.29047972444137055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 767] Loss: 0.29047709941612954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 768] Loss: 0.2904599433435804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 769] Loss: 0.2904612686829249\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 770] Loss: 0.29045141985432343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 771] Loss: 0.290449169014339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 772] Loss: 0.2904557313015381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 773] Loss: 0.2904605396106211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 774] Loss: 0.2904512180714571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 775] Loss: 0.29044544349702434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 776] Loss: 0.2904582292794514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 777] Loss: 0.29045366898375796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 778] Loss: 0.29046645031508556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 779] Loss: 0.2905109083146331\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 780] Loss: 0.29052094847376764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 781] Loss: 0.2905400748470276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 782] Loss: 0.2905318515175051\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 783] Loss: 0.29053269976924123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 784] Loss: 0.29053536442907896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 785] Loss: 0.29053303284264825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 786] Loss: 0.2905177466283241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 787] Loss: 0.2905101576944542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 788] Loss: 0.29051345034752696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 789] Loss: 0.2905172329150007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 790] Loss: 0.2905273324233926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 791] Loss: 0.2905497002432606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 792] Loss: 0.29054144959114403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 793] Loss: 0.29053345533014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 794] Loss: 0.29055175841467845\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 795] Loss: 0.2905383617139348\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 796] Loss: 0.29053187168577754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 797] Loss: 0.2905360191788437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 798] Loss: 0.2905211560173371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 799] Loss: 0.2905099069207385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 800] Loss: 0.2904980002683782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 801] Loss: 0.2905087574762842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 802] Loss: 0.290492046514315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 803] Loss: 0.2904828388950176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 804] Loss: 0.29051602757281725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 805] Loss: 0.2905150731697903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 806] Loss: 0.29053080793567104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 807] Loss: 0.2905342502589113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 808] Loss: 0.29052657497142664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 809] Loss: 0.29050924231256914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 810] Loss: 0.2904971862265149\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 811] Loss: 0.2904872281870488\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 812] Loss: 0.29049532909118003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 813] Loss: 0.2905338144965723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 814] Loss: 0.29051813727613374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 815] Loss: 0.29054132334805693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 816] Loss: 0.29054898403464535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 817] Loss: 0.2905401602821958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 818] Loss: 0.29053304431425103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 819] Loss: 0.2905341556194059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 820] Loss: 0.29053298621225826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 821] Loss: 0.29053445270323514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 822] Loss: 0.2905330134829852\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 823] Loss: 0.29052689670357873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 824] Loss: 0.29051710745531417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 825] Loss: 0.2905127508653591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 826] Loss: 0.2905027882238321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 827] Loss: 0.29050057473477775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 828] Loss: 0.29048366118384966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 829] Loss: 0.29049503834066936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 830] Loss: 0.29048232824917425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 831] Loss: 0.2904849136319182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 832] Loss: 0.29047868904400304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 833] Loss: 0.2904811335470334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 834] Loss: 0.2904719765663822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 835] Loss: 0.2904876132113094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 836] Loss: 0.29046888224830575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 837] Loss: 0.2904903720504528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 838] Loss: 0.29049876729142304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 839] Loss: 0.2904989193563009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 840] Loss: 0.29049024679770336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 841] Loss: 0.290481070516903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 842] Loss: 0.2904890826496232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 843] Loss: 0.2904976470034881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 844] Loss: 0.2905559533794329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 845] Loss: 0.290559095421338\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 846] Loss: 0.29055165290234447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 847] Loss: 0.2905626223949019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 848] Loss: 0.2905730713350087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 849] Loss: 0.29057796453888984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 850] Loss: 0.29056617334902496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 851] Loss: 0.2905776228881678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 852] Loss: 0.2905781931893443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 853] Loss: 0.2905746594404496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 854] Loss: 0.29059779319097395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 855] Loss: 0.2905848173972219\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 856] Loss: 0.29058280186357943\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8890000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 857] Loss: 0.29057814148335703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 858] Loss: 0.2905620459575546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 859] Loss: 0.29056384275871805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 860] Loss: 0.29056390666852944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 861] Loss: 0.29057350339505367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 862] Loss: 0.2905603882025485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 863] Loss: 0.2905482224919807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 864] Loss: 0.290541696551365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 865] Loss: 0.29055136832018774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 866] Loss: 0.29054284490620375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 867] Loss: 0.29054048283660394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 868] Loss: 0.2905635280862211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 869] Loss: 0.2905521126445781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 870] Loss: 0.2905671415998379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 871] Loss: 0.29056336872462635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 872] Loss: 0.2905673125658913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 873] Loss: 0.29055790613981974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 874] Loss: 0.2905562676590489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 875] Loss: 0.2905489705662855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 876] Loss: 0.290529052297648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 877] Loss: 0.2905451135597997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 878] Loss: 0.290570398250166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 879] Loss: 0.2905622304644635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 880] Loss: 0.2905533195950295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 881] Loss: 0.2905507606390842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 882] Loss: 0.2905401375341257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 883] Loss: 0.29052392234464813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 884] Loss: 0.29050791125957426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 885] Loss: 0.29050681438568166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 886] Loss: 0.29050495712506774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 887] Loss: 0.290499365423246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 888] Loss: 0.2904976204584968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 889] Loss: 0.2905073979142173\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 890] Loss: 0.2904992013230482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 891] Loss: 0.2904916550468946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 892] Loss: 0.2905244024323285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 893] Loss: 0.2905320563108845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 894] Loss: 0.2905213504919096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 895] Loss: 0.2905171291565011\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 896] Loss: 0.29050007669932043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 897] Loss: 0.2905068361548919\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 898] Loss: 0.2905170647407966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 899] Loss: 0.29051453994319276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 900] Loss: 0.29054146513135176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 901] Loss: 0.29054302290688644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 902] Loss: 0.2905459485827951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 903] Loss: 0.2905431112217432\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 904] Loss: 0.29053892269775056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 905] Loss: 0.2905219249267928\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 906] Loss: 0.2905127820638997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 907] Loss: 0.2905184263445013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 908] Loss: 0.290514310049448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 909] Loss: 0.2905295339802227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 910] Loss: 0.290518361153308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 911] Loss: 0.2905104824574479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 912] Loss: 0.2905144933212956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 913] Loss: 0.2905128727635366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 914] Loss: 0.2905359848054595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 915] Loss: 0.29053394560392937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 916] Loss: 0.29051989172053505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 917] Loss: 0.2905091366439442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 918] Loss: 0.2905158254837233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 919] Loss: 0.29050064289280536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 920] Loss: 0.2905043146808498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 921] Loss: 0.29051227488745796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 922] Loss: 0.29051891482128306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 923] Loss: 0.29054823394474066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 924] Loss: 0.2905343923407327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 925] Loss: 0.29055086855463086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 926] Loss: 0.29053879038339847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 927] Loss: 0.29054407902598983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 928] Loss: 0.2905346071818202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 929] Loss: 0.2905423876832958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 930] Loss: 0.2905341485989449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 931] Loss: 0.2905236870806876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 932] Loss: 0.2905490539311458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 933] Loss: 0.2905418359598551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 934] Loss: 0.29057283379837556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 935] Loss: 0.2905724928444547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 936] Loss: 0.2905619927245838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 937] Loss: 0.2905560487985132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 938] Loss: 0.290550887070025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 939] Loss: 0.2905333401924919\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 940] Loss: 0.2905585586806749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 941] Loss: 0.29058704402348706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 942] Loss: 0.29058873403643465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 943] Loss: 0.29058470272013737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 944] Loss: 0.29057109872755615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 945] Loss: 0.29055800175384733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 946] Loss: 0.2905706153786988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 947] Loss: 0.2905578068134552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 948] Loss: 0.2905457125976497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 949] Loss: 0.2905646872368837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 950] Loss: 0.29056470208331286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 951] Loss: 0.29055333780914416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 952] Loss: 0.29055396952506124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 953] Loss: 0.290547011820771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 954] Loss: 0.2905353758318499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 955] Loss: 0.29053070771554024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 956] Loss: 0.29051351016568977\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 957] Loss: 0.2905182472633545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 958] Loss: 0.2905236458000064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 959] Loss: 0.29050982517971174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 960] Loss: 0.2905163621718874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 961] Loss: 0.29053709037563613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 962] Loss: 0.29055705991597314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 963] Loss: 0.29057114958861896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 964] Loss: 0.2905812698933895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 965] Loss: 0.29056078076591063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 966] Loss: 0.29055600758557587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 967] Loss: 0.2905534603924025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 968] Loss: 0.29058645656325927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 969] Loss: 0.29057658667145203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 970] Loss: 0.2905667260351802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 971] Loss: 0.2905536097153644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 972] Loss: 0.2905660081874615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 973] Loss: 0.2905563017632131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 974] Loss: 0.2905575673777232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 975] Loss: 0.2905598072809988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 976] Loss: 0.2905469390784236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 977] Loss: 0.2905578450339364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 978] Loss: 0.29054708360917636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 979] Loss: 0.290581501922848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 980] Loss: 0.2905680381865413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 981] Loss: 0.29057770523031295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 982] Loss: 0.29057494628083036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 983] Loss: 0.29056964337930236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 984] Loss: 0.2905605585791626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 985] Loss: 0.2905561343273047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 986] Loss: 0.2905739717304636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 987] Loss: 0.290582270710775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 988] Loss: 0.290575698441227\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 989] Loss: 0.29059465923656286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 990] Loss: 0.29061079098598147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 991] Loss: 0.29060466139854446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 992] Loss: 0.29059366969421285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 993] Loss: 0.29057331368199263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 994] Loss: 0.2905580655129066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 995] Loss: 0.2905752606568693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 996] Loss: 0.2905676671367268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 997] Loss: 0.2905647436763303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 998] Loss: 0.2905661678032581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 999] Loss: 0.2905532107082539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1000] Loss: 0.2905511814742884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1001] Loss: 0.2905494268387321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1002] Loss: 0.29053760326593286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1003] Loss: 0.2905629102179577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1004] Loss: 0.2905575361934944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1005] Loss: 0.29056581515506863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1006] Loss: 0.29055629992357923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1007] Loss: 0.2905643174950976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1008] Loss: 0.2905548906323799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1009] Loss: 0.2905376369402514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1010] Loss: 0.2905538016571633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1011] Loss: 0.29054665838358334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1012] Loss: 0.2905618617665237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1013] Loss: 0.2905480775301482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1014] Loss: 0.29055349169975014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1015] Loss: 0.29055582129572494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1016] Loss: 0.29055122694686375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1017] Loss: 0.290555542959584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1018] Loss: 0.2905439383164378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1019] Loss: 0.29056652795834215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1020] Loss: 0.2905749038275949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1021] Loss: 0.29056691420532965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1022] Loss: 0.2905583406931457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1023] Loss: 0.29055821875778537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1024] Loss: 0.2905477253891061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1025] Loss: 0.29055850098611036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1026] Loss: 0.2905582706562544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1027] Loss: 0.2905545044691103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1028] Loss: 0.2905401927471647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1029] Loss: 0.29054248452894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1030] Loss: 0.29053345762357957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1031] Loss: 0.2905243816591489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1032] Loss: 0.2905310690326114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1033] Loss: 0.2905536619307666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1034] Loss: 0.29056738480348776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1035] Loss: 0.2906099997252255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1036] Loss: 0.2906020416314394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1037] Loss: 0.29060987883308353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1038] Loss: 0.2906177358221302\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1039] Loss: 0.2905972887453157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1040] Loss: 0.2905863306231369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1041] Loss: 0.29057642247073606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1042] Loss: 0.2905817903882536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1043] Loss: 0.29057103804081186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1044] Loss: 0.2905636646258625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1045] Loss: 0.2905686322526533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1046] Loss: 0.2905588460238633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1047] Loss: 0.29057901377754586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1048] Loss: 0.2905784190299729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1049] Loss: 0.29057959190425287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1050] Loss: 0.2905726877492891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1051] Loss: 0.29061005324550576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1052] Loss: 0.2905974457619663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1053] Loss: 0.2905836241481602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1054] Loss: 0.290590276846872\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1055] Loss: 0.29060085789939644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1056] Loss: 0.2906044814050922\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1057] Loss: 0.29058878299581364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1058] Loss: 0.29060717055682317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1059] Loss: 0.2905911111368888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1060] Loss: 0.29057039390886197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1061] Loss: 0.2905523712448769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1062] Loss: 0.2905490006253297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1063] Loss: 0.29055134719712106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1064] Loss: 0.29054119884537216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1065] Loss: 0.29054417136899696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1066] Loss: 0.29053246536308647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1067] Loss: 0.29052666748395645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1068] Loss: 0.2905290160683061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1069] Loss: 0.29051989521659854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1070] Loss: 0.2905190797676745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1071] Loss: 0.2905084835726421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1072] Loss: 0.2905031172785639\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1073] Loss: 0.29050200417117206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1074] Loss: 0.29050864482868827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1075] Loss: 0.2904976117885772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1076] Loss: 0.290490975420313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1077] Loss: 0.290508293582269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1078] Loss: 0.29048717117962286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1079] Loss: 0.29049263452471225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1080] Loss: 0.29047692135194986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1081] Loss: 0.29046780429780483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1082] Loss: 0.2904776916424381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1083] Loss: 0.29047393347298756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1084] Loss: 0.2904851833155007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1085] Loss: 0.2905039436084638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1086] Loss: 0.29049674194666975\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1087] Loss: 0.29048489259579235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1088] Loss: 0.29048599394035224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1089] Loss: 0.29048184091500096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1090] Loss: 0.2904922022487428\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1091] Loss: 0.29049443564734023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1092] Loss: 0.2904926313046769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1093] Loss: 0.290481637417033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1094] Loss: 0.29046675490346585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1095] Loss: 0.29045794056110025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1096] Loss: 0.29043735589473035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1097] Loss: 0.2904454708281956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1098] Loss: 0.29044724519731047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1099] Loss: 0.2904541712746676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1100] Loss: 0.29043538978931493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1101] Loss: 0.29042100823002065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1102] Loss: 0.29043489218891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1103] Loss: 0.29041992872917066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1104] Loss: 0.2904008700066089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1105] Loss: 0.29040711236545225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1106] Loss: 0.29039190704466866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1107] Loss: 0.29037554980660096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1108] Loss: 0.2903673594257436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1109] Loss: 0.2903653084283565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1110] Loss: 0.2903705709983028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1111] Loss: 0.29037027856133085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1112] Loss: 0.29036832127843243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1113] Loss: 0.29041319657977815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1114] Loss: 0.2904055558736212\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1115] Loss: 0.2903946233404978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1116] Loss: 0.2903818102482147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1117] Loss: 0.29039298449654755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1118] Loss: 0.2903935996746414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1119] Loss: 0.290382976025642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1120] Loss: 0.2903733105639597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1121] Loss: 0.29036295221029507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1122] Loss: 0.2903701369068755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1123] Loss: 0.2903669701633742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1124] Loss: 0.29036432201776835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1125] Loss: 0.2903688825278778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1126] Loss: 0.2903643731749977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1127] Loss: 0.29036661968863864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1128] Loss: 0.29035557116009497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1129] Loss: 0.2903401431258995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1130] Loss: 0.290331969131736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1131] Loss: 0.29032755078716693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1132] Loss: 0.2903180930966839\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1133] Loss: 0.2903054845636255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1134] Loss: 0.290314504030669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1135] Loss: 0.29029953344199616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1136] Loss: 0.2902969792198983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1137] Loss: 0.29028935603611933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1138] Loss: 0.2903236861191353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1139] Loss: 0.29032680822716744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1140] Loss: 0.2903105064158047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1141] Loss: 0.29030680912524176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1142] Loss: 0.2903143588960159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1143] Loss: 0.2903131790525058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1144] Loss: 0.2903102063343533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1145] Loss: 0.2903012118151138\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1146] Loss: 0.2903210032728037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1147] Loss: 0.2903212554969279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1148] Loss: 0.2903682906877325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1149] Loss: 0.2903643451657849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1150] Loss: 0.29036682676949105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1151] Loss: 0.2903651347497468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1152] Loss: 0.29035583609617244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1153] Loss: 0.2903424422510777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1154] Loss: 0.29035733366702143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1155] Loss: 0.29035612879266726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1156] Loss: 0.2903475182606675\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1157] Loss: 0.2903567397158063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1158] Loss: 0.2903616982822677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1159] Loss: 0.2903560460252277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1160] Loss: 0.29035416658984003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1161] Loss: 0.2903592297946501\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1162] Loss: 0.2903835616833783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1163] Loss: 0.29036721592059966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1164] Loss: 0.29035596775977096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1165] Loss: 0.29036486149954344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1166] Loss: 0.29037163598565363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1167] Loss: 0.2903643976402078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1168] Loss: 0.2903811196026921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1169] Loss: 0.2903637876905702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1170] Loss: 0.29040250185221594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1171] Loss: 0.2904018426159438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1172] Loss: 0.29040303400435036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1173] Loss: 0.29039267460138823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1174] Loss: 0.29039615720664996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1175] Loss: 0.29040820185535465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1176] Loss: 0.29039920336950276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1177] Loss: 0.2903899434590133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1178] Loss: 0.29039984757762183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1179] Loss: 0.29040078181883117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1180] Loss: 0.29042188454374274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1181] Loss: 0.29042732847395236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1182] Loss: 0.290442345398586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1183] Loss: 0.2904382716712133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1184] Loss: 0.29042180438921533\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1185] Loss: 0.2904106368684901\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1186] Loss: 0.2904497992376075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1187] Loss: 0.29043223226548937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1188] Loss: 0.29044653081989386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1189] Loss: 0.2904295549903354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1190] Loss: 0.29043553237378394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1191] Loss: 0.29043912661770765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1192] Loss: 0.29043387298196444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1193] Loss: 0.29044230036368324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1194] Loss: 0.2904260716071263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1195] Loss: 0.2904105942728758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1196] Loss: 0.290397428836756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1197] Loss: 0.2904281620140827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1198] Loss: 0.2904096524623997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1199] Loss: 0.29039655511403617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1200] Loss: 0.29038192926937106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1201] Loss: 0.2903687496886173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1202] Loss: 0.2903528461461224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1203] Loss: 0.2903563716536324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1204] Loss: 0.2903669197517284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1205] Loss: 0.2903809380671684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1206] Loss: 0.29038051755400845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1207] Loss: 0.29038672571191404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1208] Loss: 0.29038853762390865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1209] Loss: 0.2903936699947376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1210] Loss: 0.2903903150025311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1211] Loss: 0.29038270180825504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1212] Loss: 0.29039232313102065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1213] Loss: 0.2903821257031702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1214] Loss: 0.29038507429100174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1215] Loss: 0.2903722205026701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1216] Loss: 0.2903560965562227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1217] Loss: 0.29037074449000627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1218] Loss: 0.2903584816341124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1219] Loss: 0.29035422939732414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1220] Loss: 0.2903416487322271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1221] Loss: 0.2903284204769078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1222] Loss: 0.29032744824551415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1223] Loss: 0.29032945488457407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1224] Loss: 0.29035018636773285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1225] Loss: 0.29035054131257443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1226] Loss: 0.29034942326284735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1227] Loss: 0.2903498733923345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1228] Loss: 0.2903591367026155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1229] Loss: 0.29035104743932366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1230] Loss: 0.29038913774167197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1231] Loss: 0.2903904602783908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1232] Loss: 0.29038536504063234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1233] Loss: 0.29038526066649817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1234] Loss: 0.29041390488630764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1235] Loss: 0.290415209815667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1236] Loss: 0.29040496745017175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1237] Loss: 0.2904269779214043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1238] Loss: 0.29042525878899944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1239] Loss: 0.29042011449430627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1240] Loss: 0.2904291173096138\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1241] Loss: 0.29045466129142045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1242] Loss: 0.2904483563165107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1243] Loss: 0.29042924726242036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1244] Loss: 0.29043860644688735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1245] Loss: 0.29043959854448553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1246] Loss: 0.2904384809968051\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1247] Loss: 0.29043080027625995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1248] Loss: 0.290435105515971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1249] Loss: 0.29042704868004404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1250] Loss: 0.2904073408739879\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1251] Loss: 0.29039360861866953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1252] Loss: 0.2904105298006129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1253] Loss: 0.29040268905013716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1254] Loss: 0.2903982244327368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1255] Loss: 0.29038631109341695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1256] Loss: 0.29038520450281613\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1257] Loss: 0.2904174466429565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1258] Loss: 0.29043611717064555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1259] Loss: 0.2904470836892667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1260] Loss: 0.2904576080968064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1261] Loss: 0.29045254582592545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1262] Loss: 0.2904928953233726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1263] Loss: 0.2904994166026652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1264] Loss: 0.2904927976012877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1265] Loss: 0.2904903037915513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1266] Loss: 0.2904740320036447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1267] Loss: 0.2904755954429954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1268] Loss: 0.29047451655022766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1269] Loss: 0.29045913268123214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1270] Loss: 0.2904803834075803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1271] Loss: 0.2904898066630876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1272] Loss: 0.29050925484999446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1273] Loss: 0.2905324795582852\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1274] Loss: 0.29056432550841493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1275] Loss: 0.29058902805023995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1276] Loss: 0.2905912691649171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1277] Loss: 0.2905766352138251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1278] Loss: 0.2905957905015749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1279] Loss: 0.2905936425538192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1280] Loss: 0.29058010965621445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1281] Loss: 0.29057160379719177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1282] Loss: 0.29055945350976725\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1283] Loss: 0.2905483280426791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1284] Loss: 0.290544949826655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1285] Loss: 0.29054530118900146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1286] Loss: 0.29059012028607517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1287] Loss: 0.2905795710940525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1288] Loss: 0.29057364998572593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1289] Loss: 0.29057413425552897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1290] Loss: 0.29058787405997816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1291] Loss: 0.2906002923442874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1292] Loss: 0.29060124225298417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1293] Loss: 0.29060718881954556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1294] Loss: 0.2906052417530703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1295] Loss: 0.2906097812067817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1296] Loss: 0.2906032209819349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1297] Loss: 0.29062754324455586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1298] Loss: 0.29062883551144675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1299] Loss: 0.29065580079342695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1300] Loss: 0.2906401740225694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1301] Loss: 0.29063914939739766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1302] Loss: 0.29062204584379486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1303] Loss: 0.2906063389283381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1304] Loss: 0.29060386586996717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1305] Loss: 0.29060665471238667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1306] Loss: 0.29062491070303426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1307] Loss: 0.29062581177048136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1308] Loss: 0.2906116397154539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1309] Loss: 0.2906476122289554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1310] Loss: 0.29063508419194384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1311] Loss: 0.2906187616032851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1312] Loss: 0.2906070637785734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1313] Loss: 0.2906149796607943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1314] Loss: 0.29063574293346417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1315] Loss: 0.290628003266713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1316] Loss: 0.29062207679935254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1317] Loss: 0.2906115891002767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1318] Loss: 0.2906018849627831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1319] Loss: 0.2905981166374411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1320] Loss: 0.2905915559552044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1321] Loss: 0.29058614072969907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1322] Loss: 0.2906074800441477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1323] Loss: 0.2905910277075818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1324] Loss: 0.2905964806390527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1325] Loss: 0.2905848011904625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1326] Loss: 0.2905843421505881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1327] Loss: 0.29057203911172297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1328] Loss: 0.29056870480943325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1329] Loss: 0.2906060275152761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1330] Loss: 0.29061318048749446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1331] Loss: 0.29061298583872097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1332] Loss: 0.2906096636473143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1333] Loss: 0.2906090207892131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1334] Loss: 0.29059529958234365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1335] Loss: 0.29058257470952475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1336] Loss: 0.2905777354156279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1337] Loss: 0.29058528880069295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1338] Loss: 0.29057176034500626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1339] Loss: 0.29057455673076155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1340] Loss: 0.2905795292128508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1341] Loss: 0.2905832859338461\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1342] Loss: 0.29060049412470645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1343] Loss: 0.29058601767354675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1344] Loss: 0.2905995276104774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1345] Loss: 0.2905947481742834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1346] Loss: 0.29059266614865364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1347] Loss: 0.29061650418936696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1348] Loss: 0.2905942044660831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1349] Loss: 0.2906168038577512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1350] Loss: 0.2906015997599028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1351] Loss: 0.29058992217412705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1352] Loss: 0.2905913088103385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1353] Loss: 0.29059599631902977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1354] Loss: 0.2905886818313584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1355] Loss: 0.2905772977893551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1356] Loss: 0.2905684712213166\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8870999999999999\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1357] Loss: 0.2905577563045224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1358] Loss: 0.2905563311375579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1359] Loss: 0.29054409838320583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1360] Loss: 0.2905337740354655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1361] Loss: 0.2905270804021867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1362] Loss: 0.2905552405273669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1363] Loss: 0.2905429894201029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1364] Loss: 0.2905397194483145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1365] Loss: 0.29053830021618376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1366] Loss: 0.29053820465470676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1367] Loss: 0.2905330919009269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1368] Loss: 0.29055263756150795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1369] Loss: 0.29054444432875115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1370] Loss: 0.2905373809521702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1371] Loss: 0.29054093992988694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1372] Loss: 0.29053585677580984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1373] Loss: 0.2905227610319683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1374] Loss: 0.29052367298408305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1375] Loss: 0.2905073523744516\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1376] Loss: 0.2905018078785564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1377] Loss: 0.2904885713729035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1378] Loss: 0.2904814387096642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1379] Loss: 0.29047148901096986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1380] Loss: 0.2904527455451511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1381] Loss: 0.2904525027921274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1382] Loss: 0.2904683468364221\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1383] Loss: 0.29045929347687205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1384] Loss: 0.290479298603182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1385] Loss: 0.2905020699609011\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1386] Loss: 0.29053082814656445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1387] Loss: 0.2905414746869094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1388] Loss: 0.2905298025728115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1389] Loss: 0.29052405185482577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1390] Loss: 0.2905256414415574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1391] Loss: 0.2905146641838495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1392] Loss: 0.2905381339764537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1393] Loss: 0.29052484194818984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1394] Loss: 0.2905074277050904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1395] Loss: 0.29052262552174435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1396] Loss: 0.2905052720729257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1397] Loss: 0.29050746586423254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1398] Loss: 0.29051605571157835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1399] Loss: 0.2905130542773041\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1400] Loss: 0.290506126681469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1401] Loss: 0.29051304412640105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1402] Loss: 0.29049652247223695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1403] Loss: 0.29053316969469695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1404] Loss: 0.2905482967153823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1405] Loss: 0.29053351093560315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1406] Loss: 0.2905204757428765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1407] Loss: 0.29053107249198185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1408] Loss: 0.29051461660828365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1409] Loss: 0.29049639328318844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1410] Loss: 0.2904867742357097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1411] Loss: 0.2904876571289091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1412] Loss: 0.29049118805810437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1413] Loss: 0.2905158308786461\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1414] Loss: 0.29050703993953925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1415] Loss: 0.2905048722136681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1416] Loss: 0.29050726773799873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1417] Loss: 0.2904983961821037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1418] Loss: 0.29050661838785113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1419] Loss: 0.29049239839599617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1420] Loss: 0.2904857744851978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1421] Loss: 0.29047570493549657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1422] Loss: 0.2904904073789424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1423] Loss: 0.29048750703705983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1424] Loss: 0.2904966450051814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1425] Loss: 0.29048496546539193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1426] Loss: 0.29048335740294984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1427] Loss: 0.29048064295301157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1428] Loss: 0.2904732655971653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1429] Loss: 0.2904660754361658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1430] Loss: 0.29046332066116887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1431] Loss: 0.29045964160521265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1432] Loss: 0.29045801919379477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1433] Loss: 0.2904410786686564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1434] Loss: 0.29045580026818385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1435] Loss: 0.2904614429762464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1436] Loss: 0.2904750232793114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1437] Loss: 0.2904929363551972\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1438] Loss: 0.2904835013688726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1439] Loss: 0.29047088502312834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1440] Loss: 0.29046350004266724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1441] Loss: 0.29045788442123743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1442] Loss: 0.2904700328878931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1443] Loss: 0.29047712131745024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1444] Loss: 0.2904955102683988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1445] Loss: 0.2905000739415177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1446] Loss: 0.2905127874417887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1447] Loss: 0.29050630803347993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1448] Loss: 0.29051453578662606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1449] Loss: 0.2905118011907995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1450] Loss: 0.29049734224465557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1451] Loss: 0.29049879526167477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1452] Loss: 0.29051008460168076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1453] Loss: 0.29052667350883155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1454] Loss: 0.29051672217903335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1455] Loss: 0.2905168974796697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1456] Loss: 0.29052239763874194\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1457] Loss: 0.29055137145534154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1458] Loss: 0.2905500444797826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1459] Loss: 0.2905594441201503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1460] Loss: 0.29055993432799976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1461] Loss: 0.2905518365850033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1462] Loss: 0.29054351785310906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1463] Loss: 0.2905576881586657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1464] Loss: 0.290550711928593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1465] Loss: 0.29054023929249273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1466] Loss: 0.29053026191478076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1467] Loss: 0.2905512707043949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1468] Loss: 0.2905472047742104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1469] Loss: 0.29054625775326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1470] Loss: 0.29054221341766334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1471] Loss: 0.29053937158559273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1472] Loss: 0.2905472117280813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1473] Loss: 0.29054279451009907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1474] Loss: 0.2905463219963822\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1475] Loss: 0.2905443982935025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1476] Loss: 0.290574038468765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1477] Loss: 0.2905868783349782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1478] Loss: 0.29058728316837557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1479] Loss: 0.29057825853135044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1480] Loss: 0.2905605650260168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1481] Loss: 0.29057402382682784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1482] Loss: 0.29056801383439745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1483] Loss: 0.2905857454684637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1484] Loss: 0.2905833715794985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1485] Loss: 0.2905952097993495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1486] Loss: 0.2905846779857487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1487] Loss: 0.2905757752067758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1488] Loss: 0.2905633802067877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1489] Loss: 0.29055893612902584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1490] Loss: 0.2905991751656854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1491] Loss: 0.29060085090952525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1492] Loss: 0.2906118248561932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1493] Loss: 0.29060100276190537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1494] Loss: 0.2905890639860504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1495] Loss: 0.2905767027068099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1496] Loss: 0.2905793774821555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1497] Loss: 0.2905721163124776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1498] Loss: 0.2906172919000219\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1499] Loss: 0.29060776700006363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1500] Loss: 0.2906137730861533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1501] Loss: 0.29060082222161304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1502] Loss: 0.29059619929127495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1503] Loss: 0.29058825357619095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1504] Loss: 0.2905839693173101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1505] Loss: 0.29057986690579723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1506] Loss: 0.29057651889039066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1507] Loss: 0.29057707897640295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1508] Loss: 0.290571952006657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1509] Loss: 0.29056892929812334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1510] Loss: 0.2905589386034129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1511] Loss: 0.29055428449463033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1512] Loss: 0.2905535573929624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1513] Loss: 0.2905443139709106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1514] Loss: 0.2905457291816058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1515] Loss: 0.29053385686478794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1516] Loss: 0.2905302244462978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1517] Loss: 0.2905373103952284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1518] Loss: 0.29054078527551636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1519] Loss: 0.29052958820421815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1520] Loss: 0.2905217109577258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1521] Loss: 0.29050849879971874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1522] Loss: 0.29050023147241405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1523] Loss: 0.29051400030665253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 16, Batch 1524] Loss: 0.2905194193731628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 0] Loss: 0.2905068755893503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1] Loss: 0.2905229412301329\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 2] Loss: 0.2905227560148542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 3] Loss: 0.29051529045892915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 4] Loss: 0.2905451976742155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 5] Loss: 0.2905280802962512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 6] Loss: 0.2905218464668886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 7] Loss: 0.29054945151894584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 8] Loss: 0.2905516004628385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 9] Loss: 0.29053678848278447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 10] Loss: 0.29053082848735123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 11] Loss: 0.2905334944491244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 12] Loss: 0.29052449968475913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 13] Loss: 0.29052139633885876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 14] Loss: 0.29050431191085796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 15] Loss: 0.29050351748949854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 16] Loss: 0.29049686329823105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 17] Loss: 0.290500555988504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 18] Loss: 0.2904874941917599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 19] Loss: 0.29049937534282366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 20] Loss: 0.2904932531748591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 21] Loss: 0.2904934296441077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 22] Loss: 0.29049366856989645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 23] Loss: 0.290492365160607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 24] Loss: 0.2904825155757896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 25] Loss: 0.29049197344571814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 26] Loss: 0.290484382120005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 27] Loss: 0.2904804247578856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 28] Loss: 0.29047993767555963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 29] Loss: 0.29049003009926117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 30] Loss: 0.2904802588815583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 31] Loss: 0.29048062569967675\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 32] Loss: 0.29049397665722543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 33] Loss: 0.2905308529188157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 34] Loss: 0.29053234325837407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 35] Loss: 0.2905333864489124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 36] Loss: 0.29052511606572134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 37] Loss: 0.290507318714862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 38] Loss: 0.2905023324699585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 39] Loss: 0.29049188893148187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 40] Loss: 0.29047661588869006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 41] Loss: 0.29049111844596326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 42] Loss: 0.29049656370062105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 43] Loss: 0.29048909671332707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 44] Loss: 0.29048006105990704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 45] Loss: 0.2904860712956364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 46] Loss: 0.29048056959217333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 47] Loss: 0.290466372258777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 48] Loss: 0.29045792361976863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 49] Loss: 0.2904476308789407\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 50] Loss: 0.2904380523422455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 51] Loss: 0.29042607982456264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 52] Loss: 0.2904261133010414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 53] Loss: 0.2904264480907376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 54] Loss: 0.2904282400387324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 55] Loss: 0.2904329782311038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 56] Loss: 0.29042941239512604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 57] Loss: 0.2904355677541537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 58] Loss: 0.2904448236406855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 59] Loss: 0.29044800896122214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 60] Loss: 0.29043980575820216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 61] Loss: 0.2904399174725801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 62] Loss: 0.2904422089149964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 63] Loss: 0.29042628402237314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 64] Loss: 0.29042131281786676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 65] Loss: 0.29041537564502906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 66] Loss: 0.29039871847172355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 67] Loss: 0.2903887933711522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 68] Loss: 0.29037946869124304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 69] Loss: 0.2903762023074882\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 70] Loss: 0.29036673470617064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 71] Loss: 0.2903507356193326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 72] Loss: 0.2903508229260587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 73] Loss: 0.29034440659411703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 74] Loss: 0.29035960592652654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 75] Loss: 0.29034415695759075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 76] Loss: 0.2903390284530609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 77] Loss: 0.2903512560761512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 78] Loss: 0.29035173157270916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 79] Loss: 0.2903515773419253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 80] Loss: 0.29034162373009115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 81] Loss: 0.2903496650728572\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 82] Loss: 0.2903563879188455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 83] Loss: 0.2903436041688\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 84] Loss: 0.29035155228045556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 85] Loss: 0.2903549967183866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 86] Loss: 0.2903598956178421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 87] Loss: 0.2903480024427727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 88] Loss: 0.29034955853426303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 89] Loss: 0.29035176363876564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 90] Loss: 0.29034455924720737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 91] Loss: 0.29035341435393075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 92] Loss: 0.2903786232475869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 93] Loss: 0.2903638874521284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 94] Loss: 0.29036170839868825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 95] Loss: 0.2903667504444573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 96] Loss: 0.29037642871022235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 97] Loss: 0.29038808787826476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 98] Loss: 0.2903839483504968\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 99] Loss: 0.29040980827464946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 100] Loss: 0.2904218660945984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 101] Loss: 0.29041337738891476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 102] Loss: 0.2904096901955583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 103] Loss: 0.2903963657711377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 104] Loss: 0.29039545172691644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 105] Loss: 0.29038454863940755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 106] Loss: 0.29037410161840305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 107] Loss: 0.2903768807117971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 108] Loss: 0.29036032375121773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 109] Loss: 0.2903701062018287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 110] Loss: 0.29036132689385724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 111] Loss: 0.2903626766745179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 112] Loss: 0.2903676816882323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 113] Loss: 0.290362654019779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 114] Loss: 0.29034812059439996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 115] Loss: 0.29035368012363677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 116] Loss: 0.29034844722067593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 117] Loss: 0.2903632427704498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 118] Loss: 0.2903608432257859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 119] Loss: 0.29035706145037854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 120] Loss: 0.29036336531750406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 121] Loss: 0.2903655145872088\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 122] Loss: 0.2903593040466325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 123] Loss: 0.29037492822955463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 124] Loss: 0.2903708860796509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 125] Loss: 0.2903734210297127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 126] Loss: 0.29037897451081984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 127] Loss: 0.2903736604435765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 128] Loss: 0.2903738565095063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 129] Loss: 0.2903598212623295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 130] Loss: 0.29034501372767546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 131] Loss: 0.2903408582061295\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 132] Loss: 0.29034569989003545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 133] Loss: 0.29032763489343155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 134] Loss: 0.29032699333559703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 135] Loss: 0.2903376265467317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 136] Loss: 0.29033009589577513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 137] Loss: 0.2903548731680874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 138] Loss: 0.2903408301228328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 139] Loss: 0.2903362231490895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 140] Loss: 0.29033437229513737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 141] Loss: 0.29035018180566685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 142] Loss: 0.2903624562018758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 143] Loss: 0.2903544086948124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 144] Loss: 0.2903695495222856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 145] Loss: 0.29035638017968285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 146] Loss: 0.2903514973961847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 147] Loss: 0.29034884998848126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 148] Loss: 0.2903472170020912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 149] Loss: 0.2903534660222752\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 150] Loss: 0.2903391127772423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 151] Loss: 0.29032868753594365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 152] Loss: 0.29033570510382256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 153] Loss: 0.290328022758266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 154] Loss: 0.29031528224094777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 155] Loss: 0.2903019913869284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 156] Loss: 0.29029051303752035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 157] Loss: 0.2903055087577645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 158] Loss: 0.2902981452544099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 159] Loss: 0.290297826471182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 160] Loss: 0.2902838468658269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 161] Loss: 0.29026996234161695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 162] Loss: 0.2902724463791838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 163] Loss: 0.29027150686994313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 164] Loss: 0.29025917622771885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 165] Loss: 0.2902698032606099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 166] Loss: 0.29026787878478605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 167] Loss: 0.2902955238507899\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 168] Loss: 0.2903037078264076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 169] Loss: 0.2902967370304166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 170] Loss: 0.2902976433560939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 171] Loss: 0.2903164633741409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 172] Loss: 0.2903025653268238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 173] Loss: 0.2902845276445274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 174] Loss: 0.2902734209767916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 175] Loss: 0.29026634932326906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 176] Loss: 0.2902505338647429\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 177] Loss: 0.2902479892931628\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 178] Loss: 0.29023307093019407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 179] Loss: 0.2902430056442626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 180] Loss: 0.29023291675828844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 181] Loss: 0.29022894900609036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 182] Loss: 0.2902276483029704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 183] Loss: 0.2902217162104534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 184] Loss: 0.29022186160360786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 185] Loss: 0.2902316683667396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 186] Loss: 0.2902183214908736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 187] Loss: 0.29020280220822986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 188] Loss: 0.29018702641641386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 189] Loss: 0.29017573722631074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 190] Loss: 0.2901810337492637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 191] Loss: 0.290181400169477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 192] Loss: 0.2901860022054146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 193] Loss: 0.2902101612620966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 194] Loss: 0.2902381074541442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 195] Loss: 0.2902344140759466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 196] Loss: 0.2902330160795814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 197] Loss: 0.29023193011991594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 198] Loss: 0.2902461893641948\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 199] Loss: 0.29024863845750737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 200] Loss: 0.29027709993803674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 201] Loss: 0.29028577012964374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 202] Loss: 0.2903016981054742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 203] Loss: 0.29030541770395235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 204] Loss: 0.2902879265522598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 205] Loss: 0.2902755706587742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 206] Loss: 0.2902605199258817\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 207] Loss: 0.2902529739710632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 208] Loss: 0.29026379223688986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 209] Loss: 0.29025468750774547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 210] Loss: 0.2902473660901545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 211] Loss: 0.29024752479345467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 212] Loss: 0.2902363624164937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 213] Loss: 0.29022809857921733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 214] Loss: 0.29022911584223127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 215] Loss: 0.29023143862419903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 216] Loss: 0.29022557010157907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 217] Loss: 0.29023434094625467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 218] Loss: 0.29023642103652375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 219] Loss: 0.29027209556797107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 220] Loss: 0.2902961467446621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 221] Loss: 0.29030758988879435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 222] Loss: 0.29030192512786274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 223] Loss: 0.2902973136577075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 224] Loss: 0.29030813081612306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 225] Loss: 0.29030171805268695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 226] Loss: 0.2903044286519754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 227] Loss: 0.290294477224659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 228] Loss: 0.29027961883820413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 229] Loss: 0.2902675848271373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 230] Loss: 0.29025430114205003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 231] Loss: 0.29027403137792623\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 232] Loss: 0.29026411486086695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 233] Loss: 0.290255416696494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 234] Loss: 0.2902504596015355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 235] Loss: 0.29025235334765687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 236] Loss: 0.2902658018878752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 237] Loss: 0.2902615098790751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 238] Loss: 0.29025652509204547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 239] Loss: 0.2902637130143482\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 240] Loss: 0.29026401585341866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 241] Loss: 0.29026556753210064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 242] Loss: 0.2902760370337483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 243] Loss: 0.2902687912310015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 244] Loss: 0.29026261022710714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 245] Loss: 0.29025090719226354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 246] Loss: 0.2902577742186848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 247] Loss: 0.29025868436542906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 248] Loss: 0.29028672742466116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 249] Loss: 0.29027230942804183\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 250] Loss: 0.29028554318077016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 251] Loss: 0.29028515792682474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 252] Loss: 0.2902751455918798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 253] Loss: 0.29027394287013303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 254] Loss: 0.29026150340893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 255] Loss: 0.29025024344262323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 256] Loss: 0.2902486510585046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 257] Loss: 0.29024815931166204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 258] Loss: 0.29025704079883047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 259] Loss: 0.290256670624105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 260] Loss: 0.290257348481835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 261] Loss: 0.2902660735296386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 262] Loss: 0.2902558705750055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 263] Loss: 0.29023992683171823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 264] Loss: 0.2902319621490984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 265] Loss: 0.2902340279184333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 266] Loss: 0.29022421983146296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 267] Loss: 0.2902155618280475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 268] Loss: 0.290203003123577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 269] Loss: 0.2902146600382982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 270] Loss: 0.29023055945177456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 271] Loss: 0.29024265272109223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 272] Loss: 0.2902578450381903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 273] Loss: 0.29025280538425197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 274] Loss: 0.2902546459090711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 275] Loss: 0.29025859869693227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 276] Loss: 0.2902630261240315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 277] Loss: 0.2902461038811371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 278] Loss: 0.29025264401233813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 279] Loss: 0.29024910867514975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 280] Loss: 0.29026646501703623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 281] Loss: 0.2902687811335655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 282] Loss: 0.29025429535835406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 283] Loss: 0.2902566208700132\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 284] Loss: 0.2902967153320771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 285] Loss: 0.2902876444671019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 286] Loss: 0.2902697964713206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 287] Loss: 0.29026577165463363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 288] Loss: 0.2902658626686718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 289] Loss: 0.2902611538224607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 290] Loss: 0.29026854671231966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 291] Loss: 0.2902687070749366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 292] Loss: 0.29027558531448355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 293] Loss: 0.29026678257783994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 294] Loss: 0.29026868947792156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 295] Loss: 0.29027838692531105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 296] Loss: 0.2902795926436915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 297] Loss: 0.2902762904397045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 298] Loss: 0.29027585802059064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 299] Loss: 0.2902762696791564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 300] Loss: 0.29028786693545994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 301] Loss: 0.290297045761475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 302] Loss: 0.29031197512654755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 303] Loss: 0.29032448474866074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 304] Loss: 0.2903139039623552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 305] Loss: 0.2903049892971212\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 306] Loss: 0.2902906161781167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 307] Loss: 0.2902850965169449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 308] Loss: 0.29027592190123086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 309] Loss: 0.290287788834527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 310] Loss: 0.2902816208303382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 311] Loss: 0.29026936601303593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 312] Loss: 0.29027283893692224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 313] Loss: 0.2903008919657308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 314] Loss: 0.290299515715052\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 315] Loss: 0.2903116292634307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 316] Loss: 0.2903060339497707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 317] Loss: 0.2903190464513056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 318] Loss: 0.29034896463414767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 319] Loss: 0.2903386801545657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 320] Loss: 0.29034377465005945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 321] Loss: 0.2903279308024626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 322] Loss: 0.290325234319101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 323] Loss: 0.29033238848049986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 324] Loss: 0.290322951755961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 325] Loss: 0.2903375409679041\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 326] Loss: 0.2903409922256979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 327] Loss: 0.29033766794267857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 328] Loss: 0.2903417338743998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 329] Loss: 0.2903296781455059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 330] Loss: 0.2903185846361924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 331] Loss: 0.29031620376843525\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.892\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 332] Loss: 0.2903064119899935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 333] Loss: 0.2903060841705136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 334] Loss: 0.29030831709743815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 335] Loss: 0.29030747829222614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 336] Loss: 0.29030309663180076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 337] Loss: 0.29030009683049307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 338] Loss: 0.29029045414570775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 339] Loss: 0.29028647491086274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 340] Loss: 0.29027820498082824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 341] Loss: 0.2902663437450504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 342] Loss: 0.2902526666731308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 343] Loss: 0.29023803444219837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 344] Loss: 0.29022392474482894\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 345] Loss: 0.29020815593847027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 346] Loss: 0.2902351058143703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 347] Loss: 0.2902352899246021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 348] Loss: 0.2902537490164986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 349] Loss: 0.2902434952063492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 350] Loss: 0.2902474860520442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 351] Loss: 0.29025377245313483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 352] Loss: 0.2902553095454406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 353] Loss: 0.29024393251752945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 354] Loss: 0.290241880692793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 355] Loss: 0.2902446774415283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 356] Loss: 0.2902450196809206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 357] Loss: 0.29026742005242534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 358] Loss: 0.29025627043717067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 359] Loss: 0.2902378015502543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 360] Loss: 0.29023222968975254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 361] Loss: 0.29024611251546417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 362] Loss: 0.2902256965767552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 363] Loss: 0.2902080522103781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 364] Loss: 0.2902476090852773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 365] Loss: 0.2902730378752307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 366] Loss: 0.29027913464857763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 367] Loss: 0.29028680982672184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 368] Loss: 0.2902840451226192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 369] Loss: 0.29028377095892904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 370] Loss: 0.2902870248135962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 371] Loss: 0.29028062474466076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 372] Loss: 0.2903024012344678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 373] Loss: 0.2902904898532084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 374] Loss: 0.29030566887571807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 375] Loss: 0.2902947281967493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 376] Loss: 0.2902974724609735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 377] Loss: 0.29028376753160323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 378] Loss: 0.290288689438723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 379] Loss: 0.2902942996278927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 380] Loss: 0.29030311113804985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 381] Loss: 0.29032136910435635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 382] Loss: 0.2903150830723256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 383] Loss: 0.2903070322856459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 384] Loss: 0.2903044322808564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 385] Loss: 0.2903118947329617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 386] Loss: 0.2902996306952531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 387] Loss: 0.290297607724235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 388] Loss: 0.29028518869832826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 389] Loss: 0.2903046746186569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 390] Loss: 0.29030049752282355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 391] Loss: 0.29029159241779223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 392] Loss: 0.2902903412227103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 393] Loss: 0.29028116925826214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 394] Loss: 0.2902710191438827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 395] Loss: 0.29026375521989467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 396] Loss: 0.2902731967081019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 397] Loss: 0.29027873604009985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 398] Loss: 0.2902766550794502\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 399] Loss: 0.29026667655358385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 400] Loss: 0.29025256038486097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 401] Loss: 0.29026521327349725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 402] Loss: 0.29026643423911325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 403] Loss: 0.29027466357102844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 404] Loss: 0.2902635137873645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 405] Loss: 0.29029334979152843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 406] Loss: 0.2903056549044234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 407] Loss: 0.2903081779066805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 408] Loss: 0.290311323431739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 409] Loss: 0.2903126927203801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 410] Loss: 0.29030756478429864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 411] Loss: 0.29031358012678593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 412] Loss: 0.2903159828757421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 413] Loss: 0.2903017648153305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 414] Loss: 0.2902916947887886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 415] Loss: 0.2902920352559693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 416] Loss: 0.29028092269859124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 417] Loss: 0.2902626368005739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 418] Loss: 0.29024814236956187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 419] Loss: 0.2902410178244469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 420] Loss: 0.29025560270346584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 421] Loss: 0.29025770722711314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 422] Loss: 0.2902452786497737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 423] Loss: 0.29025386368126593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 424] Loss: 0.2902647397867571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 425] Loss: 0.29029207609261465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 426] Loss: 0.2902739140730171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 427] Loss: 0.2902635528947804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 428] Loss: 0.2902873156343517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 429] Loss: 0.2902888312888168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 430] Loss: 0.29028675737245396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 431] Loss: 0.29028658541173624\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 432] Loss: 0.2902922081358233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 433] Loss: 0.2902808015870665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 434] Loss: 0.29027719528485435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 435] Loss: 0.29026322160813517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 436] Loss: 0.29025876267371886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 437] Loss: 0.2902740430654684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 438] Loss: 0.29027460143875583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 439] Loss: 0.2902638852802182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 440] Loss: 0.2902539581566727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 441] Loss: 0.2902482574353493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 442] Loss: 0.2902397545677448\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 443] Loss: 0.29027092121028614\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 444] Loss: 0.2902656288367366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 445] Loss: 0.2902527474570773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 446] Loss: 0.2902540518688095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 447] Loss: 0.29025320750404426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 448] Loss: 0.2902532607249287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 449] Loss: 0.2902464701661999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 450] Loss: 0.2902404769179713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 451] Loss: 0.29024759583994536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 452] Loss: 0.2902340309862218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 453] Loss: 0.2902190122238613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 454] Loss: 0.29021934421243417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 455] Loss: 0.29020349134253653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 456] Loss: 0.29019095060692973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 457] Loss: 0.2901946254860678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 458] Loss: 0.2901988177584373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 459] Loss: 0.29022664492113837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 460] Loss: 0.29022452394308157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 461] Loss: 0.2902234295570013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 462] Loss: 0.29021498725585243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 463] Loss: 0.29021090610431477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 464] Loss: 0.29025493614718206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 465] Loss: 0.29025789406443314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 466] Loss: 0.29027403916008826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 467] Loss: 0.2903018120706049\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 468] Loss: 0.2903023044660508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 469] Loss: 0.2903304097632021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 470] Loss: 0.29032604354375197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 471] Loss: 0.2903183856469924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 472] Loss: 0.2903169867285218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 473] Loss: 0.2903225096805989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 474] Loss: 0.2903121580885909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 475] Loss: 0.29030349545177453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 476] Loss: 0.2903032629238801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 477] Loss: 0.2902886597094589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 478] Loss: 0.2903153000581257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 479] Loss: 0.290308985654664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 480] Loss: 0.2903083395934559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 481] Loss: 0.29032336454629715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 482] Loss: 0.2903262693972769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 483] Loss: 0.2903225860272615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 484] Loss: 0.2903305110818028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 485] Loss: 0.2903169535714941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 486] Loss: 0.29033408156008744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 487] Loss: 0.29033860262021577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 488] Loss: 0.29033177881814615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 489] Loss: 0.2903254152707023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 490] Loss: 0.29031869241180014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 491] Loss: 0.29034463257423954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 492] Loss: 0.29032461909930024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 493] Loss: 0.29032410460893937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 494] Loss: 0.29032656609436774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 495] Loss: 0.29033326358503947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 496] Loss: 0.2903320974597453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 497] Loss: 0.2903242590056144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 498] Loss: 0.2903145081366954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 499] Loss: 0.29030511781575963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 500] Loss: 0.29030185540239245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 501] Loss: 0.2903168434524785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 502] Loss: 0.29030504165603416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 503] Loss: 0.29030965475098286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 504] Loss: 0.2903249218519187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 505] Loss: 0.29034185555590614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 506] Loss: 0.2903297699673892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 507] Loss: 0.290336535810435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 508] Loss: 0.2903429200460524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 509] Loss: 0.29035610203587986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 510] Loss: 0.2903488975747993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 511] Loss: 0.2903421121746816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 512] Loss: 0.2903310157411284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 513] Loss: 0.29033570517819574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 514] Loss: 0.2903349457794297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 515] Loss: 0.29034576423509506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 516] Loss: 0.29036122710531787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 517] Loss: 0.2903579899048378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 518] Loss: 0.29035489721217717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 519] Loss: 0.29035927917196014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 520] Loss: 0.29034732585811157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 521] Loss: 0.29034226836831234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 522] Loss: 0.2903339742959808\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 523] Loss: 0.2903410967192791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 524] Loss: 0.2903277644275559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 525] Loss: 0.29032924894997186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 526] Loss: 0.29031633171803584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 527] Loss: 0.2903013557180359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 528] Loss: 0.2902967631926647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 529] Loss: 0.2902859626157426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 530] Loss: 0.2902838270329604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 531] Loss: 0.29026612544407504\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 532] Loss: 0.29027007718900927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 533] Loss: 0.2902815352001728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 534] Loss: 0.29028775526183404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 535] Loss: 0.29027952217745867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 536] Loss: 0.290272329210392\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 537] Loss: 0.2902630357279678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 538] Loss: 0.2902707495376984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 539] Loss: 0.2902553090190061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 540] Loss: 0.2902541322392693\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 541] Loss: 0.2902567655179723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 542] Loss: 0.2902443718588458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 543] Loss: 0.29024353845950795\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 544] Loss: 0.29023509576487494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 545] Loss: 0.2902212123741569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 546] Loss: 0.2902231361918263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 547] Loss: 0.29020611622954545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 548] Loss: 0.2902119569789255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 549] Loss: 0.29022702672054335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 550] Loss: 0.29021764552974394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 551] Loss: 0.2902161710059889\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 552] Loss: 0.2902114311108013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 553] Loss: 0.2902151916850238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 554] Loss: 0.2902086450358559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 555] Loss: 0.2902105976886765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 556] Loss: 0.29019690019840305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 557] Loss: 0.29019115161483505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 558] Loss: 0.2901930784797319\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 559] Loss: 0.2901825254574194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 560] Loss: 0.29017919230035183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 561] Loss: 0.2901636643623634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 562] Loss: 0.29016242899893496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 563] Loss: 0.2901560503296003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 564] Loss: 0.2901523763591446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 565] Loss: 0.29013399905232945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 566] Loss: 0.29012884527289673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 567] Loss: 0.29013131450388613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 568] Loss: 0.2901218066946359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 569] Loss: 0.2901258410778039\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 570] Loss: 0.29011829600261785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 571] Loss: 0.2901212178586922\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 572] Loss: 0.2901231168125277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 573] Loss: 0.2901315355765711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 574] Loss: 0.29011735539255207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 575] Loss: 0.29012349087557776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 576] Loss: 0.29011837924477335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 577] Loss: 0.29012767580080545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 578] Loss: 0.2901203267108702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 579] Loss: 0.29011333573678144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 580] Loss: 0.29011171512570644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 581] Loss: 0.29011949490974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 582] Loss: 0.2901070590713351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 583] Loss: 0.2901024182340251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 584] Loss: 0.29009759850715294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 585] Loss: 0.290098962544905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 586] Loss: 0.29009573035936354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 587] Loss: 0.2901100483792067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 588] Loss: 0.2901179061437859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 589] Loss: 0.2901273088425316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 590] Loss: 0.2901175627656956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 591] Loss: 0.29011599129320104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 592] Loss: 0.2901121684472629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 593] Loss: 0.29012183820931164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 594] Loss: 0.2901135526332258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 595] Loss: 0.2901253790347855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 596] Loss: 0.2901228405617888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 597] Loss: 0.29011054385925944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 598] Loss: 0.29011087753916776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 599] Loss: 0.29009598190931335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 600] Loss: 0.29008795070596627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 601] Loss: 0.29009664202031804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 602] Loss: 0.2900979633587151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 603] Loss: 0.29009010282002345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 604] Loss: 0.290093195278522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 605] Loss: 0.29007961908128205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 606] Loss: 0.29009304509926676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 607] Loss: 0.2900782729153585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 608] Loss: 0.2900898799641739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 609] Loss: 0.2900823443069141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 610] Loss: 0.29008359261127914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 611] Loss: 0.290078228364485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 612] Loss: 0.29008110603946646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 613] Loss: 0.29008528134206724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 614] Loss: 0.2900813187217325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 615] Loss: 0.29008928607253043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 616] Loss: 0.29009140488608615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 617] Loss: 0.2901053644546433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 618] Loss: 0.2901100278904623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 619] Loss: 0.29010022509589345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 620] Loss: 0.29009234168664927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 621] Loss: 0.2900883435583386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 622] Loss: 0.29011329090611343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 623] Loss: 0.2901065361932716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 624] Loss: 0.29010859909492964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 625] Loss: 0.2901223711656252\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 626] Loss: 0.29013011892683666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 627] Loss: 0.29011922924648853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 628] Loss: 0.2901113253109227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 629] Loss: 0.2901046951929888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 630] Loss: 0.29009658080715256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 631] Loss: 0.2900815103362935\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 632] Loss: 0.29009647758809776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 633] Loss: 0.29010976172339636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 634] Loss: 0.2901266401957243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 635] Loss: 0.29017006166374876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 636] Loss: 0.2901702888435823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 637] Loss: 0.2901873138921087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 638] Loss: 0.2902241810239775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 639] Loss: 0.29022578210744093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 640] Loss: 0.29024322506083156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 641] Loss: 0.29023513595656186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 642] Loss: 0.29023281658362055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 643] Loss: 0.29023314008276363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 644] Loss: 0.2902352393627699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 645] Loss: 0.2902437758367382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 646] Loss: 0.2902340976496353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 647] Loss: 0.2902317041328926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 648] Loss: 0.2902205191117506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 649] Loss: 0.29023529421358746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 650] Loss: 0.2902508487711025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 651] Loss: 0.2902410569052958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 652] Loss: 0.2902440958951018\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 653] Loss: 0.2902439315683675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 654] Loss: 0.29023394783440987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 655] Loss: 0.29024050246833705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 656] Loss: 0.29024359896751434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 657] Loss: 0.2902375648165382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 658] Loss: 0.29023514130306666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 659] Loss: 0.2902281999772814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 660] Loss: 0.2902616586411226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 661] Loss: 0.29025316654264205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 662] Loss: 0.29026487630394515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 663] Loss: 0.2902600560075979\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 664] Loss: 0.2902496105925403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 665] Loss: 0.290255550223468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 666] Loss: 0.29024268868730924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 667] Loss: 0.2902402616905858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 668] Loss: 0.2902506938725605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 669] Loss: 0.2902516304756038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 670] Loss: 0.29023435328063774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 671] Loss: 0.29025150244808706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 672] Loss: 0.290245014805957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 673] Loss: 0.29025966786771007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 674] Loss: 0.2902748305550862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 675] Loss: 0.29026427218709394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 676] Loss: 0.2902555662216663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 677] Loss: 0.29026972611040874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 678] Loss: 0.290270991709082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 679] Loss: 0.29026270806375803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 680] Loss: 0.29025740067634553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 681] Loss: 0.29024249539908237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 682] Loss: 0.2902446044540037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 683] Loss: 0.2902438563775341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 684] Loss: 0.29024721465424763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 685] Loss: 0.2902471422430625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 686] Loss: 0.2902421252099996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 687] Loss: 0.2902637331096143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 688] Loss: 0.2902684202694881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 689] Loss: 0.29026336770939914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 690] Loss: 0.29025779935157475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 691] Loss: 0.29026064581151906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 692] Loss: 0.29025092010583503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 693] Loss: 0.29026256896693775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 694] Loss: 0.29025184102087626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 695] Loss: 0.29025446666798765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 696] Loss: 0.29024827834458006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 697] Loss: 0.2902542486248712\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 698] Loss: 0.29026604719220134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 699] Loss: 0.29028958210285644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 700] Loss: 0.2902930521991893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 701] Loss: 0.2903039767789779\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 702] Loss: 0.290303713712819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 703] Loss: 0.29030102093496946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 704] Loss: 0.29032121390839266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 705] Loss: 0.2903276600592084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 706] Loss: 0.29032921837872017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 707] Loss: 0.2903260194240607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 708] Loss: 0.2903255136969414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 709] Loss: 0.29033308365521476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 710] Loss: 0.29033068996642986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 711] Loss: 0.2903296719860443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 712] Loss: 0.29036741167276786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 713] Loss: 0.2903934393932585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 714] Loss: 0.29038502628478474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 715] Loss: 0.2903762815723856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 716] Loss: 0.2903710084872596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 717] Loss: 0.29037374803372584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 718] Loss: 0.290364699121231\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 719] Loss: 0.2903815320126182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 720] Loss: 0.2903758829853663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 721] Loss: 0.2903873761568865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 722] Loss: 0.2903719655414023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 723] Loss: 0.2903717617974906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 724] Loss: 0.290380380167898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 725] Loss: 0.2903763631503406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 726] Loss: 0.29037120445246034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 727] Loss: 0.2903924261147807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 728] Loss: 0.29038403968709636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 729] Loss: 0.2903794299261106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 730] Loss: 0.29037278042191217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 731] Loss: 0.2903711927360181\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 732] Loss: 0.2903645567833159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 733] Loss: 0.29038743865838346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 734] Loss: 0.2903877244785334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 735] Loss: 0.29039704681703626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 736] Loss: 0.29042264792657296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 737] Loss: 0.29042756869053926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 738] Loss: 0.29046206816483605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 739] Loss: 0.2904651860543002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 740] Loss: 0.29048270536300963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 741] Loss: 0.2904683809106168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 742] Loss: 0.2904654469186309\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 743] Loss: 0.29046129070083326\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 744] Loss: 0.29045916942085476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 745] Loss: 0.2904582077197558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 746] Loss: 0.29047162370878965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 747] Loss: 0.29046612253512616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 748] Loss: 0.29045054906570394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 749] Loss: 0.2904447932603747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 750] Loss: 0.29044518738678426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 751] Loss: 0.29043300801762695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 752] Loss: 0.29042537226285403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 753] Loss: 0.29042034950992757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 754] Loss: 0.29041067253131714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 755] Loss: 0.2903949143298997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 756] Loss: 0.29038490518000326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 757] Loss: 0.2903756065827332\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 758] Loss: 0.29035887095652124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 759] Loss: 0.29036762316511033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 760] Loss: 0.2903668958250489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 761] Loss: 0.29036192932769794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 762] Loss: 0.29035368737157896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 763] Loss: 0.2903610277671045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 764] Loss: 0.2903673044678416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 765] Loss: 0.29036601234763326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 766] Loss: 0.2903697163009271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 767] Loss: 0.29036913437807416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 768] Loss: 0.29035336628698255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 769] Loss: 0.2903672188845446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 770] Loss: 0.2903548565084258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 771] Loss: 0.290354554207253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 772] Loss: 0.29034528139606836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 773] Loss: 0.290348381980932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 774] Loss: 0.2903365042957105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 775] Loss: 0.2903392408220541\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 776] Loss: 0.2903340609974296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 777] Loss: 0.290336402151169\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 778] Loss: 0.29032899241583393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 779] Loss: 0.29033252459511727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 780] Loss: 0.2903361238230134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 781] Loss: 0.2903267033879632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 782] Loss: 0.2903218662107335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 783] Loss: 0.2903136803605047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 784] Loss: 0.2903201982027051\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 785] Loss: 0.29031171256612087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 786] Loss: 0.2903025708569073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 787] Loss: 0.2902972717613614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 788] Loss: 0.29031667580412235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 789] Loss: 0.29032248328779214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 790] Loss: 0.2903394258403939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 791] Loss: 0.2903301564300651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 792] Loss: 0.2903270931208197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 793] Loss: 0.2903375394946538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 794] Loss: 0.2903244461905177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 795] Loss: 0.2903167283043064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 796] Loss: 0.29030316482859664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 797] Loss: 0.29031698501871833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 798] Loss: 0.2903086611439734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 799] Loss: 0.2903138031080795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 800] Loss: 0.29030790797027956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 801] Loss: 0.2903212037656163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 802] Loss: 0.2903090535401337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 803] Loss: 0.2903014179077241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 804] Loss: 0.29028607247741783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 805] Loss: 0.2902698268697542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 806] Loss: 0.290263217929056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 807] Loss: 0.2902601614748314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 808] Loss: 0.29025537508540994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 809] Loss: 0.29024188739068885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 810] Loss: 0.2902568752598342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 811] Loss: 0.2902674456937768\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 812] Loss: 0.2902720506632163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 813] Loss: 0.29028738483456973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 814] Loss: 0.29027894940240245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 815] Loss: 0.2902807519089594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 816] Loss: 0.2902661257368486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 817] Loss: 0.2902564383035277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 818] Loss: 0.290273426953135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 819] Loss: 0.29026872651817354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 820] Loss: 0.2902583452170185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 821] Loss: 0.290257432857972\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 822] Loss: 0.2902680797654107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 823] Loss: 0.2902658868415423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 824] Loss: 0.29025353067795184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 825] Loss: 0.2902510954033893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 826] Loss: 0.29023913105641264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 827] Loss: 0.29023166133595196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 828] Loss: 0.2902484005613547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 829] Loss: 0.2902392307511023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 830] Loss: 0.2902322074605719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 831] Loss: 0.2902414830340483\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8833\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 832] Loss: 0.2902522472999284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 833] Loss: 0.29026142650254805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 834] Loss: 0.29026723769817225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 835] Loss: 0.2902764236487315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 836] Loss: 0.2902754575506891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 837] Loss: 0.2902906267641758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 838] Loss: 0.2902781383451987\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 839] Loss: 0.2902864947619229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 840] Loss: 0.2902872581144942\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 841] Loss: 0.29027125020631594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 842] Loss: 0.2902780025579356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 843] Loss: 0.29027034609851077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 844] Loss: 0.29029782171059765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 845] Loss: 0.2902947912465812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 846] Loss: 0.2902964220649798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 847] Loss: 0.29028233659892866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 848] Loss: 0.29026794649031834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 849] Loss: 0.2902800902185475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 850] Loss: 0.29027689001167245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 851] Loss: 0.2902723152137741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 852] Loss: 0.290256128067206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 853] Loss: 0.2902432148124411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 854] Loss: 0.29024251448834254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 855] Loss: 0.29024335437670445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 856] Loss: 0.29023350798027986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 857] Loss: 0.2902206181552333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 858] Loss: 0.29022008258206955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 859] Loss: 0.2902296126274634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 860] Loss: 0.2902426977266282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 861] Loss: 0.29025953362516815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 862] Loss: 0.29024735119638295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 863] Loss: 0.2902587797474331\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 864] Loss: 0.29026512570294194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 865] Loss: 0.29025839857101493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 866] Loss: 0.29026034798882594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 867] Loss: 0.2902549136137155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 868] Loss: 0.2902508910111263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 869] Loss: 0.2902467128469544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 870] Loss: 0.29025130289117684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 871] Loss: 0.2902487136253156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 872] Loss: 0.29026267421435875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 873] Loss: 0.2902543283656871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 874] Loss: 0.2902433033803464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 875] Loss: 0.29024471234951305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 876] Loss: 0.29025447030390245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 877] Loss: 0.290257284748595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 878] Loss: 0.29027313146169853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 879] Loss: 0.290265024046713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 880] Loss: 0.29026345227736156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 881] Loss: 0.2902645366802539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 882] Loss: 0.2902781169895738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 883] Loss: 0.29027725876797733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 884] Loss: 0.29027791081684123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 885] Loss: 0.29027892939749605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 886] Loss: 0.2902793716262109\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 887] Loss: 0.2902967156715832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 888] Loss: 0.29029332980646366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 889] Loss: 0.29028590619518935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 890] Loss: 0.2902712824374154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 891] Loss: 0.29027913440199016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 892] Loss: 0.2902755968377724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 893] Loss: 0.29027271893094186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 894] Loss: 0.29028421472268395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 895] Loss: 0.29029798900061904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 896] Loss: 0.29029319218172916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 897] Loss: 0.2902843024824183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 898] Loss: 0.2902896079427355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 899] Loss: 0.29027631526996617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 900] Loss: 0.2902873891512819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 901] Loss: 0.29027536133225845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 902] Loss: 0.29027848955400926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 903] Loss: 0.290273396905866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 904] Loss: 0.2902610868048834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 905] Loss: 0.2902617051795157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 906] Loss: 0.2902561963179278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 907] Loss: 0.2902562578280422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 908] Loss: 0.29026452197531927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 909] Loss: 0.2902555058908381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 910] Loss: 0.2902463334627025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 911] Loss: 0.29025855906228565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 912] Loss: 0.2902607417695496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 913] Loss: 0.2902498674485455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 914] Loss: 0.2902431153613194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 915] Loss: 0.2902580394694581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 916] Loss: 0.29024672542868934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 917] Loss: 0.29023747988636145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 918] Loss: 0.2902320861809334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 919] Loss: 0.29021937389912633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 920] Loss: 0.29020879465774846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 921] Loss: 0.2902099141635251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 922] Loss: 0.29020034154848784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 923] Loss: 0.29019636580704317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 924] Loss: 0.29018516401984074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 925] Loss: 0.2901866757767552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 926] Loss: 0.2901982198819189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 927] Loss: 0.29020254592804484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 928] Loss: 0.290192921243635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 929] Loss: 0.2901792721714462\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 930] Loss: 0.2901689556770926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 931] Loss: 0.2901792382657189\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 932] Loss: 0.29017574245865035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 933] Loss: 0.2901711349658804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 934] Loss: 0.29015893871699816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 935] Loss: 0.29015831840142303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 936] Loss: 0.2901446839799973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 937] Loss: 0.29014685536157286\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 938] Loss: 0.2901508878691213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 939] Loss: 0.29016561986577993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 940] Loss: 0.2901898957066989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 941] Loss: 0.29017614807679143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 942] Loss: 0.29016641780119384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 943] Loss: 0.2901509297658956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 944] Loss: 0.2901830658312122\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 945] Loss: 0.2901923278029936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 946] Loss: 0.29021514796994413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 947] Loss: 0.2902054008289824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 948] Loss: 0.2902104380694621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 949] Loss: 0.29019520392609205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 950] Loss: 0.29019583084631756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 951] Loss: 0.2902308503516379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 952] Loss: 0.29023878755648286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 953] Loss: 0.2902391959642965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 954] Loss: 0.2902291201914584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 955] Loss: 0.2902454931354651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 956] Loss: 0.2902547681235378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 957] Loss: 0.2902534674076757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 958] Loss: 0.2902489159763049\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 959] Loss: 0.29025904813416026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 960] Loss: 0.29028856992651086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 961] Loss: 0.29027295094705663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 962] Loss: 0.2902839080353064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 963] Loss: 0.2902843297314999\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 964] Loss: 0.2902757156760689\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 965] Loss: 0.2902884062227513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 966] Loss: 0.2902893716259387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 967] Loss: 0.2902902035684333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 968] Loss: 0.2902854836623828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 969] Loss: 0.29028777322145466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 970] Loss: 0.290289096251655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 971] Loss: 0.2902819171798141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 972] Loss: 0.29028233515251967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 973] Loss: 0.29027362752241553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 974] Loss: 0.29026524574287577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 975] Loss: 0.2902727329168595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 976] Loss: 0.290282567942532\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 977] Loss: 0.29026397845112195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 978] Loss: 0.2902547517643633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 979] Loss: 0.29024486085522305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 980] Loss: 0.29023965027870796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 981] Loss: 0.2902327415278931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 982] Loss: 0.2902227097526295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 983] Loss: 0.2902269139332847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 984] Loss: 0.29024946153252135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 985] Loss: 0.29025947078678904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 986] Loss: 0.2902686723952828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 987] Loss: 0.2903105263186251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 988] Loss: 0.2903221229988897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 989] Loss: 0.29031234900803793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 990] Loss: 0.29031917707799687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 991] Loss: 0.29032583729495837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 992] Loss: 0.2903288294446369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 993] Loss: 0.29032778362928413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 994] Loss: 0.29033503824664825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 995] Loss: 0.29032422437676275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 996] Loss: 0.2903303588206769\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 997] Loss: 0.29031841389485563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 998] Loss: 0.29030854681926277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 999] Loss: 0.29032054717325034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1000] Loss: 0.29031800745085407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1001] Loss: 0.29031557136995456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1002] Loss: 0.29030133104985906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1003] Loss: 0.29029017104577776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1004] Loss: 0.2902859189168336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1005] Loss: 0.29028019082973805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1006] Loss: 0.29026621835208655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1007] Loss: 0.2902544265986382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1008] Loss: 0.29026082248014984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1009] Loss: 0.2902536575337731\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1010] Loss: 0.2902832692473081\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1011] Loss: 0.29032896770590605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1012] Loss: 0.2903446990574387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1013] Loss: 0.2903568166938101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1014] Loss: 0.2903502388423368\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1015] Loss: 0.29034272501158265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1016] Loss: 0.29034852762213265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1017] Loss: 0.29036592855753734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1018] Loss: 0.2903566849424359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1019] Loss: 0.29035613529969906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1020] Loss: 0.29035471420770176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1021] Loss: 0.2903612573953657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1022] Loss: 0.2903526263321701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1023] Loss: 0.29035904424443915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1024] Loss: 0.2903572701657427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1025] Loss: 0.2903511800279649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1026] Loss: 0.2903503595699114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1027] Loss: 0.2903660252308773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1028] Loss: 0.2903547798932888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1029] Loss: 0.29034993813621285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1030] Loss: 0.29035533709881395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1031] Loss: 0.29034822136730415\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1032] Loss: 0.29033135449018127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1033] Loss: 0.2903184741375758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1034] Loss: 0.2903152589612371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1035] Loss: 0.2903046679310262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1036] Loss: 0.2902997704052541\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1037] Loss: 0.2902844650584174\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1038] Loss: 0.29027288530892453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1039] Loss: 0.29027515904415896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1040] Loss: 0.2902683922192233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1041] Loss: 0.2902809674020953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1042] Loss: 0.2902922235194548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1043] Loss: 0.29029349158582024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1044] Loss: 0.29033416059406897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1045] Loss: 0.2903399957492598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1046] Loss: 0.29033279937325374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1047] Loss: 0.29033312089044944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1048] Loss: 0.29031771852743415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1049] Loss: 0.290329404689301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1050] Loss: 0.29031908979969984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1051] Loss: 0.29031957663413244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1052] Loss: 0.2903429851189543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1053] Loss: 0.2903421909989177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1054] Loss: 0.29032770273536546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1055] Loss: 0.29032204985713905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1056] Loss: 0.2903200902274149\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1057] Loss: 0.2903174878934887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1058] Loss: 0.2903030610724008\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1059] Loss: 0.2902889029763754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1060] Loss: 0.29028099214742303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1061] Loss: 0.29028636008194925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1062] Loss: 0.29029731614946397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1063] Loss: 0.2903033034099847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1064] Loss: 0.2902912565540171\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1065] Loss: 0.29028145653907167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1066] Loss: 0.2902890981750178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1067] Loss: 0.29029306531989707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1068] Loss: 0.2902846096359951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1069] Loss: 0.2902850129836203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1070] Loss: 0.29028105512293123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1071] Loss: 0.2903029814082677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1072] Loss: 0.29032190534072205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1073] Loss: 0.2903182539087381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1074] Loss: 0.2903147887033144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1075] Loss: 0.29031622356439\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1076] Loss: 0.2903297483282303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1077] Loss: 0.2903606390409762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1078] Loss: 0.29038914494631207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1079] Loss: 0.290381596839606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1080] Loss: 0.29037591605395713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1081] Loss: 0.290395252871664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1082] Loss: 0.29038350632442117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1083] Loss: 0.2903771655725807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1084] Loss: 0.2903913424089006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1085] Loss: 0.2903871641358458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1086] Loss: 0.2903987741929312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1087] Loss: 0.2903903838457137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1088] Loss: 0.2903810694277018\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1089] Loss: 0.2903849973584638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1090] Loss: 0.2903850046173372\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1091] Loss: 0.290383938530183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1092] Loss: 0.29038595886990654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1093] Loss: 0.2903867810575049\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1094] Loss: 0.29038645720045375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1095] Loss: 0.29039005363368753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1096] Loss: 0.2903773184465512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1097] Loss: 0.2903789321839647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1098] Loss: 0.29038810617316396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1099] Loss: 0.290385813741853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1100] Loss: 0.2903907988904109\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1101] Loss: 0.29039386687552643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1102] Loss: 0.29039213939079705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1103] Loss: 0.29039363707762644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1104] Loss: 0.290386324795224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1105] Loss: 0.29037989190587826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1106] Loss: 0.290375987051685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1107] Loss: 0.2903702806248354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1108] Loss: 0.29035339842185615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1109] Loss: 0.29036156926691525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1110] Loss: 0.2903444454311085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1111] Loss: 0.29035768124694916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1112] Loss: 0.2903503746719453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1113] Loss: 0.2903402084558203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1114] Loss: 0.2903403984512362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1115] Loss: 0.2903749588094604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1116] Loss: 0.2903671001757236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1117] Loss: 0.29035878601141324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1118] Loss: 0.2903467931651643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1119] Loss: 0.2903353444301619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1120] Loss: 0.2903358426355805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1121] Loss: 0.29033583406898905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1122] Loss: 0.2903290860177355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1123] Loss: 0.29032436383792254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1124] Loss: 0.2903150998663024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1125] Loss: 0.2903540278385003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1126] Loss: 0.29034402870335296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1127] Loss: 0.29032854160084415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1128] Loss: 0.29032152976760534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1129] Loss: 0.2903077745471847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1130] Loss: 0.29029402141948624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1131] Loss: 0.29032265578428423\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1132] Loss: 0.29031205961979145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1133] Loss: 0.29030479095818773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1134] Loss: 0.2902919021509247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1135] Loss: 0.29029767528390893\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1136] Loss: 0.29028077524135815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1137] Loss: 0.29028195402068074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1138] Loss: 0.29033128063473346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1139] Loss: 0.2903361692472577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1140] Loss: 0.29033870109649507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1141] Loss: 0.29032390968813065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1142] Loss: 0.29034421434976054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1143] Loss: 0.2903382232663246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1144] Loss: 0.29032708967676246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1145] Loss: 0.290319792172425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1146] Loss: 0.2903183964377467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1147] Loss: 0.2903172647416733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1148] Loss: 0.2903092729155006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1149] Loss: 0.29029749288438883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1150] Loss: 0.2903032114672555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1151] Loss: 0.29028961826681526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1152] Loss: 0.29029056924310026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1153] Loss: 0.29030594111823604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1154] Loss: 0.29030445126686827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1155] Loss: 0.29028856839961026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1156] Loss: 0.2902794364095112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1157] Loss: 0.290274331133069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1158] Loss: 0.29027154711212133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1159] Loss: 0.29025885634932386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1160] Loss: 0.2902658898055881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1161] Loss: 0.29026162795745786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1162] Loss: 0.29025028475312775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1163] Loss: 0.2902488903241917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1164] Loss: 0.2902362889366107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1165] Loss: 0.29022497432231653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1166] Loss: 0.2902082190324841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1167] Loss: 0.2902056936980618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1168] Loss: 0.2902098112189074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1169] Loss: 0.29019818340601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1170] Loss: 0.2902128937286073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1171] Loss: 0.2901990791961592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1172] Loss: 0.290208443912151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1173] Loss: 0.29021854325436935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1174] Loss: 0.29021998534462806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1175] Loss: 0.29021026597383054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1176] Loss: 0.29020942909202474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1177] Loss: 0.29021035915854926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1178] Loss: 0.290214787819973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1179] Loss: 0.29023263650514103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1180] Loss: 0.29022335812858135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1181] Loss: 0.2902105022201371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1182] Loss: 0.2902252778289585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1183] Loss: 0.2902094736449514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1184] Loss: 0.29020837986892395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1185] Loss: 0.29024509093690803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1186] Loss: 0.2902418386876739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1187] Loss: 0.2902564252617162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1188] Loss: 0.2902608012633207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1189] Loss: 0.29024712769866734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1190] Loss: 0.290252629248818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1191] Loss: 0.2902457773488842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1192] Loss: 0.29023602883336347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1193] Loss: 0.29022747956352624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1194] Loss: 0.2902303199171995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1195] Loss: 0.2902309171302857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1196] Loss: 0.2902247937260858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1197] Loss: 0.29025386763796096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1198] Loss: 0.29025389445285815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1199] Loss: 0.290255340387608\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1200] Loss: 0.2902475070189481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1201] Loss: 0.2902424591724854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1202] Loss: 0.29024121296031\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1203] Loss: 0.29022998454303844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1204] Loss: 0.2902277898826477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1205] Loss: 0.29023919106251317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1206] Loss: 0.290239980281511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1207] Loss: 0.29023352614488274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1208] Loss: 0.2902458225632355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1209] Loss: 0.29023825079208926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1210] Loss: 0.2902309355921793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1211] Loss: 0.2902242100972633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1212] Loss: 0.29024250030942716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1213] Loss: 0.2902637149276185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1214] Loss: 0.29026677039771953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1215] Loss: 0.290268546253518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1216] Loss: 0.2902758408089604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1217] Loss: 0.2902648661909602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1218] Loss: 0.29027920786490663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1219] Loss: 0.29027245627663384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1220] Loss: 0.29027016829194846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1221] Loss: 0.2902625202665832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1222] Loss: 0.29027490454998145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1223] Loss: 0.2902754670940731\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1224] Loss: 0.2902685079726102\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1225] Loss: 0.29025301973219175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1226] Loss: 0.2902417452320101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1227] Loss: 0.2902363956990994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1228] Loss: 0.29022271852363585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1229] Loss: 0.290214590250216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1230] Loss: 0.29021532599759453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1231] Loss: 0.29022115293779743\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1232] Loss: 0.29021599866610087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1233] Loss: 0.2902146203281133\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1234] Loss: 0.2902105977274953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1235] Loss: 0.29022149410550957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1236] Loss: 0.29023761048508745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1237] Loss: 0.29023404565937094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1238] Loss: 0.290232338869383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1239] Loss: 0.29023111525424705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1240] Loss: 0.29022483880953015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1241] Loss: 0.2902173129872179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1242] Loss: 0.29023064015172995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1243] Loss: 0.2902277792231453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1244] Loss: 0.29022283335769006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1245] Loss: 0.2902194683507641\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1246] Loss: 0.2902238727308067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1247] Loss: 0.2902195815922575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1248] Loss: 0.2902174731961886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1249] Loss: 0.29021439658468057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1250] Loss: 0.29021617542771033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1251] Loss: 0.2901986541554473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1252] Loss: 0.29021894164518397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1253] Loss: 0.29024524966528686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1254] Loss: 0.29025351723917864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1255] Loss: 0.29025868868084326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1256] Loss: 0.2902518944337192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1257] Loss: 0.29024847501900897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1258] Loss: 0.2902364665227002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1259] Loss: 0.29024706949452633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1260] Loss: 0.29025134404298397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1261] Loss: 0.2902383648343687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1262] Loss: 0.290234284010565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1263] Loss: 0.2902471671263495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1264] Loss: 0.29024947847492366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1265] Loss: 0.29025293967347543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1266] Loss: 0.2902481843790429\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1267] Loss: 0.29023385904538995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1268] Loss: 0.29021897316222406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1269] Loss: 0.2902178847900971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1270] Loss: 0.2902214304745272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1271] Loss: 0.2902598617603562\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1272] Loss: 0.29026518275565416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1273] Loss: 0.2902655473672509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1274] Loss: 0.29026735193087994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1275] Loss: 0.2902834839763353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1276] Loss: 0.29028351348571424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1277] Loss: 0.29028025543253433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1278] Loss: 0.290272874800363\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1279] Loss: 0.29026773904778136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1280] Loss: 0.29026023896512376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1281] Loss: 0.29026678509361253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1282] Loss: 0.2902795009728312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1283] Loss: 0.29027413953484316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1284] Loss: 0.2902880329370713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1285] Loss: 0.2903031731673311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1286] Loss: 0.29029587330810747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1287] Loss: 0.29030507069027794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1288] Loss: 0.29030388780935124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1289] Loss: 0.29030379030060105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1290] Loss: 0.29029770780659986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1291] Loss: 0.2902991848000888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1292] Loss: 0.29028459853646676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1293] Loss: 0.29027946784402286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1294] Loss: 0.2902996905614469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1295] Loss: 0.29029282084162195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1296] Loss: 0.2902993494761507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1297] Loss: 0.2902960821290761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1298] Loss: 0.2902910714364551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1299] Loss: 0.29028147068760163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1300] Loss: 0.2902907704789761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1301] Loss: 0.2902811854587926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1302] Loss: 0.2902679117483601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1303] Loss: 0.29027277783336003\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1304] Loss: 0.29027405490746266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1305] Loss: 0.2902822500632178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1306] Loss: 0.2902743524925269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1307] Loss: 0.29029046913903306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1308] Loss: 0.29028429230340763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1309] Loss: 0.29029356860391275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1310] Loss: 0.2902921158232678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1311] Loss: 0.29029418421274444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1312] Loss: 0.2902843617123572\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1313] Loss: 0.29029264204532995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1314] Loss: 0.29027869841873455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1315] Loss: 0.29027564492811747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1316] Loss: 0.2902650881618331\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1317] Loss: 0.290263032266107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1318] Loss: 0.29025901082535216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1319] Loss: 0.2902537052208816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1320] Loss: 0.29023917092350593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1321] Loss: 0.29024038305977556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1322] Loss: 0.29023551565562045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1323] Loss: 0.29023681603416207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1324] Loss: 0.29023915107741055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1325] Loss: 0.29023327581388797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1326] Loss: 0.29021991408271974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1327] Loss: 0.29022616728323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1328] Loss: 0.2902160248157009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1329] Loss: 0.2902185229207513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1330] Loss: 0.29025256759952756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1331] Loss: 0.2902562928425378\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8877999999999999\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1332] Loss: 0.2902537939144763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1333] Loss: 0.2902676508840542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1334] Loss: 0.29027339265710045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1335] Loss: 0.29027177592687486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1336] Loss: 0.290314932979796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1337] Loss: 0.29032425154157204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1338] Loss: 0.29032303313498226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1339] Loss: 0.2903139950205483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1340] Loss: 0.2903249102608176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1341] Loss: 0.2903142184687166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1342] Loss: 0.2903076778645148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1343] Loss: 0.29030396560299887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1344] Loss: 0.29029683443693316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1345] Loss: 0.29030620191483464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1346] Loss: 0.29031702054053116\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1347] Loss: 0.2903102977305038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1348] Loss: 0.29030746607059377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1349] Loss: 0.29032555759003925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1350] Loss: 0.29031782709279047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1351] Loss: 0.29031527475905977\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1352] Loss: 0.29030072582491157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1353] Loss: 0.29030550080251993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1354] Loss: 0.2902983035102204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1355] Loss: 0.2902948801101741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1356] Loss: 0.2903171249595777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1357] Loss: 0.2903098703075612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1358] Loss: 0.290310476372024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1359] Loss: 0.2903004156772296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1360] Loss: 0.2902999782258163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1361] Loss: 0.2902924562819496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1362] Loss: 0.290284104705008\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1363] Loss: 0.29028187579381926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1364] Loss: 0.2902744289441785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1365] Loss: 0.2902846728391577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1366] Loss: 0.29031709364243036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1367] Loss: 0.29031491517566305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1368] Loss: 0.29031126442439464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1369] Loss: 0.29030772599685833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1370] Loss: 0.2903118999139638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1371] Loss: 0.2903112570666468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1372] Loss: 0.2903174369617136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1373] Loss: 0.2903178827451217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1374] Loss: 0.2903098463705932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1375] Loss: 0.29030910511050745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1376] Loss: 0.2903221268571756\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1377] Loss: 0.2903138141885228\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1378] Loss: 0.2902966660214279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1379] Loss: 0.290290196764995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1380] Loss: 0.2903034203777793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1381] Loss: 0.2903009641174582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1382] Loss: 0.2903009625943053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1383] Loss: 0.2902865305213471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1384] Loss: 0.29030502612435377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1385] Loss: 0.2903149499138798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1386] Loss: 0.29030140344017663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1387] Loss: 0.29028863614652006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1388] Loss: 0.29027733137946327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1389] Loss: 0.29027379260025654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1390] Loss: 0.2902771134284722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1391] Loss: 0.2902811435874017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1392] Loss: 0.29028137088040623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1393] Loss: 0.2902707506400761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1394] Loss: 0.29025921827109297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1395] Loss: 0.29025393390632054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1396] Loss: 0.29025367129246443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1397] Loss: 0.2902429661385297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1398] Loss: 0.29024408244918987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1399] Loss: 0.2902393337709253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1400] Loss: 0.2902785628222318\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1401] Loss: 0.2902806671503717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1402] Loss: 0.2902637296701881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1403] Loss: 0.2902565319993631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1404] Loss: 0.2902457466068544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1405] Loss: 0.2902330336169056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1406] Loss: 0.29024051510645293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1407] Loss: 0.2902310619720972\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1408] Loss: 0.290227366300458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1409] Loss: 0.2902278014542681\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1410] Loss: 0.2902251786845335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1411] Loss: 0.29021654106314565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1412] Loss: 0.29022762267219027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1413] Loss: 0.29022603389977925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1414] Loss: 0.2902130761496089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1415] Loss: 0.2902069533471115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1416] Loss: 0.2902429889623725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1417] Loss: 0.29023110369748795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1418] Loss: 0.2902310680968454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1419] Loss: 0.29023120264083685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1420] Loss: 0.290233445569651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1421] Loss: 0.2902312136480352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1422] Loss: 0.29022123303826314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1423] Loss: 0.2902058418586026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1424] Loss: 0.2901938020291581\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1425] Loss: 0.2901898209435258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1426] Loss: 0.29018644104673524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1427] Loss: 0.29018169024267393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1428] Loss: 0.2901933160113966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1429] Loss: 0.290210334918651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1430] Loss: 0.290204014016146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1431] Loss: 0.2901912188310902\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1432] Loss: 0.29018264715662134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1433] Loss: 0.29017870068226304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1434] Loss: 0.29016834381513507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1435] Loss: 0.29016589266346143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1436] Loss: 0.29014898102232617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1437] Loss: 0.2901472821736317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1438] Loss: 0.290139685572609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1439] Loss: 0.29014654879383106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1440] Loss: 0.29014339010784146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1441] Loss: 0.2901339408801688\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1442] Loss: 0.2901226967764475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1443] Loss: 0.2901276796044376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1444] Loss: 0.2901255797621847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1445] Loss: 0.29013327307848463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1446] Loss: 0.29013064881333495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1447] Loss: 0.29013144379741307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1448] Loss: 0.29012986513558375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1449] Loss: 0.29012110934573804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1450] Loss: 0.29010980246871365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1451] Loss: 0.29010042546304843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1452] Loss: 0.2901212411805572\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1453] Loss: 0.290116523343716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1454] Loss: 0.2901164198586799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1455] Loss: 0.2901145522923199\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1456] Loss: 0.29011726522526043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1457] Loss: 0.29012926465705674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1458] Loss: 0.29015519223354097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1459] Loss: 0.29015628484612405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1460] Loss: 0.29015324275298787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1461] Loss: 0.2901517785594264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1462] Loss: 0.2901509719901708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1463] Loss: 0.2901380255865651\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1464] Loss: 0.2901386808823823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1465] Loss: 0.2901284690567211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1466] Loss: 0.2901183659921111\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1467] Loss: 0.2901168471565821\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1468] Loss: 0.29012401559963896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1469] Loss: 0.2901152314441617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1470] Loss: 0.29011746192261995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1471] Loss: 0.2901301726593938\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1472] Loss: 0.2901322105149302\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1473] Loss: 0.29012841518431776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1474] Loss: 0.29013053153896745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1475] Loss: 0.2901373608909722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1476] Loss: 0.29013258969436095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1477] Loss: 0.2901359076218452\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1478] Loss: 0.29012853864661353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1479] Loss: 0.29011374376625304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1480] Loss: 0.29010794847805066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1481] Loss: 0.2901117141553158\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1482] Loss: 0.2901161945298672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1483] Loss: 0.2901221863241276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1484] Loss: 0.2901385272406342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1485] Loss: 0.29014537546497127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1486] Loss: 0.29012593503198064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1487] Loss: 0.290120787560349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1488] Loss: 0.29011126719096525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1489] Loss: 0.2901035904193755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1490] Loss: 0.29010188287326233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1491] Loss: 0.2901033754598745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1492] Loss: 0.2901297744021709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1493] Loss: 0.29013301198410035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1494] Loss: 0.2901211047915032\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1495] Loss: 0.2901265077017312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1496] Loss: 0.29012129612612414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1497] Loss: 0.29012631682100837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1498] Loss: 0.2901247139033702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1499] Loss: 0.2901326970636786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1500] Loss: 0.29013116849385867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1501] Loss: 0.2901206884070069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1502] Loss: 0.2901149340775176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1503] Loss: 0.2901308532382849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1504] Loss: 0.29013915032825244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1505] Loss: 0.2901259675055926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1506] Loss: 0.29012248365657645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1507] Loss: 0.2901160971019801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1508] Loss: 0.29011468411545477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1509] Loss: 0.29010670413206935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1510] Loss: 0.290099843821759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1511] Loss: 0.2900995187841826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1512] Loss: 0.29008689366080426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1513] Loss: 0.2900764482588293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1514] Loss: 0.29007541396742587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1515] Loss: 0.29007042395742055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1516] Loss: 0.29008666675549444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1517] Loss: 0.2900974010440224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1518] Loss: 0.2901078296280006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1519] Loss: 0.2901346158070921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1520] Loss: 0.29012255956181204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1521] Loss: 0.2901201679573513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1522] Loss: 0.29010491919174564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1523] Loss: 0.29011026292979436\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 17, Batch 1524] Loss: 0.2901038778907616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 0] Loss: 0.2900975114967966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1] Loss: 0.2900977423665059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 2] Loss: 0.2901031184030442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 3] Loss: 0.2900918687938554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 4] Loss: 0.29008644698727465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 5] Loss: 0.2900921602928858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 6] Loss: 0.2900884503063736\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 7] Loss: 0.2900958353248415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 8] Loss: 0.2900986652511447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 9] Loss: 0.2900865753716995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 10] Loss: 0.29008886297826686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 11] Loss: 0.29009413298884174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 12] Loss: 0.2900942322423698\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 13] Loss: 0.2900862467843248\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 14] Loss: 0.29009640070519205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 15] Loss: 0.290083097417234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 16] Loss: 0.2900835081124035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 17] Loss: 0.29009316988439443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 18] Loss: 0.2900916242935871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 19] Loss: 0.29009532476240835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 20] Loss: 0.29009178843237904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 21] Loss: 0.29008424477761496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 22] Loss: 0.29008054123209387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 23] Loss: 0.29007686574861496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 24] Loss: 0.2900868536083878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 25] Loss: 0.2900904097970813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 26] Loss: 0.29009042466061613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 27] Loss: 0.29009819642346973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 28] Loss: 0.29008842560612835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 29] Loss: 0.2900854451389842\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 30] Loss: 0.29009410716303774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 31] Loss: 0.29009864816334574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 32] Loss: 0.29012040591254795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 33] Loss: 0.2901327186917577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 34] Loss: 0.2901434721987975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 35] Loss: 0.29013218174757816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 36] Loss: 0.2901299437163053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 37] Loss: 0.2901290381650707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 38] Loss: 0.29011588431069507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 39] Loss: 0.2901056752547327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 40] Loss: 0.2900960034824387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 41] Loss: 0.29009080933143294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 42] Loss: 0.29013455693825474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 43] Loss: 0.2901524165114058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 44] Loss: 0.2901412649481176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 45] Loss: 0.2901415558291441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 46] Loss: 0.29013222552298495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 47] Loss: 0.2901254461311672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 48] Loss: 0.29012183561850524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 49] Loss: 0.2901120587178755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 50] Loss: 0.2901061011792422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 51] Loss: 0.2901126538463717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 52] Loss: 0.29011457945719255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 53] Loss: 0.2901242390207479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 54] Loss: 0.290120188287946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 55] Loss: 0.2901159988754258\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 56] Loss: 0.29011446788710005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 57] Loss: 0.29012012096785356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 58] Loss: 0.2901150083071595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 59] Loss: 0.29011335864788373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 60] Loss: 0.2901213754799866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 61] Loss: 0.29013684161606806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 62] Loss: 0.29012582880955656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 63] Loss: 0.2901332857615141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 64] Loss: 0.29013574615458804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 65] Loss: 0.2901317051318619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 66] Loss: 0.2901380913833884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 67] Loss: 0.29012748175453323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 68] Loss: 0.29013374331929376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 69] Loss: 0.29012775764298193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 70] Loss: 0.29013748693341046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 71] Loss: 0.29013726146356883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 72] Loss: 0.29013164910462247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 73] Loss: 0.2901279362675287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 74] Loss: 0.2901533361436593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 75] Loss: 0.2901516843556799\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 76] Loss: 0.29014943796109427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 77] Loss: 0.2901384736046394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 78] Loss: 0.2901361709030962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 79] Loss: 0.29012133370637555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 80] Loss: 0.2901103189074702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 81] Loss: 0.2901042433050162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 82] Loss: 0.29011101129881195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 83] Loss: 0.2901201588199535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 84] Loss: 0.2901283683242724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 85] Loss: 0.29013095183467874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 86] Loss: 0.2901207141441089\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 87] Loss: 0.290117796569604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 88] Loss: 0.2901031536429327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 89] Loss: 0.29009128422866043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 90] Loss: 0.29008294161792736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 91] Loss: 0.2900771459071237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 92] Loss: 0.29007364200738744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 93] Loss: 0.29006957399936556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 94] Loss: 0.2900602973825409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 95] Loss: 0.2900981717730097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 96] Loss: 0.29009105520737094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 97] Loss: 0.2900821836531924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 98] Loss: 0.2900739671012577\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 99] Loss: 0.2900684248081514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 100] Loss: 0.29008383864386145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 101] Loss: 0.29009108358591784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 102] Loss: 0.2900843483542726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 103] Loss: 0.29007771216965406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 104] Loss: 0.2900732314965741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 105] Loss: 0.290062217053662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 106] Loss: 0.2900523546110706\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 107] Loss: 0.2900574272124506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 108] Loss: 0.2900598544237125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 109] Loss: 0.29005282838896684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 110] Loss: 0.2900569909489504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 111] Loss: 0.29004613370431037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 112] Loss: 0.2900500453341435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 113] Loss: 0.290038160035713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 114] Loss: 0.29003679304703667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 115] Loss: 0.2900319128287434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 116] Loss: 0.29002053457488625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 117] Loss: 0.29001534013973235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 118] Loss: 0.290041521464194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 119] Loss: 0.29003543032592516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 120] Loss: 0.2900346149277915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 121] Loss: 0.29004761958891023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 122] Loss: 0.2900471213403229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 123] Loss: 0.2900442193702461\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 124] Loss: 0.29002998117818657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 125] Loss: 0.29002492018437087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 126] Loss: 0.2900172777125616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 127] Loss: 0.29003197134582326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 128] Loss: 0.2900266271844237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 129] Loss: 0.2900226612465625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 130] Loss: 0.290010843922896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 131] Loss: 0.2900173822126203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 132] Loss: 0.2900203923957935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 133] Loss: 0.29002114291357856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 134] Loss: 0.29001934607521934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 135] Loss: 0.29001849250231976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 136] Loss: 0.29002778237807464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 137] Loss: 0.29002756608285774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 138] Loss: 0.29004060833296624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 139] Loss: 0.2900389471908786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 140] Loss: 0.2900250230925282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 141] Loss: 0.29003854593516953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 142] Loss: 0.2900393129332971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 143] Loss: 0.290029307341236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 144] Loss: 0.2900158657893673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 145] Loss: 0.29001491183333544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 146] Loss: 0.29000953161184456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 147] Loss: 0.2900151458142222\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 148] Loss: 0.29000894845595604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 149] Loss: 0.29000248572302045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 150] Loss: 0.29000334458349447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 151] Loss: 0.2899919751676718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 152] Loss: 0.2899874071163297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 153] Loss: 0.2899876987419185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 154] Loss: 0.28998151427731117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 155] Loss: 0.28998208211410625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 156] Loss: 0.28997772137740246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 157] Loss: 0.289975637464164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 158] Loss: 0.28997000821774943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 159] Loss: 0.2899834598389421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 160] Loss: 0.2899840221288401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 161] Loss: 0.2899887379118374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 162] Loss: 0.2899736913310422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 163] Loss: 0.28996594911065904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 164] Loss: 0.28995863820545753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 165] Loss: 0.28995578625483565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 166] Loss: 0.28995673165048375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 167] Loss: 0.28998120241542163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 168] Loss: 0.2899651906439438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 169] Loss: 0.28996167070796064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 170] Loss: 0.2899675965565229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 171] Loss: 0.28996804448306446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 172] Loss: 0.2899622808013172\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 173] Loss: 0.28997172196733045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 174] Loss: 0.28996199373074816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 175] Loss: 0.2899478782034609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 176] Loss: 0.28993986091291873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 177] Loss: 0.28994425596212237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 178] Loss: 0.2899477581244336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 179] Loss: 0.28995120101681343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 180] Loss: 0.28994221066676007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 181] Loss: 0.2899408412885954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 182] Loss: 0.2899700687725512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 183] Loss: 0.28997246784275876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 184] Loss: 0.289963831261531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 185] Loss: 0.28996529903876717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 186] Loss: 0.28997451395028656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 187] Loss: 0.2900156180823062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 188] Loss: 0.2900045587424242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 189] Loss: 0.2900078859121854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 190] Loss: 0.28999949345574966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 191] Loss: 0.28999141273797807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 192] Loss: 0.2899950695502067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 193] Loss: 0.28998568498378835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 194] Loss: 0.28999976209609973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 195] Loss: 0.28999222502070293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 196] Loss: 0.28998896509400535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 197] Loss: 0.28999399193673797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 198] Loss: 0.2899970887385908\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 199] Loss: 0.2899936831220972\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 200] Loss: 0.29000558151881173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 201] Loss: 0.2899956057598275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 202] Loss: 0.28999750716378103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 203] Loss: 0.2900050636326825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 204] Loss: 0.2900141603302495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 205] Loss: 0.2900229381682772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 206] Loss: 0.2900333432235114\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 207] Loss: 0.29002826970471307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 208] Loss: 0.29003141014370803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 209] Loss: 0.2900271123234316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 210] Loss: 0.29002327945763573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 211] Loss: 0.29002279234026423\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 212] Loss: 0.2900164782255708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 213] Loss: 0.29000124974635044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 214] Loss: 0.28999689462479783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 215] Loss: 0.2899983637302057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 216] Loss: 0.29000577570447444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 217] Loss: 0.29001053380880465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 218] Loss: 0.28999744719144205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 219] Loss: 0.28998856721540445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 220] Loss: 0.29000422986696145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 221] Loss: 0.29003434472197676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 222] Loss: 0.29003852957647674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 223] Loss: 0.290032964009287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 224] Loss: 0.29002862567072596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 225] Loss: 0.29004241876190173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 226] Loss: 0.29003507079880947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 227] Loss: 0.29003006621812577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 228] Loss: 0.2900451522647068\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 229] Loss: 0.2900615002769864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 230] Loss: 0.2900484039255191\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 231] Loss: 0.2900519057476955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 232] Loss: 0.29004050335937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 233] Loss: 0.2900327799492853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 234] Loss: 0.2900264360872635\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 235] Loss: 0.2900216857437419\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 236] Loss: 0.2900097449475321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 237] Loss: 0.2900162604244898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 238] Loss: 0.2900122751233963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 239] Loss: 0.29001694246293674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 240] Loss: 0.290041743484028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 241] Loss: 0.2900289342705437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 242] Loss: 0.2900366772715351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 243] Loss: 0.29002302650031\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 244] Loss: 0.29003460352517263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 245] Loss: 0.29003897120313676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 246] Loss: 0.29002894545146896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 247] Loss: 0.29003759870373624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 248] Loss: 0.2900412155390576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 249] Loss: 0.2900381760823812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 250] Loss: 0.2900336671280856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 251] Loss: 0.2900272701025397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 252] Loss: 0.2900427204857627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 253] Loss: 0.29003103173686223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 254] Loss: 0.2900276521692262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 255] Loss: 0.2900206065827225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 256] Loss: 0.29000956920197657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 257] Loss: 0.2899971014981293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 258] Loss: 0.28999106847464745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 259] Loss: 0.2900015439105915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 260] Loss: 0.2900256476769936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 261] Loss: 0.29002894251353145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 262] Loss: 0.29002616576957474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 263] Loss: 0.2900230202747326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 264] Loss: 0.2900167146204433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 265] Loss: 0.2900107897534058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 266] Loss: 0.2899959417595597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 267] Loss: 0.2899895953736414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 268] Loss: 0.2899924331626908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 269] Loss: 0.28997989352380044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 270] Loss: 0.29002876571602626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 271] Loss: 0.2900239507009235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 272] Loss: 0.2900362260426913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 273] Loss: 0.29003514178165374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 274] Loss: 0.29003498261339283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 275] Loss: 0.2900426356846343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 276] Loss: 0.2900698272846373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 277] Loss: 0.29005960816866344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 278] Loss: 0.2900558461818256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 279] Loss: 0.2900495835354007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 280] Loss: 0.29005102720522213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 281] Loss: 0.29004508491340364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 282] Loss: 0.2900385188324416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 283] Loss: 0.2900350820959823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 284] Loss: 0.29003377647188167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 285] Loss: 0.29004331718001947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 286] Loss: 0.29003932139140876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 287] Loss: 0.2900401305913462\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 288] Loss: 0.29007561238731633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 289] Loss: 0.2900799191322623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 290] Loss: 0.2900781119563816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 291] Loss: 0.2900691104585303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 292] Loss: 0.29005546775930896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 293] Loss: 0.2900434984016536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 294] Loss: 0.29004214071283224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 295] Loss: 0.29004474978974526\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 296] Loss: 0.2900502422453047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 297] Loss: 0.29003762395911126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 298] Loss: 0.2900329047943277\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 299] Loss: 0.2900246716001133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 300] Loss: 0.29002012299383434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 301] Loss: 0.29000244328719965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 302] Loss: 0.28999551751850516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 303] Loss: 0.2899840642058896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 304] Loss: 0.2900202962796334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 305] Loss: 0.29000644753545524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 306] Loss: 0.2900102088225931\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8912\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 307] Loss: 0.29000503400323097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 308] Loss: 0.29001730922253516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 309] Loss: 0.29000463642707897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 310] Loss: 0.2899898974851268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 311] Loss: 0.2900018565348986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 312] Loss: 0.29000405449710026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 313] Loss: 0.28998908431063425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 314] Loss: 0.289980644509238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 315] Loss: 0.289968648413829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 316] Loss: 0.2899861966022536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 317] Loss: 0.2899766064590186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 318] Loss: 0.289963997771576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 319] Loss: 0.28997148487455543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 320] Loss: 0.2899699729395851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 321] Loss: 0.2899574123875983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 322] Loss: 0.28995489581807493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 323] Loss: 0.2899455753790269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 324] Loss: 0.2899549800695198\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 325] Loss: 0.28995392403264825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 326] Loss: 0.2899398066671133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 327] Loss: 0.28993805422647095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 328] Loss: 0.28992909657190535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 329] Loss: 0.28991589071249924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 330] Loss: 0.28992102075925846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 331] Loss: 0.2899119503342261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 332] Loss: 0.2899237763705515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 333] Loss: 0.28993204291903985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 334] Loss: 0.2899388279817878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 335] Loss: 0.28994532596150446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 336] Loss: 0.2899381408674354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 337] Loss: 0.28993777929993386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 338] Loss: 0.28994061168930313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 339] Loss: 0.2899523939246626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 340] Loss: 0.28995731790101664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 341] Loss: 0.28996420538007744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 342] Loss: 0.28995099725094603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 343] Loss: 0.289936004144143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 344] Loss: 0.2899368569698737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 345] Loss: 0.2899258550966494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 346] Loss: 0.2899602650006858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 347] Loss: 0.28996593022210815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 348] Loss: 0.28995170644993445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 349] Loss: 0.2899449799802007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 350] Loss: 0.2899442364992897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 351] Loss: 0.2899662735608445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 352] Loss: 0.28996951526919096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 353] Loss: 0.2899683625112232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 354] Loss: 0.2899608519336471\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 355] Loss: 0.2899625916633592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 356] Loss: 0.28995424877481485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 357] Loss: 0.289961420582789\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 358] Loss: 0.28996061855115607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 359] Loss: 0.2899507991278178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 360] Loss: 0.28995390206260874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 361] Loss: 0.28996750143365224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 362] Loss: 0.28996107542730537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 363] Loss: 0.28995714410706386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 364] Loss: 0.2899436817461233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 365] Loss: 0.28993616484574214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 366] Loss: 0.2899249548690233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 367] Loss: 0.2899126868215792\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 368] Loss: 0.28991214408023497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 369] Loss: 0.2899036149579447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 370] Loss: 0.2898962724623313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 371] Loss: 0.2899087739679396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 372] Loss: 0.289910701155755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 373] Loss: 0.2899135738480208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 374] Loss: 0.2899120876458728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 375] Loss: 0.28990961223360606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 376] Loss: 0.28990094274165107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 377] Loss: 0.2899058546217273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 378] Loss: 0.2899032421024903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 379] Loss: 0.28990845034482304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 380] Loss: 0.2899062856240846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 381] Loss: 0.289913965625814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 382] Loss: 0.2899018631378849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 383] Loss: 0.2898902794911019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 384] Loss: 0.2898921324481375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 385] Loss: 0.2898840708610719\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 386] Loss: 0.2898716837858848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 387] Loss: 0.2898633515776421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 388] Loss: 0.28985191435054053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 389] Loss: 0.2898443482436704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 390] Loss: 0.2898375786171932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 391] Loss: 0.28983613055648694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 392] Loss: 0.2898275704100365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 393] Loss: 0.28982231655866286\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 394] Loss: 0.28981760176518173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 395] Loss: 0.2898244040541061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 396] Loss: 0.2898112695852636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 397] Loss: 0.2898080452091476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 398] Loss: 0.28981151358997065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 399] Loss: 0.2898010715386125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 400] Loss: 0.2898026406708787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 401] Loss: 0.28979488072438336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 402] Loss: 0.2897920780578232\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 403] Loss: 0.28978018398933525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 404] Loss: 0.2898055698968904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 405] Loss: 0.2897938836002279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 406] Loss: 0.2898132643592106\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 407] Loss: 0.28980100453118685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 408] Loss: 0.2898059427258747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 409] Loss: 0.28980843860228045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 410] Loss: 0.2897967508760935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 411] Loss: 0.289801023825152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 412] Loss: 0.2898047989268458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 413] Loss: 0.28979323944224633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 414] Loss: 0.28977823563204314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 415] Loss: 0.28977740421081755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 416] Loss: 0.28976819294605777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 417] Loss: 0.28975653010510594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 418] Loss: 0.28975794717260717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 419] Loss: 0.2897548107951596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 420] Loss: 0.28974519110138197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 421] Loss: 0.28973438668631124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 422] Loss: 0.28972779973819374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 423] Loss: 0.28972222216847743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 424] Loss: 0.28971836773296505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 425] Loss: 0.28970935101135264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 426] Loss: 0.2897419664814269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 427] Loss: 0.2897425869677816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 428] Loss: 0.28972826337223856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 429] Loss: 0.28973006966986536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 430] Loss: 0.28971842210570187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 431] Loss: 0.28970920826762364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 432] Loss: 0.2897027897699184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 433] Loss: 0.28971347736568204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 434] Loss: 0.2897038095511165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 435] Loss: 0.289704306717145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 436] Loss: 0.2896993531257931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 437] Loss: 0.28969202785616466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 438] Loss: 0.28968262037211157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 439] Loss: 0.289675560878501\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 440] Loss: 0.28967026493644404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 441] Loss: 0.2896815164518242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 442] Loss: 0.28966950323815027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 443] Loss: 0.2896535487267498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 444] Loss: 0.2896559858552163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 445] Loss: 0.2896585511276498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 446] Loss: 0.28968241668842126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 447] Loss: 0.28967120821000303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 448] Loss: 0.2897000875187587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 449] Loss: 0.2896939347041738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 450] Loss: 0.2896899179039547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 451] Loss: 0.28968491877847347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 452] Loss: 0.28967576040625287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 453] Loss: 0.2896676505786225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 454] Loss: 0.2896577216259542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 455] Loss: 0.28968771668497045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 456] Loss: 0.28967531019373144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 457] Loss: 0.2896724746495113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 458] Loss: 0.28967517360192246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 459] Loss: 0.2896719566234762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 460] Loss: 0.2896667088172573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 461] Loss: 0.2896872982568215\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 462] Loss: 0.28967638284425945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 463] Loss: 0.2896764405618551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 464] Loss: 0.28968245538237186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 465] Loss: 0.28966869437304954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 466] Loss: 0.2896625852700296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 467] Loss: 0.2896551093567046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 468] Loss: 0.2896548433488653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 469] Loss: 0.2896509877281335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 470] Loss: 0.2896455839367497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 471] Loss: 0.2896373451039014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 472] Loss: 0.2896312356405145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 473] Loss: 0.2896316156351782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 474] Loss: 0.2896271577300909\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 475] Loss: 0.2896165913713312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 476] Loss: 0.28961940258053415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 477] Loss: 0.28964106623827707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 478] Loss: 0.2896287319782684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 479] Loss: 0.28965657292128183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 480] Loss: 0.2896509920601426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 481] Loss: 0.28963734409879127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 482] Loss: 0.2896439369391467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 483] Loss: 0.2896420297344098\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 484] Loss: 0.2896298186969438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 485] Loss: 0.2896414856479301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 486] Loss: 0.28964032932314554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 487] Loss: 0.2896498467431278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 488] Loss: 0.28964629199531783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 489] Loss: 0.2896595804222828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 490] Loss: 0.2896529407310566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 491] Loss: 0.28966397435634733\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 492] Loss: 0.2896534212895546\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 493] Loss: 0.28964733817699173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 494] Loss: 0.2896587260323468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 495] Loss: 0.2896569780262673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 496] Loss: 0.2896612835815378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 497] Loss: 0.28965765496717666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 498] Loss: 0.28964556882566317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 499] Loss: 0.2896337978860558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 500] Loss: 0.28962297629874584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 501] Loss: 0.2896360209004551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 502] Loss: 0.2896322390564696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 503] Loss: 0.28962856349514016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 504] Loss: 0.2896223998427537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 505] Loss: 0.2896220497848598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 506] Loss: 0.28962179739226196\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 507] Loss: 0.2896116541338129\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 508] Loss: 0.28960846385173017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 509] Loss: 0.2896055639654009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 510] Loss: 0.2896307141084293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 511] Loss: 0.2896294993128966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 512] Loss: 0.28963876856845555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 513] Loss: 0.2896314696416137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 514] Loss: 0.28964689681543176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 515] Loss: 0.2896502920500436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 516] Loss: 0.28963619247887784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 517] Loss: 0.2896330880298343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 518] Loss: 0.28962894604442246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 519] Loss: 0.289636538208846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 520] Loss: 0.2896222035677073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 521] Loss: 0.28961823964429967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 522] Loss: 0.2896096323962982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 523] Loss: 0.2896047246518361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 524] Loss: 0.2896015428115605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 525] Loss: 0.28958544265976727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 526] Loss: 0.2895750781763824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 527] Loss: 0.2895737522347738\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 528] Loss: 0.28956866169505446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 529] Loss: 0.2895574001835455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 530] Loss: 0.28954782845726884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 531] Loss: 0.289544457098422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 532] Loss: 0.2895317599396891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 533] Loss: 0.2895228666467633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 534] Loss: 0.2895405343331684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 535] Loss: 0.2895381030461986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 536] Loss: 0.2895248198224677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 537] Loss: 0.28951807190470125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 538] Loss: 0.2895178737403072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 539] Loss: 0.28952165838294125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 540] Loss: 0.28952506471982553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 541] Loss: 0.28955640451749787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 542] Loss: 0.28955777110341085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 543] Loss: 0.28955926359065953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 544] Loss: 0.28955874310249163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 545] Loss: 0.28955690071741164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 546] Loss: 0.28955317731163405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 547] Loss: 0.28954381020361064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 548] Loss: 0.28954846737347234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 549] Loss: 0.28954426649863185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 550] Loss: 0.2895475594543882\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 551] Loss: 0.2895510523167474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 552] Loss: 0.28955261795898085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 553] Loss: 0.28953855708188864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 554] Loss: 0.2895566151951568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 555] Loss: 0.2895564906056605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 556] Loss: 0.28956365468902695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 557] Loss: 0.28955302166126184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 558] Loss: 0.28959602826754505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 559] Loss: 0.2895842279540192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 560] Loss: 0.28957761781109825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 561] Loss: 0.2895778978480609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 562] Loss: 0.2895807805110843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 563] Loss: 0.28957953271085957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 564] Loss: 0.289570996357219\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 565] Loss: 0.28956540285530835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 566] Loss: 0.2895597679075328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 567] Loss: 0.2895934937504009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 568] Loss: 0.28959794454454607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 569] Loss: 0.28960098751357444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 570] Loss: 0.2895994397524498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 571] Loss: 0.2895945635844987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 572] Loss: 0.2895929813883972\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 573] Loss: 0.28958634778193165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 574] Loss: 0.2895860372827289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 575] Loss: 0.2895813199321717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 576] Loss: 0.28956506908652824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 577] Loss: 0.28956180875417986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 578] Loss: 0.28956170985792357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 579] Loss: 0.2895595996866782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 580] Loss: 0.2896445782925868\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 581] Loss: 0.2896440432846672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 582] Loss: 0.28963335798618156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 583] Loss: 0.28963500400166975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 584] Loss: 0.2896360627314543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 585] Loss: 0.2896309996268629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 586] Loss: 0.2896304634143145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 587] Loss: 0.28962830357670905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 588] Loss: 0.2896124429354983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 589] Loss: 0.2896246753947154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 590] Loss: 0.2896229409800291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 591] Loss: 0.2896324610872255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 592] Loss: 0.28962726404949707\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 593] Loss: 0.289630653790216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 594] Loss: 0.28961864756477784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 595] Loss: 0.2896050735944016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 596] Loss: 0.28961300844060567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 597] Loss: 0.28960807427846647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 598] Loss: 0.2896032830666645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 599] Loss: 0.28960059978456887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 600] Loss: 0.2896050711073951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 601] Loss: 0.28960151224018366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 602] Loss: 0.28959279649837033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 603] Loss: 0.289589577652415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 604] Loss: 0.28961645788074714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 605] Loss: 0.2896037628382328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 606] Loss: 0.28960371885118974\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 607] Loss: 0.28961009886380595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 608] Loss: 0.28959805242927344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 609] Loss: 0.2895984678086672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 610] Loss: 0.28959858863062105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 611] Loss: 0.2895937369024844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 612] Loss: 0.2895801138882406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 613] Loss: 0.28956501706498095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 614] Loss: 0.289554420688193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 615] Loss: 0.28955870496136754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 616] Loss: 0.28955099900137454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 617] Loss: 0.28957898049000935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 618] Loss: 0.289572193834564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 619] Loss: 0.2895930114091582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 620] Loss: 0.28958961742345446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 621] Loss: 0.28958423609366307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 622] Loss: 0.2895883657962486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 623] Loss: 0.28962779658188137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 624] Loss: 0.28962302200710666\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 625] Loss: 0.28962127712065594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 626] Loss: 0.2896094568551433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 627] Loss: 0.2896009149711746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 628] Loss: 0.28959910942361095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 629] Loss: 0.28961029479800676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 630] Loss: 0.28960372051038935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 631] Loss: 0.28960685979132855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 632] Loss: 0.28961857370094163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 633] Loss: 0.2896152442480322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 634] Loss: 0.2896212182093465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 635] Loss: 0.2896209675412181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 636] Loss: 0.28963934794630425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 637] Loss: 0.28963154197834257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 638] Loss: 0.28965162310531867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 639] Loss: 0.289647228411781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 640] Loss: 0.28963573639889517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 641] Loss: 0.2896283567204245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 642] Loss: 0.28962552935132274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 643] Loss: 0.2896304057635661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 644] Loss: 0.28963243900256763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 645] Loss: 0.2896187388262714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 646] Loss: 0.2896055380028525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 647] Loss: 0.28960924570153823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 648] Loss: 0.2896530869859458\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 649] Loss: 0.28965063225559146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 650] Loss: 0.28965809867714065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 651] Loss: 0.28964086999661703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 652] Loss: 0.2896332438094793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 653] Loss: 0.28963511440628076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 654] Loss: 0.28962563367090677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 655] Loss: 0.28962396151795694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 656] Loss: 0.2896228347913749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 657] Loss: 0.28961609469490734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 658] Loss: 0.2896215232470444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 659] Loss: 0.289621344322499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 660] Loss: 0.28963114365599163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 661] Loss: 0.2896352677713841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 662] Loss: 0.2896297262213557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 663] Loss: 0.28962782955558836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 664] Loss: 0.28962530449000334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 665] Loss: 0.28962801200877963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 666] Loss: 0.28962303663738437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 667] Loss: 0.2896092908016386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 668] Loss: 0.2896033285583478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 669] Loss: 0.28959095764260007\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 670] Loss: 0.2895881187609439\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 671] Loss: 0.28960544309500225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 672] Loss: 0.28960391484491055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 673] Loss: 0.28962073070372035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 674] Loss: 0.2896285596408505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 675] Loss: 0.2896197633619515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 676] Loss: 0.28961551380303574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 677] Loss: 0.28961598358769375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 678] Loss: 0.2896092305063928\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 679] Loss: 0.2896020094707379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 680] Loss: 0.28959266078327467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 681] Loss: 0.28959405329599214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 682] Loss: 0.28958568733726225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 683] Loss: 0.2895848865109208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 684] Loss: 0.2895829558036386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 685] Loss: 0.28956998331582184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 686] Loss: 0.2895606611157975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 687] Loss: 0.2895558541332657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 688] Loss: 0.2895592928437618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 689] Loss: 0.2895784935020824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 690] Loss: 0.28958589644783866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 691] Loss: 0.2895814880220491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 692] Loss: 0.2896007665832561\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 693] Loss: 0.28959312503879936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 694] Loss: 0.2895900032095237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 695] Loss: 0.28959372627546703\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 696] Loss: 0.28959870958859285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 697] Loss: 0.28958642101588594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 698] Loss: 0.28958996089086786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 699] Loss: 0.28958670155215865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 700] Loss: 0.28958486040585396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 701] Loss: 0.2895858656899846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 702] Loss: 0.28959477201769673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 703] Loss: 0.28958374890256927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 704] Loss: 0.2895722904266618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 705] Loss: 0.2895746902673859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 706] Loss: 0.2895676504366366\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 707] Loss: 0.28956424459033125\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 708] Loss: 0.28955672198167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 709] Loss: 0.2895495043186989\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 710] Loss: 0.2895628836027576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 711] Loss: 0.2895525391162361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 712] Loss: 0.28954367906615996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 713] Loss: 0.2895327379250553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 714] Loss: 0.289529716542187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 715] Loss: 0.2895176839106518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 716] Loss: 0.2895111641605309\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 717] Loss: 0.2894971255121275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 718] Loss: 0.2894939034256943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 719] Loss: 0.2894877895175723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 720] Loss: 0.2894817363097254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 721] Loss: 0.28947884916881955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 722] Loss: 0.28948902102339985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 723] Loss: 0.28949889399510387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 724] Loss: 0.2895009543809283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 725] Loss: 0.28949600004795767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 726] Loss: 0.28950078129017603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 727] Loss: 0.2894910716227847\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 728] Loss: 0.28949828040750414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 729] Loss: 0.28948502059096626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 730] Loss: 0.28948781958487557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 731] Loss: 0.2894925326693981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 732] Loss: 0.2894847097603394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 733] Loss: 0.2894732246419189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 734] Loss: 0.2894650865537803\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 735] Loss: 0.28946273965532016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 736] Loss: 0.28945771681782806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 737] Loss: 0.2894408459637742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 738] Loss: 0.2894619511578514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 739] Loss: 0.28945506388387476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 740] Loss: 0.2894533447834876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 741] Loss: 0.289447237654827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 742] Loss: 0.28944068246699517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 743] Loss: 0.2894336447660453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 744] Loss: 0.2894218364346547\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 745] Loss: 0.2894187638271237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 746] Loss: 0.28940619815556357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 747] Loss: 0.28940646033264894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 748] Loss: 0.28939626896904297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 749] Loss: 0.28939698833819105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 750] Loss: 0.2894099114089341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 751] Loss: 0.2894109181703523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 752] Loss: 0.2894088615431508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 753] Loss: 0.28940051776070147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 754] Loss: 0.2893968365145163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 755] Loss: 0.2894174697589575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 756] Loss: 0.28941802230701924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 757] Loss: 0.28942930572272185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 758] Loss: 0.2894249912275156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 759] Loss: 0.28941549952789714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 760] Loss: 0.28942205782236397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 761] Loss: 0.28941320820891836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 762] Loss: 0.2894080359342957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 763] Loss: 0.2894099098967331\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 764] Loss: 0.2894070724940809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 765] Loss: 0.2894105590141183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 766] Loss: 0.2894297125398993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 767] Loss: 0.28944375358701274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 768] Loss: 0.2894455690784931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 769] Loss: 0.2894603106357325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 770] Loss: 0.289468016090411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 771] Loss: 0.28947195780987717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 772] Loss: 0.28948181954677105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 773] Loss: 0.28946952022845357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 774] Loss: 0.28946412105866715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 775] Loss: 0.2894519079562762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 776] Loss: 0.2894517042078253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 777] Loss: 0.28944281388689874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 778] Loss: 0.2894264397316975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 779] Loss: 0.28943875546035797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 780] Loss: 0.2894283833195364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 781] Loss: 0.2894690522425102\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 782] Loss: 0.2894638474240361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 783] Loss: 0.28945566749489854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 784] Loss: 0.28945788993090515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 785] Loss: 0.2894729188171277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 786] Loss: 0.28947356708164707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 787] Loss: 0.28947630009115055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 788] Loss: 0.28950103666875254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 789] Loss: 0.2895129405255289\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 790] Loss: 0.28953184739822463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 791] Loss: 0.2895268512091958\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 792] Loss: 0.2895313388233928\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 793] Loss: 0.2895325273972362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 794] Loss: 0.2895449589017573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 795] Loss: 0.28956376554686764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 796] Loss: 0.2895730772305811\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 797] Loss: 0.28956351597911534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 798] Loss: 0.2895506150179878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 799] Loss: 0.2895662094319745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 800] Loss: 0.28957076807803445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 801] Loss: 0.2895817416879182\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 802] Loss: 0.2895783759152544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 803] Loss: 0.2895896397579755\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 804] Loss: 0.28958608524955387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 805] Loss: 0.2895865079551362\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 806] Loss: 0.2895766221578259\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8887\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 807] Loss: 0.2895609952106561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 808] Loss: 0.28957998452548117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 809] Loss: 0.2895860329487509\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 810] Loss: 0.28960342624212904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 811] Loss: 0.28959414868138805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 812] Loss: 0.28959856141581153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 813] Loss: 0.2896133220235062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 814] Loss: 0.28959835648510335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 815] Loss: 0.2895901815675136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 816] Loss: 0.2895876363789643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 817] Loss: 0.2895746958063262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 818] Loss: 0.28957399370446935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 819] Loss: 0.28958118443217157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 820] Loss: 0.28956945354388713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 821] Loss: 0.28956471850635335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 822] Loss: 0.2895686567270981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 823] Loss: 0.2895762179004517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 824] Loss: 0.28956666421418464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 825] Loss: 0.2895700165704351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 826] Loss: 0.28956980287665945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 827] Loss: 0.2895642293165414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 828] Loss: 0.2895833456807698\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 829] Loss: 0.28957024813630833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 830] Loss: 0.2895653467486664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 831] Loss: 0.28955919494111426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 832] Loss: 0.28955771370983285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 833] Loss: 0.2895523210710654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 834] Loss: 0.289556339970697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 835] Loss: 0.2895574239529355\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 836] Loss: 0.2895667107396875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 837] Loss: 0.28960290045418563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 838] Loss: 0.2895904947765418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 839] Loss: 0.2895817743205317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 840] Loss: 0.28957197614011293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 841] Loss: 0.2895875669768336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 842] Loss: 0.28960506250523044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 843] Loss: 0.2896024034528425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 844] Loss: 0.28959011302639376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 845] Loss: 0.2895816654843947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 846] Loss: 0.28957675224828966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 847] Loss: 0.2895634289890336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 848] Loss: 0.2895605566822216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 849] Loss: 0.2895534517013887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 850] Loss: 0.2895571082107931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 851] Loss: 0.2895504999471788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 852] Loss: 0.28955926702301366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 853] Loss: 0.2895504445037996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 854] Loss: 0.2895421928171797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 855] Loss: 0.28953781176429827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 856] Loss: 0.2895237007943705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 857] Loss: 0.2895148764908145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 858] Loss: 0.2895141274503531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 859] Loss: 0.28950799519422454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 860] Loss: 0.28950979840402763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 861] Loss: 0.2895052186587218\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 862] Loss: 0.2894915817796892\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 863] Loss: 0.28949469795370975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 864] Loss: 0.2894922659204445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 865] Loss: 0.2894868492466864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 866] Loss: 0.2894891151842781\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 867] Loss: 0.2895008027077273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 868] Loss: 0.28950450509703335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 869] Loss: 0.2895095067835012\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 870] Loss: 0.28950860674410683\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 871] Loss: 0.28949457764216124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 872] Loss: 0.28948583346100576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 873] Loss: 0.2894880181032495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 874] Loss: 0.28947727279822394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 875] Loss: 0.2894694137012881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 876] Loss: 0.2894614094195835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 877] Loss: 0.2894577119705202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 878] Loss: 0.2894460069274794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 879] Loss: 0.28945069592796563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 880] Loss: 0.2894547146812338\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 881] Loss: 0.2894497505831026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 882] Loss: 0.28944874584356917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 883] Loss: 0.2894599705670752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 884] Loss: 0.2894545825630807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 885] Loss: 0.2894430819172583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 886] Loss: 0.2894540053097633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 887] Loss: 0.2894454967814895\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 888] Loss: 0.28946361976157386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 889] Loss: 0.2894553812363053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 890] Loss: 0.2894483007900602\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 891] Loss: 0.28948255911914694\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 892] Loss: 0.2894917003528027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 893] Loss: 0.2894855532672798\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 894] Loss: 0.2895092170459832\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 895] Loss: 0.2895080605443536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 896] Loss: 0.28951116778868813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 897] Loss: 0.2895010185094759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 898] Loss: 0.2894944604627067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 899] Loss: 0.28950467535052904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 900] Loss: 0.28950599000745486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 901] Loss: 0.289505021677625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 902] Loss: 0.2895277719095667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 903] Loss: 0.28952457222835554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 904] Loss: 0.28951356518441723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 905] Loss: 0.2895283162073438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 906] Loss: 0.2895322873293853\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 907] Loss: 0.2895228253139251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 908] Loss: 0.28951202101120144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 909] Loss: 0.28950312210250334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 910] Loss: 0.28950664252469166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 911] Loss: 0.2895075789216878\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 912] Loss: 0.2895141826125284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 913] Loss: 0.2895178982310468\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 914] Loss: 0.2895215263287425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 915] Loss: 0.2895488549529665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 916] Loss: 0.28953446641657565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 917] Loss: 0.2895285200485033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 918] Loss: 0.28951388445812015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 919] Loss: 0.2895018367626004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 920] Loss: 0.2895162629779518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 921] Loss: 0.28950408758915397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 922] Loss: 0.28949996650592713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 923] Loss: 0.28949845381896705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 924] Loss: 0.28949286559346343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 925] Loss: 0.28949199385583263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 926] Loss: 0.28948488900824504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 927] Loss: 0.2894821519798925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 928] Loss: 0.28948103435718503\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 929] Loss: 0.2894928189146672\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 930] Loss: 0.28950274655052594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 931] Loss: 0.28949923306146164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 932] Loss: 0.28949377375292407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 933] Loss: 0.2894929224704117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 934] Loss: 0.2895101504813931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 935] Loss: 0.28952048389838336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 936] Loss: 0.2895345715354093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 937] Loss: 0.28952400170991716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 938] Loss: 0.2895156150136441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 939] Loss: 0.28950784247988604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 940] Loss: 0.2895018795281819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 941] Loss: 0.28949240970155327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 942] Loss: 0.28950490156293385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 943] Loss: 0.289496980551083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 944] Loss: 0.28948920024019487\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 945] Loss: 0.28949809176606356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 946] Loss: 0.2894975367728551\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 947] Loss: 0.2894992144666969\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 948] Loss: 0.2894972399246187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 949] Loss: 0.2894886707061164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 950] Loss: 0.28948035511245246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 951] Loss: 0.289480492428725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 952] Loss: 0.28948551555043517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 953] Loss: 0.2894915135283786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 954] Loss: 0.2894883419522537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 955] Loss: 0.28949261408483346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 956] Loss: 0.2894894727459358\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 957] Loss: 0.2894875828322327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 958] Loss: 0.2894808535346498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 959] Loss: 0.28947750234208075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 960] Loss: 0.2894724237812943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 961] Loss: 0.2894699292803366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 962] Loss: 0.28946081913926686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 963] Loss: 0.2894698198995709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 964] Loss: 0.28945605634076904\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 965] Loss: 0.2894521394288762\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 966] Loss: 0.28946835014834094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 967] Loss: 0.28948172908590486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 968] Loss: 0.2894778941713898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 969] Loss: 0.2894736993421165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 970] Loss: 0.28946550869263715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 971] Loss: 0.2894524810605117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 972] Loss: 0.2894408877362099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 973] Loss: 0.2894380838315445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 974] Loss: 0.2894227781500425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 975] Loss: 0.2894213926861729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 976] Loss: 0.2894222236791868\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 977] Loss: 0.2894190808615599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 978] Loss: 0.28944298946674424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 979] Loss: 0.2894533306619833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 980] Loss: 0.2894546649869411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 981] Loss: 0.28944796326009836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 982] Loss: 0.28945886943999044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 983] Loss: 0.28947290609196213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 984] Loss: 0.28947219487738424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 985] Loss: 0.2894718676508717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 986] Loss: 0.28948152884411377\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 987] Loss: 0.28947428430858313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 988] Loss: 0.2894668841765189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 989] Loss: 0.28946137680187234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 990] Loss: 0.28945670811248225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 991] Loss: 0.2894618467112923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 992] Loss: 0.28945451119665067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 993] Loss: 0.2894520652736064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 994] Loss: 0.2894618239001528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 995] Loss: 0.2894693722610925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 996] Loss: 0.2894635861860247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 997] Loss: 0.2894678914057076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 998] Loss: 0.2894663160081897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 999] Loss: 0.2894611534395303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1000] Loss: 0.2894621994308266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1001] Loss: 0.28946275537041427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1002] Loss: 0.2894725732758247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1003] Loss: 0.28946394177588675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1004] Loss: 0.2894638535201197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1005] Loss: 0.2894687583204359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1006] Loss: 0.2894751249670951\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1007] Loss: 0.2894849907900303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1008] Loss: 0.2894859341206893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1009] Loss: 0.2894892942420495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1010] Loss: 0.2894906346969354\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1011] Loss: 0.289487915013144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1012] Loss: 0.2894747825757604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1013] Loss: 0.28949959139829823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1014] Loss: 0.2894921514008004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1015] Loss: 0.2894980703479837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1016] Loss: 0.2894899228421517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1017] Loss: 0.2894890862930134\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1018] Loss: 0.2894918271303425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1019] Loss: 0.28948376504094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1020] Loss: 0.2894824487608556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1021] Loss: 0.28946983828079675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1022] Loss: 0.289464959180397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1023] Loss: 0.28946283896253433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1024] Loss: 0.28945328664716313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1025] Loss: 0.2894419099270463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1026] Loss: 0.28944172644499533\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1027] Loss: 0.28944886221297256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1028] Loss: 0.28945207414476837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1029] Loss: 0.28944411591744457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1030] Loss: 0.2894404385296631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1031] Loss: 0.28942922738199195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1032] Loss: 0.28942303677849457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1033] Loss: 0.2894361944731729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1034] Loss: 0.2894294620958669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1035] Loss: 0.2894275327408102\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1036] Loss: 0.28941808932059415\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1037] Loss: 0.2894077702268323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1038] Loss: 0.28941686237117586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1039] Loss: 0.2894240480943224\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1040] Loss: 0.2894167898596459\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1041] Loss: 0.28942334587973123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1042] Loss: 0.28943683104003504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1043] Loss: 0.2894304088456296\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1044] Loss: 0.2894162886273542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1045] Loss: 0.2894027935118721\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1046] Loss: 0.2894086351128771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1047] Loss: 0.28939422658491154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1048] Loss: 0.2893798488977024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1049] Loss: 0.2893876307411893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1050] Loss: 0.28937829156058514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1051] Loss: 0.2893671481711918\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1052] Loss: 0.2893517982095734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1053] Loss: 0.2893574552243274\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1054] Loss: 0.28936109476956084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1055] Loss: 0.2893517422448352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1056] Loss: 0.2893473900400277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1057] Loss: 0.2893554278439399\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1058] Loss: 0.2893450297949328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1059] Loss: 0.28934688445518497\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1060] Loss: 0.28935939999081556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1061] Loss: 0.289349632501528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1062] Loss: 0.2893518859173091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1063] Loss: 0.2893524027549923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1064] Loss: 0.2893708894564916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1065] Loss: 0.2893787298545376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1066] Loss: 0.28939294734692045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1067] Loss: 0.28938455540895275\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1068] Loss: 0.28939086008118414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1069] Loss: 0.2893806868763607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1070] Loss: 0.2893750550426877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1071] Loss: 0.2893793371785587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1072] Loss: 0.2893700250450067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1073] Loss: 0.2893769570997219\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1074] Loss: 0.2893724430699739\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1075] Loss: 0.289364330033465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1076] Loss: 0.28937877252334143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1077] Loss: 0.289390527946687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1078] Loss: 0.28938988861739795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1079] Loss: 0.28939816269013824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1080] Loss: 0.2893976496614642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1081] Loss: 0.28940952207127646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1082] Loss: 0.2893981913254344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1083] Loss: 0.2893911718147776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1084] Loss: 0.2894065071230342\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1085] Loss: 0.2894099897174838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1086] Loss: 0.2894076606846813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1087] Loss: 0.28940836541318754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1088] Loss: 0.2894143615890333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1089] Loss: 0.28940617235882776\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1090] Loss: 0.2894070687329787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1091] Loss: 0.28939701236860726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1092] Loss: 0.2894152605942157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1093] Loss: 0.28941463091545827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1094] Loss: 0.2894120358089887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1095] Loss: 0.2894181839036955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1096] Loss: 0.2894290386069266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1097] Loss: 0.28942840949587667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1098] Loss: 0.28944031079900523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1099] Loss: 0.2894363185785665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1100] Loss: 0.28942905695798615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1101] Loss: 0.2894261383261558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1102] Loss: 0.28941586761551663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1103] Loss: 0.289414888623234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1104] Loss: 0.28940338907769386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1105] Loss: 0.28941041603656414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1106] Loss: 0.289406167048759\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1107] Loss: 0.28940650643821436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1108] Loss: 0.28939613838893824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1109] Loss: 0.28939335421707774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1110] Loss: 0.28938714007978955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1111] Loss: 0.2894006319151984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1112] Loss: 0.2894037948721019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1113] Loss: 0.28940871812717844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1114] Loss: 0.289410658034322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1115] Loss: 0.2894017405933921\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1116] Loss: 0.2894200157755041\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1117] Loss: 0.28940950168600044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1118] Loss: 0.2894064067452777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1119] Loss: 0.28941424405858407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1120] Loss: 0.28940880199320057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1121] Loss: 0.2893977199092767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1122] Loss: 0.2893890720782534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1123] Loss: 0.2893802809124394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1124] Loss: 0.28938998325896187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1125] Loss: 0.289389077763915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1126] Loss: 0.28939002273505976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1127] Loss: 0.2893863025814465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1128] Loss: 0.2893772251347021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1129] Loss: 0.2893707737496055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1130] Loss: 0.28936412005526263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1131] Loss: 0.2893701253558657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1132] Loss: 0.28938134264142507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1133] Loss: 0.289371984742195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1134] Loss: 0.28936595382205643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1135] Loss: 0.2893733474848665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1136] Loss: 0.2893671330214597\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1137] Loss: 0.2893600492879856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1138] Loss: 0.28935066202337895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1139] Loss: 0.2893406960619266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1140] Loss: 0.2893369116514843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1141] Loss: 0.28932634051850076\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1142] Loss: 0.28933610166951285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1143] Loss: 0.2893352196375203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1144] Loss: 0.2893372297159078\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1145] Loss: 0.2893587387927893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1146] Loss: 0.2893539222716709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1147] Loss: 0.28935088694130234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1148] Loss: 0.2893405729888159\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1149] Loss: 0.28933809643775465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1150] Loss: 0.2893287843085599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1151] Loss: 0.2893203820131086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1152] Loss: 0.2893224204274642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1153] Loss: 0.2893097246805882\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1154] Loss: 0.2893066554142546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1155] Loss: 0.289317576330197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1156] Loss: 0.28933988258624316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1157] Loss: 0.28932966658713855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1158] Loss: 0.2893546953273406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1159] Loss: 0.28934895418142464\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1160] Loss: 0.28933799060926996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1161] Loss: 0.28935466285328615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1162] Loss: 0.2893673177592932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1163] Loss: 0.2893677269466816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1164] Loss: 0.2893614608511966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1165] Loss: 0.2893632735206584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1166] Loss: 0.28937812230800203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1167] Loss: 0.28939312662304234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1168] Loss: 0.2894040333274519\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1169] Loss: 0.2894096921873495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1170] Loss: 0.28942140346351874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1171] Loss: 0.28943331374604375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1172] Loss: 0.28942495017152425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1173] Loss: 0.28941716004150764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1174] Loss: 0.28940882557334724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1175] Loss: 0.28943403964707587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1176] Loss: 0.28942925675479797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1177] Loss: 0.2894193501496699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1178] Loss: 0.2894166828008019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1179] Loss: 0.2894067850807344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1180] Loss: 0.28943171097157155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1181] Loss: 0.289449110482073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1182] Loss: 0.2894369426663858\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1183] Loss: 0.28943159979431016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1184] Loss: 0.28942353626257483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1185] Loss: 0.2894300310520291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1186] Loss: 0.2894482620858899\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1187] Loss: 0.2894502074122674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1188] Loss: 0.2894420634075829\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1189] Loss: 0.28945219812767764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1190] Loss: 0.2894414078209063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1191] Loss: 0.2894546249303919\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1192] Loss: 0.2894560838894727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1193] Loss: 0.28944609310502456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1194] Loss: 0.2894354485091084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1195] Loss: 0.289443887752092\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1196] Loss: 0.2894310493590553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1197] Loss: 0.2894384097705708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1198] Loss: 0.2894310178412495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1199] Loss: 0.2894440197765636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1200] Loss: 0.28949082658173164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1201] Loss: 0.2894804262603133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1202] Loss: 0.28947173891052513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1203] Loss: 0.2895241554927708\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1204] Loss: 0.28952573454523944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1205] Loss: 0.28952706953455043\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1206] Loss: 0.28953169951721736\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1207] Loss: 0.28953340439880015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1208] Loss: 0.2895267403012695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1209] Loss: 0.2895358696252539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1210] Loss: 0.2895371562348667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1211] Loss: 0.28955871678386386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1212] Loss: 0.2895560687000923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1213] Loss: 0.28955282121513326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1214] Loss: 0.28953979842826527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1215] Loss: 0.2895370552378813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1216] Loss: 0.28952705769220966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1217] Loss: 0.28951445838551476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1218] Loss: 0.28951588421943364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1219] Loss: 0.2895120953763604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1220] Loss: 0.2895155519616391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1221] Loss: 0.28953121646324625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1222] Loss: 0.28953030340974917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1223] Loss: 0.28954616635910113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1224] Loss: 0.2895467947993342\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1225] Loss: 0.2895495519999709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1226] Loss: 0.28954254798399615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1227] Loss: 0.2895370947413292\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1228] Loss: 0.28953627767688667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1229] Loss: 0.2895322509592286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1230] Loss: 0.2895354870935725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1231] Loss: 0.28953827458115966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1232] Loss: 0.2895298416851073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1233] Loss: 0.28954503815432486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1234] Loss: 0.2895602031904749\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1235] Loss: 0.2895641578074298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1236] Loss: 0.28955736546220723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1237] Loss: 0.2895583006588852\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1238] Loss: 0.2895574708255881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1239] Loss: 0.28954934778607894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1240] Loss: 0.2895509055770925\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1241] Loss: 0.28954885649625417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1242] Loss: 0.28956860965071374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1243] Loss: 0.28956622404062576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1244] Loss: 0.2895757718913607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1245] Loss: 0.2895839638367761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1246] Loss: 0.2895753759555156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1247] Loss: 0.28956873027870805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1248] Loss: 0.2895734061456605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1249] Loss: 0.28957332215169085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1250] Loss: 0.2895584085467869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1251] Loss: 0.289545720091823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1252] Loss: 0.2895563798814939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1253] Loss: 0.28956208759608554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1254] Loss: 0.28957388296621434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1255] Loss: 0.28957200964947394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1256] Loss: 0.28957578253285404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1257] Loss: 0.2895681652016971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1258] Loss: 0.28959713868743103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1259] Loss: 0.2896043381224407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1260] Loss: 0.2895996936091988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1261] Loss: 0.2896011101846626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1262] Loss: 0.2896073510599183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1263] Loss: 0.2896113532979685\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1264] Loss: 0.28962309271937514\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1265] Loss: 0.2896297286750325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1266] Loss: 0.2896263870112357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1267] Loss: 0.28960970445416945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1268] Loss: 0.28964774717927455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1269] Loss: 0.2896403828289955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1270] Loss: 0.2896529531441809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1271] Loss: 0.2896391196528024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1272] Loss: 0.2896513859648375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1273] Loss: 0.28964081706569367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1274] Loss: 0.2896318746296036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1275] Loss: 0.28962758566268787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1276] Loss: 0.28963140568032747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1277] Loss: 0.2896325144600986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1278] Loss: 0.2896230462724259\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1279] Loss: 0.28961014619028824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1280] Loss: 0.28960076699368587\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1281] Loss: 0.28958865265870887\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1282] Loss: 0.28959562197778727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1283] Loss: 0.28959434049869837\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1284] Loss: 0.2896007535971828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1285] Loss: 0.2896073589618221\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1286] Loss: 0.28961998429106556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1287] Loss: 0.2896187291095831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1288] Loss: 0.2896200483161322\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1289] Loss: 0.2896089073780563\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1290] Loss: 0.28961805179494154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1291] Loss: 0.28960645262162454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1292] Loss: 0.28960412910992966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1293] Loss: 0.2895935640672016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1294] Loss: 0.289600123429214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1295] Loss: 0.2895906326838053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1296] Loss: 0.28959419829676347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1297] Loss: 0.28958646990242864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1298] Loss: 0.2895904175770417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1299] Loss: 0.28960155422394984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1300] Loss: 0.2895959507587023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1301] Loss: 0.28960273352164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1302] Loss: 0.28960696618620085\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1303] Loss: 0.2896057699220099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1304] Loss: 0.28960589246474044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1305] Loss: 0.2896012596766706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1306] Loss: 0.28960365099732666\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8862\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1307] Loss: 0.2895930156648631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1308] Loss: 0.28959432610374136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1309] Loss: 0.2895893566975242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1310] Loss: 0.28959779069213815\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1311] Loss: 0.289603380381004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1312] Loss: 0.28959693891155763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1313] Loss: 0.28959453832920723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1314] Loss: 0.28959057316706893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1315] Loss: 0.28958230596049506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1316] Loss: 0.28959962320795607\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1317] Loss: 0.2895873485355268\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1318] Loss: 0.28957936628023145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1319] Loss: 0.2895735929433002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1320] Loss: 0.2895894317223084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1321] Loss: 0.2895920945565181\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1322] Loss: 0.2895865285483996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1323] Loss: 0.28957573566649164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1324] Loss: 0.28957887246655867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1325] Loss: 0.28957497356321726\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1326] Loss: 0.28957094138513106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1327] Loss: 0.2895644327592297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1328] Loss: 0.2895891757212689\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1329] Loss: 0.2896052975385513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1330] Loss: 0.28961504603737825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1331] Loss: 0.2896093785155217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1332] Loss: 0.28960222952685294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1333] Loss: 0.28961097083502246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1334] Loss: 0.28960251236445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1335] Loss: 0.2896360679075992\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1336] Loss: 0.2896383546543869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1337] Loss: 0.28965520593297794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1338] Loss: 0.2896468702502489\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1339] Loss: 0.2896439473496898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1340] Loss: 0.28963261403151663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1341] Loss: 0.28964021788655736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1342] Loss: 0.28964189677200997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1343] Loss: 0.28964152211131583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1344] Loss: 0.2896432715867561\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1345] Loss: 0.2896338101352621\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1346] Loss: 0.28962210474000594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1347] Loss: 0.28961223207211684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1348] Loss: 0.28959825383348664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1349] Loss: 0.2895929473765828\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1350] Loss: 0.2895815953268539\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1351] Loss: 0.28958772870035593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1352] Loss: 0.2895783717264401\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1353] Loss: 0.28957762202516896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1354] Loss: 0.2895804419099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1355] Loss: 0.2895841775831811\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1356] Loss: 0.28957204399000197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1357] Loss: 0.28957691307755795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1358] Loss: 0.28958927134874735\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1359] Loss: 0.2895844015670052\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1360] Loss: 0.28958608780656736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1361] Loss: 0.28958092834064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1362] Loss: 0.2895801957742566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1363] Loss: 0.28959767774669565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1364] Loss: 0.28960114008115057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1365] Loss: 0.2896006627983598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1366] Loss: 0.28960321486313806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1367] Loss: 0.28960694710764845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1368] Loss: 0.28961516447870117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1369] Loss: 0.2896127829573163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1370] Loss: 0.2896058278380067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1371] Loss: 0.28960572305523585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1372] Loss: 0.28961268137022056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1373] Loss: 0.2896050549486217\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1374] Loss: 0.28959342703713015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1375] Loss: 0.28958073237893023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1376] Loss: 0.2895752285481505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1377] Loss: 0.28958602047080956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1378] Loss: 0.28958353711060086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1379] Loss: 0.2895790087129123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1380] Loss: 0.2895863553483732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1381] Loss: 0.2896105082435056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1382] Loss: 0.2896246831734336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1383] Loss: 0.28961842020323053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1384] Loss: 0.2896168644807701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1385] Loss: 0.28960399309773993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1386] Loss: 0.2895983472484573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1387] Loss: 0.28959064099357845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1388] Loss: 0.2895867542194278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1389] Loss: 0.28958281394817126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1390] Loss: 0.2895877136302139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1391] Loss: 0.2895911167166391\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1392] Loss: 0.2895924196016287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1393] Loss: 0.2895832375908687\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1394] Loss: 0.2895977151769243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1395] Loss: 0.28960970966467486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1396] Loss: 0.2896013717423294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1397] Loss: 0.2895992254455184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1398] Loss: 0.28959659412053784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1399] Loss: 0.2896220839472905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1400] Loss: 0.2896287428985825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1401] Loss: 0.28962942231314986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1402] Loss: 0.2896273438510184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1403] Loss: 0.28963687102970465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1404] Loss: 0.28966172470989426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1405] Loss: 0.2896703030790805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1406] Loss: 0.2897119617813953\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1407] Loss: 0.28971257278307616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1408] Loss: 0.2897025123197485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1409] Loss: 0.2896936797533897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1410] Loss: 0.28968553666829056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1411] Loss: 0.28967369436603313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1412] Loss: 0.28966724790318976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1413] Loss: 0.2896586283085992\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1414] Loss: 0.2896578950152262\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1415] Loss: 0.28964839666441333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1416] Loss: 0.2896426472347332\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1417] Loss: 0.2896400914020352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1418] Loss: 0.28963476913207853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1419] Loss: 0.28964969710055183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1420] Loss: 0.2896514937941025\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1421] Loss: 0.28963989785327393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1422] Loss: 0.2896384273990775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1423] Loss: 0.2896725018291939\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1424] Loss: 0.28967601205595345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1425] Loss: 0.2896906281212856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1426] Loss: 0.28967790026621104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1427] Loss: 0.28967056807139874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1428] Loss: 0.2896763607577952\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1429] Loss: 0.28970112710511964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1430] Loss: 0.2897074248120872\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1431] Loss: 0.2897016767059197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1432] Loss: 0.2896973406720351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1433] Loss: 0.2896868550695101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1434] Loss: 0.28968339205150107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1435] Loss: 0.2896815152230699\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1436] Loss: 0.28968488701181455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1437] Loss: 0.28967748116783826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1438] Loss: 0.2896981658653788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1439] Loss: 0.28968679217996834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1440] Loss: 0.2896813864468282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1441] Loss: 0.2896729957448397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1442] Loss: 0.2896700329000106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1443] Loss: 0.2896688642778723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1444] Loss: 0.28967116766327444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1445] Loss: 0.2896895306275016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1446] Loss: 0.28968150818214006\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1447] Loss: 0.2896741151865946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1448] Loss: 0.2896661814876987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1449] Loss: 0.289655922450632\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1450] Loss: 0.2896931205541387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1451] Loss: 0.28968010000637745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1452] Loss: 0.28966967472244926\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1453] Loss: 0.28966097858601225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1454] Loss: 0.2896844833023195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1455] Loss: 0.2896924551849291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1456] Loss: 0.28969133703177136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1457] Loss: 0.2896907308774433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1458] Loss: 0.2896881661403291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1459] Loss: 0.28968388564836645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1460] Loss: 0.28968553536297725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1461] Loss: 0.28970873996797636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1462] Loss: 0.28970011539755613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1463] Loss: 0.2897044218640771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1464] Loss: 0.2897001879262609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1465] Loss: 0.28970080277641164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1466] Loss: 0.28969109961083334\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1467] Loss: 0.2896780181766678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1468] Loss: 0.2896954574160949\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1469] Loss: 0.28970016621023775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1470] Loss: 0.28972093462644155\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1471] Loss: 0.2897229821253696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1472] Loss: 0.2897305833244604\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1473] Loss: 0.28971982815700564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1474] Loss: 0.28972854558317135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1475] Loss: 0.289717608442\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1476] Loss: 0.2897168816089146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1477] Loss: 0.28970493835458644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1478] Loss: 0.2897040141411417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1479] Loss: 0.28969844173859594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1480] Loss: 0.28970201506938315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1481] Loss: 0.289701370670291\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1482] Loss: 0.2896970657241848\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1483] Loss: 0.2897073288232585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1484] Loss: 0.2896960494942188\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1485] Loss: 0.28969023638916863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1486] Loss: 0.28968750055440684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1487] Loss: 0.28967240168318914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1488] Loss: 0.2896655494100686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1489] Loss: 0.28967343477705954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1490] Loss: 0.2896767935988173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1491] Loss: 0.28967106587646974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1492] Loss: 0.2896711406640721\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1493] Loss: 0.2896697828739398\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1494] Loss: 0.2896744518325335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1495] Loss: 0.28966643695683486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1496] Loss: 0.28968436175364287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1497] Loss: 0.28967934699330217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1498] Loss: 0.2896741723021053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1499] Loss: 0.2896849629575991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1500] Loss: 0.2896858108551257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1501] Loss: 0.2897148027857787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1502] Loss: 0.2897408888504946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1503] Loss: 0.2897318279057097\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1504] Loss: 0.28972860371295717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1505] Loss: 0.2897240903138788\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1506] Loss: 0.2897275432563095\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1507] Loss: 0.2897355373276727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1508] Loss: 0.28972907490755906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1509] Loss: 0.289714642976271\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1510] Loss: 0.2897340749118264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1511] Loss: 0.2897285463107136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1512] Loss: 0.289732524209111\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1513] Loss: 0.28974334503650917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1514] Loss: 0.2897440583305661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1515] Loss: 0.28974614076229405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1516] Loss: 0.2897315192776104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1517] Loss: 0.2897211820131131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1518] Loss: 0.28972000597930575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1519] Loss: 0.2897269267391099\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1520] Loss: 0.28971587140635985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1521] Loss: 0.2897385185389959\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1522] Loss: 0.2897303398874282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1523] Loss: 0.2897341471200179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 18, Batch 1524] Loss: 0.28973911543210357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 0] Loss: 0.28975450411009934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1] Loss: 0.2897586224674114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 2] Loss: 0.28974899995082226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 3] Loss: 0.2897630102887933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 4] Loss: 0.28977207405966604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 5] Loss: 0.28975913694891775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 6] Loss: 0.28974767066657753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 7] Loss: 0.28974172841194956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 8] Loss: 0.2897345822096622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 9] Loss: 0.28973651105457016\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 10] Loss: 0.2897376259217732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 11] Loss: 0.2897509709236623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 12] Loss: 0.28975791440196413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 13] Loss: 0.28975409972697425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 14] Loss: 0.28978318925819463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 15] Loss: 0.2897800931563953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 16] Loss: 0.2897915836257115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 17] Loss: 0.28978258021441083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 18] Loss: 0.2897716429011867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 19] Loss: 0.28977859275789314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 20] Loss: 0.28977144732339283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 21] Loss: 0.28976593173719867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 22] Loss: 0.2897775222874962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 23] Loss: 0.2898105490069861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 24] Loss: 0.2898110748814556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 25] Loss: 0.2898178377688451\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 26] Loss: 0.2898229808680346\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 27] Loss: 0.2898214181928888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 28] Loss: 0.2898487819684954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 29] Loss: 0.28983556487479595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 30] Loss: 0.2898317260002548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 31] Loss: 0.28983733708784065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 32] Loss: 0.2898301930890601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 33] Loss: 0.28982715356962546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 34] Loss: 0.28982110723004806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 35] Loss: 0.28982215141928314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 36] Loss: 0.28982567934898373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 37] Loss: 0.2898378462959269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 38] Loss: 0.28982542787353205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 39] Loss: 0.2898448848205711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 40] Loss: 0.28984065672117393\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 41] Loss: 0.28983451627196505\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 42] Loss: 0.2898422200866168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 43] Loss: 0.2898597335988986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 44] Loss: 0.28985630999059697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 45] Loss: 0.28985589341660695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 46] Loss: 0.28985534382102646\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 47] Loss: 0.2898461087845986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 48] Loss: 0.2898355815091529\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 49] Loss: 0.28983705061644033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 50] Loss: 0.28987063057991197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 51] Loss: 0.2898706416616698\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 52] Loss: 0.28987475496268045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 53] Loss: 0.2898770665030031\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 54] Loss: 0.2898728303588019\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 55] Loss: 0.2898722982306554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 56] Loss: 0.28987945348287214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 57] Loss: 0.2898684179180093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 58] Loss: 0.28986108376586284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 59] Loss: 0.28984995691118026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 60] Loss: 0.28984628574806437\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 61] Loss: 0.2898432850752827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 62] Loss: 0.28983183736404605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 63] Loss: 0.2898291326466812\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 64] Loss: 0.28983810244752956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 65] Loss: 0.2898385090391987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 66] Loss: 0.28983947414168204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 67] Loss: 0.28983718747991966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 68] Loss: 0.289836080423074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 69] Loss: 0.2898251350397139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 70] Loss: 0.2898173378644061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 71] Loss: 0.28981616724284937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 72] Loss: 0.2898286822824058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 73] Loss: 0.28981957715584017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 74] Loss: 0.2898228482614808\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 75] Loss: 0.2898126034240936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 76] Loss: 0.28981285021286896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 77] Loss: 0.28981051813939857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 78] Loss: 0.2898046266161211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 79] Loss: 0.2898174833461261\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 80] Loss: 0.28980791328690836\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 81] Loss: 0.2897937114266617\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 82] Loss: 0.28980856634181984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 83] Loss: 0.289800720925615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 84] Loss: 0.28980093719629324\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 85] Loss: 0.2897909823952162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 86] Loss: 0.2898034142576105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 87] Loss: 0.2898138798158118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 88] Loss: 0.2898234349520616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 89] Loss: 0.2898217301643903\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 90] Loss: 0.28980940145876727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 91] Loss: 0.2898015088952194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 92] Loss: 0.2898100391885305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 93] Loss: 0.28981246980222564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 94] Loss: 0.28980332331585484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 95] Loss: 0.28979545150233965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 96] Loss: 0.289793414985637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 97] Loss: 0.28980894740855606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 98] Loss: 0.28979964429675187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 99] Loss: 0.2897858711316609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 100] Loss: 0.2897922700044582\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 101] Loss: 0.2897817418903757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 102] Loss: 0.2897959414469581\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 103] Loss: 0.2897835734961869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 104] Loss: 0.28979932896214045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 105] Loss: 0.2897915690606477\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 106] Loss: 0.28981175893451816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 107] Loss: 0.28980470734561586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 108] Loss: 0.28979906549239204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 109] Loss: 0.28979774931322744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 110] Loss: 0.2897915449419038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 111] Loss: 0.2897993148762473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 112] Loss: 0.2897975528130643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 113] Loss: 0.28979235335749304\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 114] Loss: 0.2897920663602778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 115] Loss: 0.2898184022424257\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 116] Loss: 0.28982254615267317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 117] Loss: 0.28983256999609736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 118] Loss: 0.28982707720655315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 119] Loss: 0.2898218724213094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 120] Loss: 0.28982515791565655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 121] Loss: 0.28984282616778617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 122] Loss: 0.28982865007177344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 123] Loss: 0.2898271204681033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 124] Loss: 0.28983243317250457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 125] Loss: 0.28982566441464785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 126] Loss: 0.2898162393332298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 127] Loss: 0.2898174170376424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 128] Loss: 0.289827089563845\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 129] Loss: 0.2898172716076787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 130] Loss: 0.2898106153785429\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 131] Loss: 0.28980815157877365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 132] Loss: 0.2898003154421086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 133] Loss: 0.2897904288879669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 134] Loss: 0.2898008511730439\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 135] Loss: 0.2897888436356915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 136] Loss: 0.2897772377065983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 137] Loss: 0.28977969445370444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 138] Loss: 0.2897724577032598\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 139] Loss: 0.28977223586803674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 140] Loss: 0.2897735277002659\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 141] Loss: 0.2897639275422263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 142] Loss: 0.2897675799348601\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 143] Loss: 0.2897688307540956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 144] Loss: 0.28975907802249673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 145] Loss: 0.28974864463615063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 146] Loss: 0.28974330222663186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 147] Loss: 0.28975736339532016\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 148] Loss: 0.2897509090075356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 149] Loss: 0.2897428851363804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 150] Loss: 0.2897432341030178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 151] Loss: 0.28973705486692103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 152] Loss: 0.28975358822451736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 153] Loss: 0.2897579663257987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 154] Loss: 0.28976110218145695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 155] Loss: 0.2897612228251255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 156] Loss: 0.2897513795036766\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 157] Loss: 0.289743460064361\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 158] Loss: 0.2897385628377241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 159] Loss: 0.2897408297341133\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 160] Loss: 0.2897308019505175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 161] Loss: 0.2897180246690961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 162] Loss: 0.2897148655182479\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 163] Loss: 0.2897326367275499\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 164] Loss: 0.28974130691504857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 165] Loss: 0.28977524326925747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 166] Loss: 0.28978514001036343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 167] Loss: 0.28977489571844284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 168] Loss: 0.28977353575565395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 169] Loss: 0.2897573321458523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 170] Loss: 0.2897535398685627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 171] Loss: 0.2897486802632771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 172] Loss: 0.28974157880365276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 173] Loss: 0.28972987725607846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 174] Loss: 0.2897255172207898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 175] Loss: 0.28975891766224504\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 176] Loss: 0.28976111362779927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 177] Loss: 0.2897828426805835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 178] Loss: 0.2897809300617784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 179] Loss: 0.2897760402841404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 180] Loss: 0.28978063394456244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 181] Loss: 0.28978927531761667\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 182] Loss: 0.289810750099954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 183] Loss: 0.2898008712021913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 184] Loss: 0.2898009706926784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 185] Loss: 0.28980405951861166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 186] Loss: 0.28979409353047564\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 187] Loss: 0.28978777847642473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 188] Loss: 0.28978661715588994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 189] Loss: 0.2897764527558899\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 190] Loss: 0.28977263289886057\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 191] Loss: 0.28978709303430156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 192] Loss: 0.28977251740851584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 193] Loss: 0.28977063202181297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 194] Loss: 0.2897854675575652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 195] Loss: 0.2897948142046064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 196] Loss: 0.28978689314867656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 197] Loss: 0.28980020115741834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 198] Loss: 0.2897980411225569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 199] Loss: 0.2897978098753245\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 200] Loss: 0.28979431105076453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 201] Loss: 0.2897879613181951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 202] Loss: 0.28978511802755824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 203] Loss: 0.28978035806167923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 204] Loss: 0.2897885447850676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 205] Loss: 0.28980934138705744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 206] Loss: 0.289809063099711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 207] Loss: 0.2898081393208008\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 208] Loss: 0.289810953180625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 209] Loss: 0.28979704829525416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 210] Loss: 0.28979561741076737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 211] Loss: 0.2897931779302589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 212] Loss: 0.2897860466679352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 213] Loss: 0.28978788206534395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 214] Loss: 0.2897774884085586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 215] Loss: 0.2897731182876885\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 216] Loss: 0.28977218782181724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 217] Loss: 0.28977194263282335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 218] Loss: 0.2897653119882585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 219] Loss: 0.2897877091926335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 220] Loss: 0.2898028799943316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 221] Loss: 0.2898119084411971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 222] Loss: 0.289831008514962\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 223] Loss: 0.2898204585540387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 224] Loss: 0.2898216525087912\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 225] Loss: 0.28982401148510606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 226] Loss: 0.2898127489456009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 227] Loss: 0.2898035585410712\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 228] Loss: 0.28980671863037166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 229] Loss: 0.2898067149815508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 230] Loss: 0.28981905260099944\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 231] Loss: 0.2898177306440349\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 232] Loss: 0.2898140374355877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 233] Loss: 0.28983796873376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 234] Loss: 0.2898327937306695\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 235] Loss: 0.2898377577430862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 236] Loss: 0.2898354889156591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 237] Loss: 0.2898270648832012\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 238] Loss: 0.2898270857530677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 239] Loss: 0.28982625224613745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 240] Loss: 0.2898257173728571\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 241] Loss: 0.28982348427716553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 242] Loss: 0.28981625762510743\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 243] Loss: 0.28981937651314954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 244] Loss: 0.2898195863041661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 245] Loss: 0.28984622569295226\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 246] Loss: 0.289855277471988\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 247] Loss: 0.2898481221530986\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 248] Loss: 0.2898500244448416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 249] Loss: 0.28985332787101964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 250] Loss: 0.28985509705211787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 251] Loss: 0.2898457081299578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 252] Loss: 0.28985547045933124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 253] Loss: 0.2898858582157985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 254] Loss: 0.28988974847174626\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 255] Loss: 0.289884759297791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 256] Loss: 0.2898816854589574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 257] Loss: 0.28987745875252147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 258] Loss: 0.28986467371403846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 259] Loss: 0.2898593738425148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 260] Loss: 0.2898666911767809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 261] Loss: 0.28985713191043194\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 262] Loss: 0.2898576018407018\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 263] Loss: 0.2898626879392042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 264] Loss: 0.28986448619980365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 265] Loss: 0.28985702117261053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 266] Loss: 0.28984818977087917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 267] Loss: 0.28986846012424017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 268] Loss: 0.28986024869483923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 269] Loss: 0.2898496679223494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 270] Loss: 0.2898554708286747\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 271] Loss: 0.2898517707552484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 272] Loss: 0.2898408582840347\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 273] Loss: 0.28983666210645714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 274] Loss: 0.2898410370112066\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 275] Loss: 0.2898511445633575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 276] Loss: 0.2898491821402195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 277] Loss: 0.2898475937390933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 278] Loss: 0.28985241941302897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 279] Loss: 0.28984561125397323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 280] Loss: 0.28983745900142405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 281] Loss: 0.28983186354183377\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8889\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 282] Loss: 0.28982500108021436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 283] Loss: 0.289811379400654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 284] Loss: 0.2898082017541566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 285] Loss: 0.2898044674740077\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 286] Loss: 0.28980019188806677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 287] Loss: 0.2898046924391067\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 288] Loss: 0.28980281080615555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 289] Loss: 0.28979356067136935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 290] Loss: 0.2897908725224696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 291] Loss: 0.28978024714344336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 292] Loss: 0.28978239563396113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 293] Loss: 0.2897692411983263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 294] Loss: 0.2897695664436569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 295] Loss: 0.28976688861044186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 296] Loss: 0.28976566052265235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 297] Loss: 0.2897831428873923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 298] Loss: 0.2897927324601312\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 299] Loss: 0.2897953084434237\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 300] Loss: 0.2897915996348063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 301] Loss: 0.28978726871709676\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 302] Loss: 0.2898001670011154\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 303] Loss: 0.2898104003637419\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 304] Loss: 0.2898110666097643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 305] Loss: 0.28982663363935474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 306] Loss: 0.289819583341662\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 307] Loss: 0.28981857482932705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 308] Loss: 0.289818611037701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 309] Loss: 0.2898252586425795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 310] Loss: 0.28982067980158993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 311] Loss: 0.28982319209575186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 312] Loss: 0.28982011258560725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 313] Loss: 0.2898199232671454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 314] Loss: 0.28982882182161207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 315] Loss: 0.28982389262792135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 316] Loss: 0.28982391022543863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 317] Loss: 0.28984085792595865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 318] Loss: 0.28984647006468356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 319] Loss: 0.2898571860519911\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 320] Loss: 0.2898634191690029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 321] Loss: 0.2898701319476175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 322] Loss: 0.2898773524725565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 323] Loss: 0.2898782067635409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 324] Loss: 0.2898712335409062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 325] Loss: 0.2898661291317589\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 326] Loss: 0.2898633353129426\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 327] Loss: 0.28985302831632576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 328] Loss: 0.28984544013604596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 329] Loss: 0.2898512202462795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 330] Loss: 0.28984378002318834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 331] Loss: 0.28983428936342176\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 332] Loss: 0.2898326649418307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 333] Loss: 0.2898423833206443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 334] Loss: 0.28985491820725173\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 335] Loss: 0.2898458417437438\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 336] Loss: 0.2898336411538594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 337] Loss: 0.2898254857755741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 338] Loss: 0.2898225792996222\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 339] Loss: 0.2898166342566558\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 340] Loss: 0.28980989484182673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 341] Loss: 0.2898077594042015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 342] Loss: 0.2898011094558194\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 343] Loss: 0.28979926707845177\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 344] Loss: 0.28978647993622186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 345] Loss: 0.2897957256300209\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 346] Loss: 0.2897852252947412\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 347] Loss: 0.2897822495769457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 348] Loss: 0.28977657012182784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 349] Loss: 0.28977346462362763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 350] Loss: 0.2897631207109027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 351] Loss: 0.28978288055306034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 352] Loss: 0.2897891447060827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 353] Loss: 0.28979178574890513\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 354] Loss: 0.28979267142300147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 355] Loss: 0.28978005036394083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 356] Loss: 0.28979011641169017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 357] Loss: 0.2897828205202087\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 358] Loss: 0.28978970061444115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 359] Loss: 0.28980202765258056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 360] Loss: 0.2897966083491073\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 361] Loss: 0.289789482052978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 362] Loss: 0.289786549184528\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 363] Loss: 0.289784817814008\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 364] Loss: 0.2897829823367863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 365] Loss: 0.2897812163521412\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 366] Loss: 0.28977827609551193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 367] Loss: 0.2897773821919981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 368] Loss: 0.2897879509072833\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 369] Loss: 0.289775777015875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 370] Loss: 0.2897726254877446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 371] Loss: 0.2897741131402024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 372] Loss: 0.2897677344306796\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 373] Loss: 0.2897598504703467\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 374] Loss: 0.289778180498901\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 375] Loss: 0.2897644749632136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 376] Loss: 0.28976422063419627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 377] Loss: 0.2897663800756843\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 378] Loss: 0.28976069187122105\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 379] Loss: 0.2897751751373728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 380] Loss: 0.28977730080089037\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 381] Loss: 0.28978278864095564\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 382] Loss: 0.2897792986206211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 383] Loss: 0.28976815806267653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 384] Loss: 0.28976466526397276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 385] Loss: 0.28976407961346123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 386] Loss: 0.2897690530051923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 387] Loss: 0.28977685169012946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 388] Loss: 0.28977226811984774\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 389] Loss: 0.2897693499544124\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 390] Loss: 0.2897770828735757\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 391] Loss: 0.28978651236677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 392] Loss: 0.2897801993812314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 393] Loss: 0.2897683382237552\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 394] Loss: 0.28976910620683927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 395] Loss: 0.28975511829464773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 396] Loss: 0.289744664789287\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 397] Loss: 0.2897514837733378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 398] Loss: 0.2897479802569028\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 399] Loss: 0.2897575006922701\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 400] Loss: 0.28974530029508994\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 401] Loss: 0.2897542663663573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 402] Loss: 0.28975433940583606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 403] Loss: 0.28976407960312595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 404] Loss: 0.2897535922766445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 405] Loss: 0.2897499091058178\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 406] Loss: 0.28974517206375927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 407] Loss: 0.28973971114064667\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 408] Loss: 0.2897390765599413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 409] Loss: 0.2897257939467778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 410] Loss: 0.2897192851945452\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 411] Loss: 0.28972836090530585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 412] Loss: 0.28972194235014814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 413] Loss: 0.2897303118805723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 414] Loss: 0.289720351098376\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 415] Loss: 0.28971529852876954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 416] Loss: 0.2897185184840001\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 417] Loss: 0.2897069548280609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 418] Loss: 0.28970404607709566\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 419] Loss: 0.28970962176993265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 420] Loss: 0.28970224920059573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 421] Loss: 0.2896971140104531\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 422] Loss: 0.28969910685665795\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 423] Loss: 0.28969948253706485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 424] Loss: 0.28971007961924167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 425] Loss: 0.2897150211199491\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 426] Loss: 0.2897328430288742\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 427] Loss: 0.28972805629184095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 428] Loss: 0.2897479581926518\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 429] Loss: 0.28973798776206444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 430] Loss: 0.289743208662202\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 431] Loss: 0.2897404064819544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 432] Loss: 0.2897471400278417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 433] Loss: 0.2897476105252791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 434] Loss: 0.28974186004674696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 435] Loss: 0.28973547165094227\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 436] Loss: 0.2897292742285668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 437] Loss: 0.2897374868256637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 438] Loss: 0.2897329703870614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 439] Loss: 0.28973792458324205\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 440] Loss: 0.2897387159601548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 441] Loss: 0.2897323749598541\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 442] Loss: 0.28972819710171144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 443] Loss: 0.289725006241379\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 444] Loss: 0.2897194290251714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 445] Loss: 0.2897184807961058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 446] Loss: 0.28970667522005633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 447] Loss: 0.28970379389077267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 448] Loss: 0.28972265155794535\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 449] Loss: 0.2897189474272946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 450] Loss: 0.28971582253765243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 451] Loss: 0.2897098806539026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 452] Loss: 0.28970794536088174\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 453] Loss: 0.2896957529237858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 454] Loss: 0.2896885499050671\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 455] Loss: 0.28969306796500083\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 456] Loss: 0.28968277054709546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 457] Loss: 0.28969324183738804\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 458] Loss: 0.28968399433485587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 459] Loss: 0.2896837998580673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 460] Loss: 0.28968829966960696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 461] Loss: 0.28967920001592273\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 462] Loss: 0.28967021165688567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 463] Loss: 0.2896692214216789\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 464] Loss: 0.2896772216576337\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 465] Loss: 0.28966662011302646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 466] Loss: 0.2896659335295618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 467] Loss: 0.2896651988491272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 468] Loss: 0.28965867005739165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 469] Loss: 0.28965686981113553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 470] Loss: 0.2896586495437846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 471] Loss: 0.28966662281805605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 472] Loss: 0.2896596033894168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 473] Loss: 0.28965032683238034\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 474] Loss: 0.28965400346305875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 475] Loss: 0.28965362517288473\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 476] Loss: 0.28965265619670605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 477] Loss: 0.2896596131500072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 478] Loss: 0.28964676783784987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 479] Loss: 0.28963905469336615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 480] Loss: 0.2896451182865446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 481] Loss: 0.2896393518940818\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 482] Loss: 0.2896427739193353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 483] Loss: 0.2896343246078727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 484] Loss: 0.2896504009901004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 485] Loss: 0.28965114279480014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 486] Loss: 0.28965138669180673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 487] Loss: 0.28966373522701233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 488] Loss: 0.2896667020639241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 489] Loss: 0.28966248732746314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 490] Loss: 0.28966301960987284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 491] Loss: 0.2896532953715045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 492] Loss: 0.28964507701035297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 493] Loss: 0.2896511436773644\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 494] Loss: 0.2896402922886278\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 495] Loss: 0.2896413706641381\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 496] Loss: 0.28963254448775877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 497] Loss: 0.289622137434778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 498] Loss: 0.2896136710508549\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 499] Loss: 0.2896075809869751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 500] Loss: 0.2895999052324732\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 501] Loss: 0.2896023501287293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 502] Loss: 0.2895964301937664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 503] Loss: 0.2895969859763308\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 504] Loss: 0.28958902044166235\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 505] Loss: 0.2895791754113455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 506] Loss: 0.2896080495488241\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 507] Loss: 0.2896093597835023\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 508] Loss: 0.28964721818059463\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 509] Loss: 0.2896512559776678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 510] Loss: 0.2896593117624396\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 511] Loss: 0.28966092151154427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 512] Loss: 0.2896556549390413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 513] Loss: 0.2896583260253657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 514] Loss: 0.2896650758711549\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 515] Loss: 0.28966511059323446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 516] Loss: 0.28967301115989225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 517] Loss: 0.28966825783630556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 518] Loss: 0.2896780323662091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 519] Loss: 0.2896664190551577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 520] Loss: 0.28968691777024586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 521] Loss: 0.28969233438265884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 522] Loss: 0.28969064467398115\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 523] Loss: 0.2896943955544838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 524] Loss: 0.28968840815535934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 525] Loss: 0.2896899775170096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 526] Loss: 0.289693034184587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 527] Loss: 0.2896972579536789\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 528] Loss: 0.2896850921403447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 529] Loss: 0.28967642758575746\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 530] Loss: 0.2896772642718595\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 531] Loss: 0.289678755945818\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 532] Loss: 0.28967409131124744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 533] Loss: 0.28968101444615024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 534] Loss: 0.28967647863073964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 535] Loss: 0.28967375174499277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 536] Loss: 0.28966917852843943\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 537] Loss: 0.2896612802770062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 538] Loss: 0.2896587472641024\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 539] Loss: 0.28965653977973094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 540] Loss: 0.2896520876603888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 541] Loss: 0.28964582538124717\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 542] Loss: 0.2896462766556277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 543] Loss: 0.2896455797115211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 544] Loss: 0.2896481391198649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 545] Loss: 0.2896472535640931\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 546] Loss: 0.2896360970647622\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 547] Loss: 0.2896436986963137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 548] Loss: 0.28964246098131763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 549] Loss: 0.2896387941380303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 550] Loss: 0.28964863097779425\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 551] Loss: 0.2896394303334648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 552] Loss: 0.28963882352341375\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 553] Loss: 0.2896479485689725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 554] Loss: 0.2896387297915877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 555] Loss: 0.28967288552644793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 556] Loss: 0.2896692328837158\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 557] Loss: 0.28968877691864015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 558] Loss: 0.2896961774682256\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 559] Loss: 0.28969764772582657\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 560] Loss: 0.2896979343748706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 561] Loss: 0.28968861633551096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 562] Loss: 0.28969243449883963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 563] Loss: 0.2896843863588429\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 564] Loss: 0.28967691097070536\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 565] Loss: 0.28967010644810537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 566] Loss: 0.2896595238472619\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 567] Loss: 0.2896577956613466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 568] Loss: 0.28965267862614624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 569] Loss: 0.2896522247083305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 570] Loss: 0.28964927809311636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 571] Loss: 0.28963463562153285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 572] Loss: 0.2896327215215317\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 573] Loss: 0.28963175584651196\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 574] Loss: 0.28961904816761064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 575] Loss: 0.2896177705253555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 576] Loss: 0.28961671560780716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 577] Loss: 0.28961928506454965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 578] Loss: 0.28962118787632896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 579] Loss: 0.2896264662566352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 580] Loss: 0.28961543674626344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 581] Loss: 0.28961115273800764\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 582] Loss: 0.28962819060250033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 583] Loss: 0.2896227881842136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 584] Loss: 0.2896146134031162\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 585] Loss: 0.28962697555258615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 586] Loss: 0.2896262167523424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 587] Loss: 0.28962677961525624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 588] Loss: 0.2896266134590148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 589] Loss: 0.2896425833663827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 590] Loss: 0.289636250224243\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 591] Loss: 0.28963446975286405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 592] Loss: 0.28963675145856316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 593] Loss: 0.2896420483124158\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 594] Loss: 0.28965312332634247\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 595] Loss: 0.2896637918102941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 596] Loss: 0.2896600408582618\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 597] Loss: 0.289657078838716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 598] Loss: 0.2896636792064808\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 599] Loss: 0.2896887414168838\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 600] Loss: 0.28968708414724875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 601] Loss: 0.2896731151299752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 602] Loss: 0.2896744518441568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 603] Loss: 0.28967734791142336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 604] Loss: 0.289703571302642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 605] Loss: 0.28969343126106223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 606] Loss: 0.2896951677996166\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 607] Loss: 0.28968673103962284\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 608] Loss: 0.28971112709485164\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 609] Loss: 0.2897048991887678\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 610] Loss: 0.2897083600591777\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 611] Loss: 0.2897159758773018\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 612] Loss: 0.2897125470402364\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 613] Loss: 0.2897134268731617\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 614] Loss: 0.28970818433614853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 615] Loss: 0.2897088704869167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 616] Loss: 0.28970305316423556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 617] Loss: 0.28972011783767704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 618] Loss: 0.2897112483564286\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 619] Loss: 0.2897048286996993\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 620] Loss: 0.2896936885999565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 621] Loss: 0.28970283855262696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 622] Loss: 0.28970072186834983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 623] Loss: 0.28971615249155613\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 624] Loss: 0.2897182324689819\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 625] Loss: 0.2897187069071403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 626] Loss: 0.2897098703676294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 627] Loss: 0.2896996731642352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 628] Loss: 0.2896915478875914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 629] Loss: 0.2896884725921033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 630] Loss: 0.2897050529466585\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 631] Loss: 0.2896988556283761\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 632] Loss: 0.2897001620308873\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 633] Loss: 0.2897218460154697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 634] Loss: 0.2897276237585721\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 635] Loss: 0.28972525633131946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 636] Loss: 0.2897434370881831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 637] Loss: 0.2897483845218469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 638] Loss: 0.2897402147490955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 639] Loss: 0.28973256965778865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 640] Loss: 0.2897421154250653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 641] Loss: 0.2897333005698339\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 642] Loss: 0.2897301329487343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 643] Loss: 0.28972652603350857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 644] Loss: 0.28972519623866216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 645] Loss: 0.2897309314287107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 646] Loss: 0.28971853580019336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 647] Loss: 0.2897202299919861\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 648] Loss: 0.28971236988094184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 649] Loss: 0.28972857394430374\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 650] Loss: 0.28974238165324356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 651] Loss: 0.28976778591300684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 652] Loss: 0.2897634505481668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 653] Loss: 0.28975830421189686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 654] Loss: 0.28974797298033655\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 655] Loss: 0.28974119640044543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 656] Loss: 0.28973648935773044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 657] Loss: 0.2897295294088677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 658] Loss: 0.2897190748373069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 659] Loss: 0.2897116412087457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 660] Loss: 0.28970667178741294\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 661] Loss: 0.2897084922514927\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 662] Loss: 0.28971105834727967\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 663] Loss: 0.289713758160433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 664] Loss: 0.28970391711699794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 665] Loss: 0.2897316248232203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 666] Loss: 0.2897261129636522\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 667] Loss: 0.2897347893376612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 668] Loss: 0.28972622196684333\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 669] Loss: 0.2897303466136187\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 670] Loss: 0.2897405585353059\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 671] Loss: 0.28972895479602573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 672] Loss: 0.28972233861934327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 673] Loss: 0.2897104217394496\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 674] Loss: 0.28974018556635106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 675] Loss: 0.2897545643404421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 676] Loss: 0.2897489333362791\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 677] Loss: 0.2897514804694179\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 678] Loss: 0.2897852251469439\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 679] Loss: 0.2897761089962587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 680] Loss: 0.2897684465920449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 681] Loss: 0.28977495302481043\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 682] Loss: 0.2897641662055069\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 683] Loss: 0.2897617228164682\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 684] Loss: 0.2897575841957982\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 685] Loss: 0.2897570696756042\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 686] Loss: 0.28975780037473914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 687] Loss: 0.28975103863094065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 688] Loss: 0.28974080309380407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 689] Loss: 0.2897754597260248\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 690] Loss: 0.2897652770791366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 691] Loss: 0.2897645880267656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 692] Loss: 0.28976144070928084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 693] Loss: 0.2897668551447625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 694] Loss: 0.2897637900837573\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 695] Loss: 0.28975788037855715\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 696] Loss: 0.2897583861888534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 697] Loss: 0.28975027025892186\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 698] Loss: 0.28974415553106625\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 699] Loss: 0.28973163984577976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 700] Loss: 0.289723572698932\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 701] Loss: 0.2897306306272276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 702] Loss: 0.2897365915406541\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 703] Loss: 0.2897489173151204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 704] Loss: 0.2897528430162369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 705] Loss: 0.28975406538030096\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 706] Loss: 0.2897706262555046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 707] Loss: 0.2897674683232276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 708] Loss: 0.289753616330998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 709] Loss: 0.28975695857188377\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 710] Loss: 0.28975806150531197\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 711] Loss: 0.2897550064240971\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 712] Loss: 0.28975023544613404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 713] Loss: 0.2897434028758974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 714] Loss: 0.2897359745060021\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 715] Loss: 0.28972776296887587\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 716] Loss: 0.2897220042243344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 717] Loss: 0.2897162253651612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 718] Loss: 0.2897034716371961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 719] Loss: 0.28969492993031615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 720] Loss: 0.28968237202660985\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 721] Loss: 0.28967401475260135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 722] Loss: 0.2896630164656802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 723] Loss: 0.2896600075722265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 724] Loss: 0.28965124910002527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 725] Loss: 0.2896606123766692\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 726] Loss: 0.2896610156192119\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 727] Loss: 0.2896652855948588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 728] Loss: 0.28967463888268163\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 729] Loss: 0.28966355519298986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 730] Loss: 0.28965199432235894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 731] Loss: 0.2896385721384893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 732] Loss: 0.28964338309206056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 733] Loss: 0.2896327536375553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 734] Loss: 0.28962022901355594\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 735] Loss: 0.28961335195235544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 736] Loss: 0.28961499740602065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 737] Loss: 0.28960807990864357\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 738] Loss: 0.2896102523189752\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 739] Loss: 0.289604252826383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 740] Loss: 0.2895966663376981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 741] Loss: 0.2895892590846451\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 742] Loss: 0.28960106397556895\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 743] Loss: 0.2896060802543208\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 744] Loss: 0.2896119783211611\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 745] Loss: 0.2896150578901486\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 746] Loss: 0.2896137477277156\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 747] Loss: 0.2896045707387647\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 748] Loss: 0.2896083170586189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 749] Loss: 0.2896118765533778\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 750] Loss: 0.2896154131601309\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 751] Loss: 0.28960652128438624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 752] Loss: 0.28960054857252737\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 753] Loss: 0.28962075778429314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 754] Loss: 0.2896110879683797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 755] Loss: 0.28960194572240167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 756] Loss: 0.2896097267765071\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 757] Loss: 0.2896132628633669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 758] Loss: 0.2896073152116751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 759] Loss: 0.2896018502944983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 760] Loss: 0.2895943040190295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 761] Loss: 0.2895849813986229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 762] Loss: 0.28959712482723365\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 763] Loss: 0.2895974270049782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 764] Loss: 0.289625428374975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 765] Loss: 0.2896121645525669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 766] Loss: 0.28960056373383786\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 767] Loss: 0.289602061067721\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 768] Loss: 0.28960020217734933\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 769] Loss: 0.2895952021475966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 770] Loss: 0.2896207485697634\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 771] Loss: 0.2896092651453923\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 772] Loss: 0.2896076424727013\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 773] Loss: 0.2896065995848233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 774] Loss: 0.28961360481880866\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 775] Loss: 0.2896264987077311\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 776] Loss: 0.28964657543065686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 777] Loss: 0.2896397089087269\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 778] Loss: 0.2896361836238797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 779] Loss: 0.2896324918087729\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 780] Loss: 0.28965335703178724\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 781] Loss: 0.28966722847481124\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.9043\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 782] Loss: 0.2896668198255794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 783] Loss: 0.2896585052784542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 784] Loss: 0.2896562200920606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 785] Loss: 0.2896483353818441\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 786] Loss: 0.28964682290191596\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 787] Loss: 0.2896505260759554\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 788] Loss: 0.2896389244509456\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 789] Loss: 0.2896264024105549\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 790] Loss: 0.28963094328746436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 791] Loss: 0.2896206589810636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 792] Loss: 0.28960498924651557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 793] Loss: 0.28961121387436267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 794] Loss: 0.2896239428093229\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 795] Loss: 0.2896291464880299\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 796] Loss: 0.28962619955551117\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 797] Loss: 0.2896249344634141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 798] Loss: 0.2896153500552783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 799] Loss: 0.28960898664043383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 800] Loss: 0.2896125891848824\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 801] Loss: 0.28960422923235957\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 802] Loss: 0.2895956016736301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 803] Loss: 0.28961308635958527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 804] Loss: 0.28960708651514183\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 805] Loss: 0.2896143530983728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 806] Loss: 0.2896087149505106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 807] Loss: 0.28959713471535276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 808] Loss: 0.2895887861964765\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 809] Loss: 0.2895882785226951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 810] Loss: 0.28958251572432636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 811] Loss: 0.2896015373199684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 812] Loss: 0.28959500780464975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 813] Loss: 0.2895892324363267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 814] Loss: 0.2895904610055758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 815] Loss: 0.2895944145354652\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 816] Loss: 0.28958275938282546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 817] Loss: 0.2895777911878546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 818] Loss: 0.28958795478159793\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 819] Loss: 0.2895789912594538\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 820] Loss: 0.2895770931680478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 821] Loss: 0.2895871170844298\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 822] Loss: 0.28957759325199517\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 823] Loss: 0.2895886427787871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 824] Loss: 0.28959560505041754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 825] Loss: 0.2895869993444323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 826] Loss: 0.28958752960024026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 827] Loss: 0.28960385260474947\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 828] Loss: 0.28960266370394905\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 829] Loss: 0.2896047939444434\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 830] Loss: 0.289604148263579\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 831] Loss: 0.28961316933562914\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 832] Loss: 0.28960992989179984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 833] Loss: 0.2896178219233691\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 834] Loss: 0.28960887558417314\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 835] Loss: 0.2896019149127591\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 836] Loss: 0.28959608861450215\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 837] Loss: 0.2895874575833792\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 838] Loss: 0.2895997136182339\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 839] Loss: 0.28960196678373495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 840] Loss: 0.2895924367573481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 841] Loss: 0.2895836729430193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 842] Loss: 0.2895832152993956\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 843] Loss: 0.28959879692210805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 844] Loss: 0.2895894165375147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 845] Loss: 0.28958843624800534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 846] Loss: 0.2895858020682952\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 847] Loss: 0.2895988718850567\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 848] Loss: 0.28959785838790936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 849] Loss: 0.2896182116299276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 850] Loss: 0.2896256396765568\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 851] Loss: 0.28961792167409856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 852] Loss: 0.2896195767054167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 853] Loss: 0.2896085898094414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 854] Loss: 0.2896068816956251\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 855] Loss: 0.28960825140132707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 856] Loss: 0.2895998857398114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 857] Loss: 0.2895930634681569\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 858] Loss: 0.28960677907678356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 859] Loss: 0.28959985089544094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 860] Loss: 0.28958450376537764\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 861] Loss: 0.2895731095986744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 862] Loss: 0.2895707956324543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 863] Loss: 0.2895657341902511\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 864] Loss: 0.28955731924942035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 865] Loss: 0.2895806903402038\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 866] Loss: 0.2895746686230606\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 867] Loss: 0.28957972596188475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 868] Loss: 0.2895779548887565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 869] Loss: 0.2895825172633605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 870] Loss: 0.28957066365490225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 871] Loss: 0.289563125846409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 872] Loss: 0.28955927676533494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 873] Loss: 0.28956135284934975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 874] Loss: 0.2895528383607144\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 875] Loss: 0.28954980939922775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 876] Loss: 0.2895549298339378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 877] Loss: 0.2895458343676723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 878] Loss: 0.28953371725538557\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 879] Loss: 0.2895438679941093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 880] Loss: 0.28953731072990047\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 881] Loss: 0.2895348891100486\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 882] Loss: 0.28954584528678673\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 883] Loss: 0.2895396927880937\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 884] Loss: 0.28953626268764976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 885] Loss: 0.28954263162282345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 886] Loss: 0.28954109897719466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 887] Loss: 0.2895349563790141\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 888] Loss: 0.28955515650858915\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 889] Loss: 0.28954339296276443\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 890] Loss: 0.2895336744262697\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 891] Loss: 0.289531388352983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 892] Loss: 0.2895448010148674\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 893] Loss: 0.2895424577803254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 894] Loss: 0.2895336197527306\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 895] Loss: 0.28953470827229744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 896] Loss: 0.2895228189804153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 897] Loss: 0.2895101647027101\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 898] Loss: 0.28950699941159713\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 899] Loss: 0.289505632410881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 900] Loss: 0.2895145414929669\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 901] Loss: 0.2895129673286409\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 902] Loss: 0.28952245445594127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 903] Loss: 0.2895290016866102\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 904] Loss: 0.28955313591622794\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 905] Loss: 0.2895424911094527\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 906] Loss: 0.2895495422361741\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 907] Loss: 0.28955357541637883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 908] Loss: 0.2895548886586147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 909] Loss: 0.2895608684871216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 910] Loss: 0.2895617630732483\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 911] Loss: 0.28955538217218924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 912] Loss: 0.2895632097599422\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 913] Loss: 0.2895566885227394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 914] Loss: 0.2895475686552609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 915] Loss: 0.2895593238352385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 916] Loss: 0.28955450695314494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 917] Loss: 0.28955051043460445\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 918] Loss: 0.28954700284924706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 919] Loss: 0.2895401558288542\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 920] Loss: 0.2895304715591661\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 921] Loss: 0.289535540039341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 922] Loss: 0.28953750607955897\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 923] Loss: 0.2895469249514969\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 924] Loss: 0.2895425587228065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 925] Loss: 0.2895350376268058\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 926] Loss: 0.28952752464928705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 927] Loss: 0.289531376800102\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 928] Loss: 0.2895265296706854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 929] Loss: 0.28954828650886366\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 930] Loss: 0.28955158859542207\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 931] Loss: 0.2895409145530609\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 932] Loss: 0.2895526347192192\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 933] Loss: 0.28955883523305026\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 934] Loss: 0.28955398874325056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 935] Loss: 0.2895505790705772\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 936] Loss: 0.28956241439265407\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 937] Loss: 0.2895582679590758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 938] Loss: 0.28954546389190705\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 939] Loss: 0.28954421739167086\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 940] Loss: 0.28954964801974054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 941] Loss: 0.28954164928868265\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 942] Loss: 0.2895340071961867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 943] Loss: 0.2895279025631908\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 944] Loss: 0.2895282522073565\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 945] Loss: 0.28953005273493326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 946] Loss: 0.2895536856505079\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 947] Loss: 0.289558499859481\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 948] Loss: 0.2895473545579225\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 949] Loss: 0.2895464514839242\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 950] Loss: 0.2895343004321475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 951] Loss: 0.28953734705586853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 952] Loss: 0.28952583299610546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 953] Loss: 0.28951854526322285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 954] Loss: 0.2895086614971787\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 955] Loss: 0.2895024401294718\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 956] Loss: 0.28950317892095934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 957] Loss: 0.2894987567478494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 958] Loss: 0.2894918291091854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 959] Loss: 0.2894916157504341\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 960] Loss: 0.28949279041316267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 961] Loss: 0.2894849861141577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 962] Loss: 0.28949784788817773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 963] Loss: 0.28950716864345816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 964] Loss: 0.28949561358782894\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 965] Loss: 0.2894901856668433\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 966] Loss: 0.2894953145220293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 967] Loss: 0.2894880630087599\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 968] Loss: 0.28947673173122157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 969] Loss: 0.28948247562557516\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 970] Loss: 0.2894777489142648\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 971] Loss: 0.2894936057673474\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 972] Loss: 0.28950469621619185\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 973] Loss: 0.2895159369600417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 974] Loss: 0.28951486014412814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 975] Loss: 0.2895137343288239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 976] Loss: 0.28951161452525975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 977] Loss: 0.28951761307599244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 978] Loss: 0.28952587299086413\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 979] Loss: 0.28951408044928084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 980] Loss: 0.2895164464850004\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 981] Loss: 0.28950177786206477\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 982] Loss: 0.2895386682808351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 983] Loss: 0.2895503991958009\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 984] Loss: 0.28954357800952546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 985] Loss: 0.2895607097781827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 986] Loss: 0.2895525133802901\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 987] Loss: 0.28954681268940485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 988] Loss: 0.2895440528139387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 989] Loss: 0.2895473655759255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 990] Loss: 0.2895458359582784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 991] Loss: 0.2895489648049332\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 992] Loss: 0.28954292396199954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 993] Loss: 0.2895343321976449\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 994] Loss: 0.2895243964913108\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 995] Loss: 0.28952634719827325\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 996] Loss: 0.2895190043733856\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 997] Loss: 0.28951335869379696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 998] Loss: 0.2895033858526148\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 999] Loss: 0.28951763920726414\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1000] Loss: 0.28950976344564444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1001] Loss: 0.2895003035047093\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1002] Loss: 0.28950438745073886\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1003] Loss: 0.28950426013394964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1004] Loss: 0.28950495481100447\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1005] Loss: 0.28951381059524767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1006] Loss: 0.2895124573875272\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1007] Loss: 0.2895030477407055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1008] Loss: 0.289497150896883\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1009] Loss: 0.2894940728172321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1010] Loss: 0.28948498493350044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1011] Loss: 0.28947643088759123\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1012] Loss: 0.28947941982016645\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1013] Loss: 0.28947299471920107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1014] Loss: 0.2894653091800048\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1015] Loss: 0.2894674110981946\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1016] Loss: 0.28947858262582643\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1017] Loss: 0.28949510025629854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1018] Loss: 0.28949390867974495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1019] Loss: 0.2894938281601478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1020] Loss: 0.2894902838330862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1021] Loss: 0.28948605834445107\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1022] Loss: 0.2894792479815816\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1023] Loss: 0.2894704830105371\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1024] Loss: 0.2894581506338033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1025] Loss: 0.2894608179066253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1026] Loss: 0.2894539645258234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1027] Loss: 0.2894545752534165\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1028] Loss: 0.28945155383815424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1029] Loss: 0.2894548573869351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1030] Loss: 0.2894850028960382\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1031] Loss: 0.2894828874708984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1032] Loss: 0.2894951421247266\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1033] Loss: 0.2894888701831326\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1034] Loss: 0.2894814707019658\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1035] Loss: 0.28947873160025905\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1036] Loss: 0.2894764678953147\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1037] Loss: 0.28946627349574255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1038] Loss: 0.2894724950841577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1039] Loss: 0.2894636562004614\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1040] Loss: 0.2894652099704476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1041] Loss: 0.28946037568547167\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1042] Loss: 0.2894671600190359\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1043] Loss: 0.28946499316200835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1044] Loss: 0.28945657841364675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1045] Loss: 0.28944501780854054\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1046] Loss: 0.28943273068164627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1047] Loss: 0.28943241136292996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1048] Loss: 0.28942714468803044\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1049] Loss: 0.28942184029750506\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1050] Loss: 0.2894198656858991\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1051] Loss: 0.28942328199686723\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1052] Loss: 0.28942966487106353\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1053] Loss: 0.2894277967780411\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1054] Loss: 0.28944343048789206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1055] Loss: 0.28946158779353254\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1056] Loss: 0.28946018134988544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1057] Loss: 0.28945895276498335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1058] Loss: 0.2894604719639841\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1059] Loss: 0.2894612733221512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1060] Loss: 0.2894617884494213\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1061] Loss: 0.2894647886387978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1062] Loss: 0.28946313579625876\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1063] Loss: 0.2894611636233783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1064] Loss: 0.28945011816297966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1065] Loss: 0.2894458937753807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1066] Loss: 0.2894384401954834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1067] Loss: 0.28943225887405094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1068] Loss: 0.28946491811814457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1069] Loss: 0.28948619790953084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1070] Loss: 0.28948808333674664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1071] Loss: 0.2894785514248851\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1072] Loss: 0.2894791381865772\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1073] Loss: 0.2894863038388854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1074] Loss: 0.2894811871362986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1075] Loss: 0.28947626488480305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1076] Loss: 0.2895163879519574\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1077] Loss: 0.28952115823700997\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1078] Loss: 0.289528404752827\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1079] Loss: 0.28951883680438323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1080] Loss: 0.2895200061546387\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1081] Loss: 0.28951741036378276\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1082] Loss: 0.2895103671938209\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1083] Loss: 0.2894993264592758\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1084] Loss: 0.2894999684020575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1085] Loss: 0.28949397581024616\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1086] Loss: 0.2895053191064951\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1087] Loss: 0.28950985268129714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1088] Loss: 0.2895028349927862\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1089] Loss: 0.2894997355005631\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1090] Loss: 0.28949191613546665\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1091] Loss: 0.2894890074494978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1092] Loss: 0.2894855943688775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1093] Loss: 0.28947651706455385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1094] Loss: 0.28947525670010343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1095] Loss: 0.28947384561733586\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1096] Loss: 0.2894692955474263\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1097] Loss: 0.289461890884584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1098] Loss: 0.2894669213381385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1099] Loss: 0.289470472315061\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1100] Loss: 0.2894750216772199\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1101] Loss: 0.28946154738403945\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1102] Loss: 0.28947219737361696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1103] Loss: 0.2894709619068869\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1104] Loss: 0.28946553390217805\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1105] Loss: 0.2894726449683384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1106] Loss: 0.2894678939036893\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1107] Loss: 0.28946904768840553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1108] Loss: 0.2894614456793771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1109] Loss: 0.2894557328211802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1110] Loss: 0.2894591429449784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1111] Loss: 0.2894629838513907\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1112] Loss: 0.28945944204769763\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1113] Loss: 0.2894567815988888\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1114] Loss: 0.28945081532242584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1115] Loss: 0.28944092027599416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1116] Loss: 0.2894399511266523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1117] Loss: 0.2894398874923403\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1118] Loss: 0.28943789873258624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1119] Loss: 0.28943629199137494\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1120] Loss: 0.28944032777130924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1121] Loss: 0.2894258370752139\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1122] Loss: 0.28941816947084203\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1123] Loss: 0.2894175857619753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1124] Loss: 0.28942866208560303\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1125] Loss: 0.2894283845769753\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1126] Loss: 0.2894191762002615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1127] Loss: 0.2894121649533745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1128] Loss: 0.289411646494108\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1129] Loss: 0.2894177250290641\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1130] Loss: 0.2894262540435751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1131] Loss: 0.2894361097894649\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1132] Loss: 0.2894506562358722\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1133] Loss: 0.28945448338064467\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1134] Loss: 0.2894518424326588\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1135] Loss: 0.28945939214201094\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1136] Loss: 0.28947071643374706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1137] Loss: 0.289474417527395\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1138] Loss: 0.28947205104356033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1139] Loss: 0.28947166751189446\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1140] Loss: 0.2894768505280745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1141] Loss: 0.28950451998743604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1142] Loss: 0.2894910322714062\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1143] Loss: 0.2894814149876675\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1144] Loss: 0.2894878010666529\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1145] Loss: 0.2894790872450313\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1146] Loss: 0.2894751870583457\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1147] Loss: 0.28949623108722017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1148] Loss: 0.2894925231763976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1149] Loss: 0.2894822237176969\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1150] Loss: 0.2894768466650638\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1151] Loss: 0.289468888036802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1152] Loss: 0.2894669736401369\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1153] Loss: 0.2894551455663992\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1154] Loss: 0.2894529244839079\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1155] Loss: 0.2894529934039637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1156] Loss: 0.28945277674856384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1157] Loss: 0.2894591935759728\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1158] Loss: 0.2894754727814351\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1159] Loss: 0.2894773053367001\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1160] Loss: 0.2894759288207898\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1161] Loss: 0.2894740351218865\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1162] Loss: 0.28947582085648305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1163] Loss: 0.2894810145970018\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1164] Loss: 0.2894970135324282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1165] Loss: 0.2894911638764195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1166] Loss: 0.28948772185518745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1167] Loss: 0.2894985729312612\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1168] Loss: 0.2894994931897045\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1169] Loss: 0.2895230425788074\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1170] Loss: 0.2895204039839709\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1171] Loss: 0.2895154442334702\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1172] Loss: 0.2895119430603854\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1173] Loss: 0.28950231888772404\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1174] Loss: 0.28949735467027654\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1175] Loss: 0.28948940912042137\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1176] Loss: 0.2894770203560399\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1177] Loss: 0.2894724424126453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1178] Loss: 0.2894665092171831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1179] Loss: 0.2894678252134493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1180] Loss: 0.28946155603400014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1181] Loss: 0.28945768352091195\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1182] Loss: 0.2894546762138801\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1183] Loss: 0.28945689281385106\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1184] Loss: 0.2894564594011664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1185] Loss: 0.28945113936353556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1186] Loss: 0.2894494695726152\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1187] Loss: 0.2894559954084716\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1188] Loss: 0.2894521633351508\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1189] Loss: 0.28944338694113875\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1190] Loss: 0.28944788348906636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1191] Loss: 0.289440738550015\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1192] Loss: 0.2894348632394388\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1193] Loss: 0.2894460282720711\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1194] Loss: 0.28944503379676206\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1195] Loss: 0.2894370905892624\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1196] Loss: 0.28944256317323\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1197] Loss: 0.28943638129648913\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1198] Loss: 0.289466826741378\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1199] Loss: 0.28948048707810664\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1200] Loss: 0.2894766057674191\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1201] Loss: 0.28949510570717646\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1202] Loss: 0.2895031490970706\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1203] Loss: 0.2894961554583297\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1204] Loss: 0.289503827794919\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1205] Loss: 0.28951173489816623\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1206] Loss: 0.289543088315384\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1207] Loss: 0.2895387320195214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1208] Loss: 0.28952751971286145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1209] Loss: 0.28953168098969184\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1210] Loss: 0.28952553885560345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1211] Loss: 0.2895140762179321\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1212] Loss: 0.2895180497177797\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1213] Loss: 0.28950616449011884\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1214] Loss: 0.28949815509520305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1215] Loss: 0.2894872604942577\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1216] Loss: 0.28948577377500223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1217] Loss: 0.28948726317342555\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1218] Loss: 0.28949160448341965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1219] Loss: 0.28948641275017367\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1220] Loss: 0.28948177505568806\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1221] Loss: 0.28947372425472984\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1222] Loss: 0.28947214778448394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1223] Loss: 0.28946443247817444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1224] Loss: 0.28946655674995075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1225] Loss: 0.2894584411320857\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1226] Loss: 0.28945555128770295\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1227] Loss: 0.2894645462044029\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1228] Loss: 0.28945542399537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1229] Loss: 0.2894482597528281\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1230] Loss: 0.289455281241881\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1231] Loss: 0.2894677286947079\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1232] Loss: 0.2894662982924234\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1233] Loss: 0.28947184738555276\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1234] Loss: 0.289461646257428\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1235] Loss: 0.28945370213378036\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1236] Loss: 0.2894515214459515\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1237] Loss: 0.2894463519487118\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1238] Loss: 0.2894427870049214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1239] Loss: 0.28947269335489745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1240] Loss: 0.2894855075969017\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1241] Loss: 0.2894993159668736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1242] Loss: 0.2895092466793075\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1243] Loss: 0.2895014143044686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1244] Loss: 0.28950077848669775\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1245] Loss: 0.28949281193423027\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1246] Loss: 0.28949221038597717\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1247] Loss: 0.2894830830007435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1248] Loss: 0.2894877892302151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1249] Loss: 0.28947629706261035\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1250] Loss: 0.2894750246632553\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1251] Loss: 0.2894777879860444\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1252] Loss: 0.28946656711357754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1253] Loss: 0.2894623347713022\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1254] Loss: 0.2894639909870478\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1255] Loss: 0.2894726946049421\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1256] Loss: 0.289469555247193\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1257] Loss: 0.2894714834088834\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1258] Loss: 0.28946874466199773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1259] Loss: 0.28947263325662986\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1260] Loss: 0.2894705396240082\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1261] Loss: 0.28947276877737055\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1262] Loss: 0.28947603329071264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1263] Loss: 0.28947133694580285\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1264] Loss: 0.28946710513676954\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1265] Loss: 0.28945453749285327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1266] Loss: 0.2894565056421307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1267] Loss: 0.28945261056661653\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1268] Loss: 0.2894599819687611\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1269] Loss: 0.2894614376672011\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1270] Loss: 0.2894613310823924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1271] Loss: 0.2894823193713127\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1272] Loss: 0.2894796035416512\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1273] Loss: 0.2894820983781352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1274] Loss: 0.2894829019349592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1275] Loss: 0.28948842082948995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1276] Loss: 0.2894789163300469\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1277] Loss: 0.2894719071433394\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1278] Loss: 0.28946951567999507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1279] Loss: 0.28946692359885406\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1280] Loss: 0.28946858900388084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1281] Loss: 0.2894559550957635\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8886\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1282] Loss: 0.28945250725955934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1283] Loss: 0.28944290588515953\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1284] Loss: 0.28943866341749974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1285] Loss: 0.2894323066567151\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1286] Loss: 0.2894198946693524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1287] Loss: 0.28941221317055305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1288] Loss: 0.28940479288588583\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1289] Loss: 0.289430524711328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1290] Loss: 0.28942272861269136\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1291] Loss: 0.28942627008341987\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1292] Loss: 0.28943358389514307\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1293] Loss: 0.28943675026135784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1294] Loss: 0.28942781107767424\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1295] Loss: 0.28945369027155543\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1296] Loss: 0.289447424757668\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1297] Loss: 0.2894542604047782\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1298] Loss: 0.28945459216090935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1299] Loss: 0.2894591832237941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1300] Loss: 0.28946245944650767\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1301] Loss: 0.28945069326917783\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1302] Loss: 0.2894520266887955\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1303] Loss: 0.28945728545003063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1304] Loss: 0.28946282813688534\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1305] Loss: 0.2894629626057906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1306] Loss: 0.28945678103510397\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1307] Loss: 0.2894512187225214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1308] Loss: 0.28945287083921867\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1309] Loss: 0.28945268446282\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1310] Loss: 0.2894709599361131\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1311] Loss: 0.28946778399387874\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1312] Loss: 0.2894694467797328\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1313] Loss: 0.2894739230202239\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1314] Loss: 0.2894634944443153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1315] Loss: 0.28945550638593864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1316] Loss: 0.28945204988320195\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1317] Loss: 0.2894405935269575\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1318] Loss: 0.28946089547874126\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1319] Loss: 0.2894754978479544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1320] Loss: 0.28947601566150627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1321] Loss: 0.2894722864910545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1322] Loss: 0.2894681487824352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1323] Loss: 0.2894640059469704\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1324] Loss: 0.2894614487193075\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1325] Loss: 0.2894626414213279\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1326] Loss: 0.2894516624099283\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1327] Loss: 0.2894473039769981\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1328] Loss: 0.28946266491174033\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1329] Loss: 0.28945583665427976\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1330] Loss: 0.28944967851191267\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1331] Loss: 0.2894378973454639\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1332] Loss: 0.28942800341943714\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1333] Loss: 0.28943676921978095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1334] Loss: 0.28944910521666906\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1335] Loss: 0.28945021398120524\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1336] Loss: 0.289459366236559\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1337] Loss: 0.2894539788316871\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1338] Loss: 0.28945723347992264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1339] Loss: 0.28945290123658773\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1340] Loss: 0.28944173031554593\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1341] Loss: 0.289439536337754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1342] Loss: 0.2894326099417802\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1343] Loss: 0.28942966579199864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1344] Loss: 0.2894321808594453\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1345] Loss: 0.28943821022746336\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1346] Loss: 0.28943250429384204\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1347] Loss: 0.2894449263866495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1348] Loss: 0.28946047551279236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1349] Loss: 0.28945780988928255\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1350] Loss: 0.28946575613886455\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1351] Loss: 0.28946244428528145\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1352] Loss: 0.2894643219642642\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1353] Loss: 0.28945672726481686\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1354] Loss: 0.28945938020912965\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1355] Loss: 0.2894576375529936\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1356] Loss: 0.2894697453204961\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1357] Loss: 0.2894674615915095\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1358] Loss: 0.28945767180481485\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1359] Loss: 0.28945707725299846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1360] Loss: 0.28946591842850417\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1361] Loss: 0.2894568186645822\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1362] Loss: 0.2894509424395072\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1363] Loss: 0.2894694402073941\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1364] Loss: 0.2894622065281005\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1365] Loss: 0.2894636032940498\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1366] Loss: 0.2894534371653998\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1367] Loss: 0.28944979070265975\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1368] Loss: 0.28944334230155605\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1369] Loss: 0.2894463885141751\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1370] Loss: 0.28944956070544153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1371] Loss: 0.2894466065636102\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1372] Loss: 0.2894452506241386\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1373] Loss: 0.28944714127774507\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1374] Loss: 0.2894508257830113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1375] Loss: 0.2894506422799721\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1376] Loss: 0.28945631375926223\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1377] Loss: 0.2894532131358725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1378] Loss: 0.2894591380338556\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1379] Loss: 0.2894668190412091\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1380] Loss: 0.2894801892280973\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1381] Loss: 0.2894727887341411\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1382] Loss: 0.2894858580293316\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1383] Loss: 0.2894751381794759\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1384] Loss: 0.2894702825746578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1385] Loss: 0.2894744823859725\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1386] Loss: 0.2894829337535813\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1387] Loss: 0.289484288587603\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1388] Loss: 0.2894783864576475\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1389] Loss: 0.2894812581856172\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1390] Loss: 0.28947110889685385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1391] Loss: 0.28945862384314935\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1392] Loss: 0.28945788279690476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1393] Loss: 0.28945563960698917\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1394] Loss: 0.2894633833352963\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1395] Loss: 0.2894656066094216\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1396] Loss: 0.28945649499884435\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1397] Loss: 0.2894542415093996\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1398] Loss: 0.2894628177108525\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1399] Loss: 0.289454795163916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1400] Loss: 0.2894551415168853\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1401] Loss: 0.2894496590960405\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1402] Loss: 0.28945272088379964\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1403] Loss: 0.28945905111613335\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1404] Loss: 0.2894483189418995\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1405] Loss: 0.2894480493989858\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1406] Loss: 0.28945669574330807\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1407] Loss: 0.28945145379512305\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1408] Loss: 0.28944554163014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1409] Loss: 0.2894387063363175\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1410] Loss: 0.28943625939154344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1411] Loss: 0.2894340501271578\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1412] Loss: 0.28942345080839604\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1413] Loss: 0.28943115547284615\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1414] Loss: 0.2894218821731831\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1415] Loss: 0.28941928653983656\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1416] Loss: 0.28941892983783707\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1417] Loss: 0.2894410045800636\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1418] Loss: 0.28943972119318084\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1419] Loss: 0.2894271367733785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1420] Loss: 0.28942677877115053\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1421] Loss: 0.28944320637740373\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1422] Loss: 0.28945607480964153\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1423] Loss: 0.28944785249850824\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1424] Loss: 0.289452462720823\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1425] Loss: 0.2894485861776331\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1426] Loss: 0.2894437850622548\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1427] Loss: 0.28943481142340916\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1428] Loss: 0.2894341007149492\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1429] Loss: 0.2894435349001736\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1430] Loss: 0.28944538451804974\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1431] Loss: 0.2894556462055629\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1432] Loss: 0.28944444713706924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1433] Loss: 0.2894418399668146\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1434] Loss: 0.2894321737150427\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1435] Loss: 0.28943987655604253\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1436] Loss: 0.289435784210785\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1437] Loss: 0.2894300096183809\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1438] Loss: 0.28941744485451465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1439] Loss: 0.2894075706545112\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1440] Loss: 0.289399167044103\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1441] Loss: 0.28942384759595546\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1442] Loss: 0.28941034975458835\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1443] Loss: 0.28941146279870383\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1444] Loss: 0.2894140651026523\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1445] Loss: 0.28940282615839436\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1446] Loss: 0.2894151011320008\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1447] Loss: 0.2894078665001859\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1448] Loss: 0.28940617581456113\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1449] Loss: 0.28940263326983484\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1450] Loss: 0.2893911592706537\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1451] Loss: 0.28940481439583476\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1452] Loss: 0.28939216599976236\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1453] Loss: 0.2893936391989825\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1454] Loss: 0.2893897674853063\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1455] Loss: 0.2893921681634677\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1456] Loss: 0.289384910237863\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1457] Loss: 0.2893917777786584\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1458] Loss: 0.2893803791497784\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1459] Loss: 0.28938739604217684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1460] Loss: 0.28938993880321545\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1461] Loss: 0.28938259218420864\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1462] Loss: 0.2893833213762684\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1463] Loss: 0.2893744970780465\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1464] Loss: 0.28938261383223896\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1465] Loss: 0.2893883416575745\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1466] Loss: 0.2893794331840418\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1467] Loss: 0.28938527868476466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1468] Loss: 0.2893836799737244\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1469] Loss: 0.28937954038128466\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1470] Loss: 0.2893739170811143\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1471] Loss: 0.28938846011726344\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1472] Loss: 0.2893813771066727\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1473] Loss: 0.2893699535178454\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1474] Loss: 0.28937178671681696\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1475] Loss: 0.28936966229285327\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1476] Loss: 0.28936637884580046\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1477] Loss: 0.2893547396750217\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1478] Loss: 0.28937437258938264\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1479] Loss: 0.2893964461264104\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1480] Loss: 0.28939475806528014\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1481] Loss: 0.2893907305214187\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1482] Loss: 0.28939550757847277\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1483] Loss: 0.28938823875248343\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1484] Loss: 0.289381487216135\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1485] Loss: 0.28938812223930416\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1486] Loss: 0.2893851051901056\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1487] Loss: 0.2893830721887633\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1488] Loss: 0.2893812667249238\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1489] Loss: 0.28937521155584855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1490] Loss: 0.2893749834165065\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1491] Loss: 0.2893678928105744\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1492] Loss: 0.28936540209640493\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1493] Loss: 0.28936019050082157\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1494] Loss: 0.2893617397498188\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1495] Loss: 0.2893552858295544\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1496] Loss: 0.28935384165816846\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1497] Loss: 0.2893544156504592\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1498] Loss: 0.2893541454025301\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1499] Loss: 0.2893435297424576\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1500] Loss: 0.28933531105537924\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1501] Loss: 0.2893312567000978\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1502] Loss: 0.2893261501244734\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1503] Loss: 0.2893241121753211\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1504] Loss: 0.28931943248982844\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1505] Loss: 0.28931282020652627\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1506] Loss: 0.2893085884199214\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1507] Loss: 0.289306804898877\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1508] Loss: 0.28931012468636114\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1509] Loss: 0.28931581485967345\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1510] Loss: 0.28931540120062826\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1511] Loss: 0.28930606334906495\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1512] Loss: 0.28930627740136966\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1513] Loss: 0.289302242004315\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1514] Loss: 0.28928980524681663\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1515] Loss: 0.28927885510753637\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1516] Loss: 0.28927511658131233\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1517] Loss: 0.2892739590803352\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1518] Loss: 0.2892663555405814\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1519] Loss: 0.2892669043788189\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1520] Loss: 0.2892679088604771\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1521] Loss: 0.28925759498107917\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1522] Loss: 0.2892579230211168\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1523] Loss: 0.28924507301007246\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 19, Batch 1524] Loss: 0.28924065201788385\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 0] Loss: 0.28923725410391754\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 1] Loss: 0.2892350733664293\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 2] Loss: 0.2892375933981891\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 3] Loss: 0.28923290889028064\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 4] Loss: 0.28924963109869983\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 5] Loss: 0.289236143782002\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 6] Loss: 0.289231818350356\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 7] Loss: 0.2892979353430849\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 8] Loss: 0.28929412640112934\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 9] Loss: 0.28930014795295855\n",
      "\n",
      "CUDA Memory Allocated: 3644769792\n",
      "[Epoch 20, Batch 10] Loss: 0.2892962989187639\n",
      "\n",
      "*********** Finished Training this Epoch in 3097.3982696533203 seconds ***********\n"
     ]
    }
   ],
   "source": [
    "learn_weights = True\n",
    "\n",
    "print(\"Pre-Training CUDA Memory Allocation:\", torch.cuda.max_memory_allocated())\n",
    "\n",
    "if learn_weights:\n",
    "\n",
    "    # set start time for cnn training\n",
    "    start_time = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net.forward(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels.unsqueeze(-1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_sched.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # update mini-batch count\n",
    "        mini_batch += 1\n",
    "        epoch = mini_batch // 1525\n",
    "\n",
    "        # print every mini-batch\n",
    "        print(\"CUDA Memory Allocated:\", torch.cuda.max_memory_allocated())\n",
    "        print(f'[Epoch {epoch}, Batch {mini_batch % 1525}] Loss: {running_loss / (i+1)}\\n')\n",
    "\n",
    "        # save and outoput every 100 mini-batch\n",
    "        if i % 100 == 0:\n",
    "            print(\"*********** Saving network weights and optimizer state *********** \\n\\n\")\n",
    "            # save the weights and optimizer\n",
    "            torch.save({'mini_batch': mini_batch,\n",
    "                        'model_state_dict': net.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'lr_sched': lr_sched.state_dict()}, PATH)\n",
    "            \n",
    "        # eval every 500 mini-batch\n",
    "        if i % 500 == 0:\n",
    "            \n",
    "            print(\"******************************************************************\")\n",
    "            print(\"*********************** Performance Update ***********************\")\n",
    "            print(\"******************************************************************\\n\")\n",
    "            \n",
    "            net.eval()\n",
    "            \n",
    "            ground_truths = []\n",
    "            probs = []\n",
    "\n",
    "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "            with torch.no_grad():\n",
    "                for j, valdata in enumerate(val_loader, 0):\n",
    "                    image, label = valdata\n",
    "                    image = image.to(device)\n",
    "\n",
    "                    # save for analysis\n",
    "                    ground_truths.append(label)\n",
    "\n",
    "                    # calculate outputs by running images through the network \n",
    "                    outputs = net(image)\n",
    "                    outputs = outputs.to(\"cpu\")\n",
    "\n",
    "                    # # save for analysis\n",
    "                    probs.append(outputs)\n",
    "\n",
    "            print(\"Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs))\n",
    "            \n",
    "            net.train()\n",
    "\n",
    "            print(\"\\n******************************************************************\")\n",
    "            print(\"****************** Performance Update Complete! ******************\")\n",
    "            print(\"******************************************************************\\n\\n\")\n",
    "\n",
    "        # save unique set of weights and optimizer for validation later\n",
    "        if mini_batch % 1525 == 0:\n",
    "\n",
    "            uPATH = f'./saved_weights3_tmp/melanoma_ResNet152_{epoch}e_{mini_batch % 1525}b.pth'\n",
    "            torch.save({'mini_batch': mini_batch,\n",
    "                        'model_state_dict': net.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'lr_sched': lr_sched.state_dict()}, uPATH)\n",
    "\n",
    "    print('*********** Finished Training this Epoch in', time.time() - start_time, 'seconds ***********')\n",
    "    \n",
    "    # save the weights and optimizer\n",
    "    torch.save({'mini_batch': mini_batch,\n",
    "                'model_state_dict': net.state_dict(), \n",
    "                'optimizer_state_dict': optimizer.state_dict(), \n",
    "                'lr_sched': lr_sched.state_dict()}, PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998de40",
   "metadata": {},
   "source": [
    "# Formally test performance on our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f63c0",
   "metadata": {},
   "source": [
    "First, let us see what the convolutional neural network thinks of a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b172eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  Benign Benign Benign Benign Benign Benign Benign Benign Malignant Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign Benign\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "test_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data256x256\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data256x256\", \"val.csv\"), \n",
    "                            num_samples=8281, up_sample=False, start_ind=0, transform=val_transf)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate2d, \n",
    "                         num_workers=n_workers)\n",
    "\n",
    "\n",
    "\n",
    "testiter = iter(test_loader)\n",
    "images, labels = next(testiter)\n",
    "\n",
    "# print images\n",
    "print('GroundTruth: ', ' '.join('%5s' % label_id[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072622c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_weights | create_new_weights:\n",
    "    \n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % label_id[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9271d8",
   "metadata": {},
   "source": [
    "Fortunately, we saved weights off at different epoch/batch values. Here is the list of saved weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc00fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['melanoma_ResNet152_12e_0b.pth',\n",
       " 'melanoma_ResNet152_14e_0b.pth',\n",
       " 'melanoma_ResNet152_16e_0b.pth',\n",
       " 'melanoma_ResNet152_18e_0b.pth',\n",
       " 'melanoma_ResNet152_20e_0b.pth']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./saved_weights3_tmp/')[11::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b2cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: melanoma_ResNet152_12e_0b.pth\n",
      "\n",
      "\t Processing Batch #0 ... Running Time 0.36078786849975586\n",
      "\t Current Testing Loss: 0.3082646429538727\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 1.270301342010498\n",
      "\t Current Testing Loss: 0.2294628918170929\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 2.1617588996887207\n",
      "\t Current Testing Loss: 0.22301614497389113\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 3.054448366165161\n",
      "\t Current Testing Loss: 0.20980459259402368\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 3.949279308319092\n",
      "\t Current Testing Loss: 0.21778727122923222\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 4.8433332443237305\n",
      "\t Current Testing Loss: 0.22222631526928321\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 5.737222671508789\n",
      "\t Current Testing Loss: 0.22043724748931948\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 6.632286071777344\n",
      "\t Current Testing Loss: 0.22361057954774777\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 7.5260233879089355\n",
      "\t Current Testing Loss: 0.22532283036429204\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 8.421087980270386\n",
      "\t Current Testing Loss: 0.22812072482410367\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 9.31743311882019\n",
      "\t Current Testing Loss: 0.2279885801495892\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 10.212355613708496\n",
      "\t Current Testing Loss: 0.22446232557565243\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 11.107511520385742\n",
      "\t Current Testing Loss: 0.2232445815254834\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 12.003877639770508\n",
      "\t Current Testing Loss: 0.22370561837922526\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 12.901575326919556\n",
      "\t Current Testing Loss: 0.22388487673820334\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 13.79967451095581\n",
      "\t Current Testing Loss: 0.2231781142732955\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 14.69743275642395\n",
      "\t Current Testing Loss: 0.2244060896105648\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 15.59640884399414\n",
      "\t Current Testing Loss: 0.2212897046354779\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 16.4947669506073\n",
      "\t Current Testing Loss: 0.22319302135738878\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 17.395090341567993\n",
      "\t Current Testing Loss: 0.22689954347479405\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 18.294513940811157\n",
      "\t Current Testing Loss: 0.22571975982456066\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 19.195326805114746\n",
      "\t Current Testing Loss: 0.22600884258888343\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 20.096657276153564\n",
      "\t Current Testing Loss: 0.22720871245429528\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 20.99879503250122\n",
      "\t Current Testing Loss: 0.22770080109050264\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 21.900439262390137\n",
      "\t Current Testing Loss: 0.22804879595011596\n",
      "\n",
      "\t Processing Batch #250 ... Running Time 22.801728010177612\n",
      "\t Current Testing Loss: 0.22686570517689109\n",
      "\n",
      "******* Final Testing Loss: 0.22716567236714383 *******\n",
      "\n",
      "Loading: melanoma_ResNet152_14e_0b.pth\n",
      "\n",
      "\t Processing Batch #0 ... Running Time 0.29945898056030273\n",
      "\t Current Testing Loss: 0.22778883576393127\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 1.2149386405944824\n",
      "\t Current Testing Loss: 0.19472810084169562\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 2.1121647357940674\n",
      "\t Current Testing Loss: 0.18985459563278018\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 3.012070655822754\n",
      "\t Current Testing Loss: 0.17757399981060334\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 3.911121129989624\n",
      "\t Current Testing Loss: 0.18810324116450985\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 4.809308052062988\n",
      "\t Current Testing Loss: 0.19829938400025462\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 5.71014142036438\n",
      "\t Current Testing Loss: 0.19488370919325312\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 6.6094069480896\n",
      "\t Current Testing Loss: 0.19851225131834058\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 7.510952711105347\n",
      "\t Current Testing Loss: 0.1987930466823372\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 8.412253379821777\n",
      "\t Current Testing Loss: 0.20108358656148334\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 9.313971996307373\n",
      "\t Current Testing Loss: 0.2004239068246714\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 10.214157819747925\n",
      "\t Current Testing Loss: 0.19662618244419228\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 11.11533260345459\n",
      "\t Current Testing Loss: 0.1955794455283437\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 12.017880916595459\n",
      "\t Current Testing Loss: 0.19636785412448962\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 12.920198917388916\n",
      "\t Current Testing Loss: 0.19526711325590493\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 13.822033643722534\n",
      "\t Current Testing Loss: 0.19414720726230286\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 14.724019527435303\n",
      "\t Current Testing Loss: 0.19590239168971962\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 15.62706208229065\n",
      "\t Current Testing Loss: 0.19329022587827074\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 16.529133319854736\n",
      "\t Current Testing Loss: 0.19562907107380215\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 17.43264675140381\n",
      "\t Current Testing Loss: 0.19926853737359895\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 18.3354549407959\n",
      "\t Current Testing Loss: 0.19804282191751607\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 19.237626791000366\n",
      "\t Current Testing Loss: 0.19819629603723213\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 20.141056060791016\n",
      "\t Current Testing Loss: 0.19963299707252516\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 21.04450035095215\n",
      "\t Current Testing Loss: 0.20055086876858363\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 21.949015378952026\n",
      "\t Current Testing Loss: 0.20078055602324454\n",
      "\n",
      "\t Processing Batch #250 ... Running Time 22.852771282196045\n",
      "\t Current Testing Loss: 0.19968749046978723\n",
      "\n",
      "******* Final Testing Loss: 0.20027715429433524 *******\n",
      "\n",
      "Loading: melanoma_ResNet152_16e_0b.pth\n",
      "\n",
      "\t Processing Batch #0 ... Running Time 0.3341984748840332\n",
      "\t Current Testing Loss: 0.2897292971611023\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 1.2415528297424316\n",
      "\t Current Testing Loss: 0.2505067099224437\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 2.146087408065796\n",
      "\t Current Testing Loss: 0.24306370814641318\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 3.050173044204712\n",
      "\t Current Testing Loss: 0.2282414914619538\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 3.955683708190918\n",
      "\t Current Testing Loss: 0.23897513183878688\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 4.861406326293945\n",
      "\t Current Testing Loss: 0.24607704623657115\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 5.767695426940918\n",
      "\t Current Testing Loss: 0.24210378710852296\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 6.674335956573486\n",
      "\t Current Testing Loss: 0.24512032545368437\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 7.580147981643677\n",
      "\t Current Testing Loss: 0.24597054333598525\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 8.488169431686401\n",
      "\t Current Testing Loss: 0.24856935408744182\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 9.395846843719482\n",
      "\t Current Testing Loss: 0.24871671804697207\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 10.303077936172485\n",
      "\t Current Testing Loss: 0.24458355852612504\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 11.211576461791992\n",
      "\t Current Testing Loss: 0.24307809499177066\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 12.118866682052612\n",
      "\t Current Testing Loss: 0.24488004564328958\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 13.026244878768921\n",
      "\t Current Testing Loss: 0.24464003329581402\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 13.93297290802002\n",
      "\t Current Testing Loss: 0.24347965695605372\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 14.840127229690552\n",
      "\t Current Testing Loss: 0.24564597010612488\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 15.74834132194519\n",
      "\t Current Testing Loss: 0.24274150038148926\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 16.657113075256348\n",
      "\t Current Testing Loss: 0.24456938825424204\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 17.56545114517212\n",
      "\t Current Testing Loss: 0.24884407639659512\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 18.47372531890869\n",
      "\t Current Testing Loss: 0.2477996648099292\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 19.382056713104248\n",
      "\t Current Testing Loss: 0.24819880095420854\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 20.28950786590576\n",
      "\t Current Testing Loss: 0.24938661996190903\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 21.19754648208618\n",
      "\t Current Testing Loss: 0.25075273322207586\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 22.10601496696472\n",
      "\t Current Testing Loss: 0.2519362345956173\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #250 ... Running Time 23.014517545700073\n",
      "\t Current Testing Loss: 0.2510249296804348\n",
      "\n",
      "******* Final Testing Loss: 0.2513790373118688 *******\n",
      "\n",
      "Loading: melanoma_ResNet152_18e_0b.pth\n",
      "\n",
      "\t Processing Batch #0 ... Running Time 0.37511491775512695\n",
      "\t Current Testing Loss: 0.26150208711624146\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 1.2896606922149658\n",
      "\t Current Testing Loss: 0.23224505511197177\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 2.196194648742676\n",
      "\t Current Testing Loss: 0.23202795464368092\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 3.1033287048339844\n",
      "\t Current Testing Loss: 0.2157557017860874\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 4.009664297103882\n",
      "\t Current Testing Loss: 0.22522815298743365\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 4.918262481689453\n",
      "\t Current Testing Loss: 0.23287717501322427\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 5.826731204986572\n",
      "\t Current Testing Loss: 0.2313623650640738\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 6.7331788539886475\n",
      "\t Current Testing Loss: 0.2366573686750842\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 7.640309810638428\n",
      "\t Current Testing Loss: 0.23790565684989648\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 8.548125982284546\n",
      "\t Current Testing Loss: 0.23952844607961046\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 9.456482172012329\n",
      "\t Current Testing Loss: 0.24044620695680674\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 10.364237785339355\n",
      "\t Current Testing Loss: 0.23711774115626877\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 11.273604393005371\n",
      "\t Current Testing Loss: 0.2360186342858086\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 12.1832594871521\n",
      "\t Current Testing Loss: 0.23752727399345572\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 13.092335224151611\n",
      "\t Current Testing Loss: 0.23733184856514558\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 14.000542879104614\n",
      "\t Current Testing Loss: 0.2359631669935801\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 14.90855860710144\n",
      "\t Current Testing Loss: 0.23813288455238993\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 15.817904472351074\n",
      "\t Current Testing Loss: 0.2355020947275106\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 16.730018854141235\n",
      "\t Current Testing Loss: 0.23801400821182608\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 17.641242265701294\n",
      "\t Current Testing Loss: 0.2421562934577153\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 18.551209688186646\n",
      "\t Current Testing Loss: 0.2409942664984447\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 19.46050786972046\n",
      "\t Current Testing Loss: 0.24077245628381794\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 20.369582176208496\n",
      "\t Current Testing Loss: 0.24157579597169998\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 21.280382871627808\n",
      "\t Current Testing Loss: 0.24215784227048165\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 22.190396070480347\n",
      "\t Current Testing Loss: 0.24296095389177197\n",
      "\n",
      "\t Processing Batch #250 ... Running Time 23.100826740264893\n",
      "\t Current Testing Loss: 0.24161707176511507\n",
      "\n",
      "******* Final Testing Loss: 0.24169248937985152 *******\n",
      "\n",
      "Loading: melanoma_ResNet152_20e_0b.pth\n",
      "\n",
      "\t Processing Batch #0 ... Running Time 0.36704373359680176\n",
      "\t Current Testing Loss: 0.23509541153907776\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 1.3000783920288086\n",
      "\t Current Testing Loss: 0.20287104763767935\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 2.2091729640960693\n",
      "\t Current Testing Loss: 0.20723946392536163\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 3.1192824840545654\n",
      "\t Current Testing Loss: 0.19044300865742467\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 4.029603481292725\n",
      "\t Current Testing Loss: 0.19839357057722604\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 4.937288761138916\n",
      "\t Current Testing Loss: 0.20763217906157175\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 5.847612380981445\n",
      "\t Current Testing Loss: 0.20537737280618948\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 6.7584388256073\n",
      "\t Current Testing Loss: 0.20915375335115782\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 7.668479681015015\n",
      "\t Current Testing Loss: 0.21031828693769597\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 8.578787326812744\n",
      "\t Current Testing Loss: 0.21101160126400517\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 9.489591598510742\n",
      "\t Current Testing Loss: 0.21149646136725303\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 10.401384353637695\n",
      "\t Current Testing Loss: 0.20793969331829398\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 11.313396453857422\n",
      "\t Current Testing Loss: 0.20627983875018507\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 12.225866556167603\n",
      "\t Current Testing Loss: 0.20661632511906952\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 13.137210130691528\n",
      "\t Current Testing Loss: 0.20641784887787298\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 14.049205780029297\n",
      "\t Current Testing Loss: 0.2056733215881499\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 14.961557388305664\n",
      "\t Current Testing Loss: 0.20794826255451818\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 15.87496542930603\n",
      "\t Current Testing Loss: 0.20497756368584102\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 16.787839889526367\n",
      "\t Current Testing Loss: 0.20757698572307662\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 17.699238061904907\n",
      "\t Current Testing Loss: 0.2113743011317952\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 18.611249685287476\n",
      "\t Current Testing Loss: 0.21133772202243853\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 19.524096727371216\n",
      "\t Current Testing Loss: 0.21143817650904587\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 20.43760871887207\n",
      "\t Current Testing Loss: 0.2127693152535555\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 21.350839138031006\n",
      "\t Current Testing Loss: 0.21372423614516403\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 22.264298677444458\n",
      "\t Current Testing Loss: 0.214332693778133\n",
      "\n",
      "\t Processing Batch #250 ... Running Time 23.177290678024292\n",
      "\t Current Testing Loss: 0.2135142369633652\n",
      "\n",
      "******* Final Testing Loss: 0.2143445352387244 *******\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_fnames = os.listdir('./saved_weights3_tmp/')\n",
    "#weight_fnames.sort() # isnt perfectly sorted, but too lazy to add the code (not important)\n",
    "batch_sizes = []\n",
    "losses = []\n",
    "\n",
    "for fname in weight_fnames[11::2]:\n",
    "    \n",
    "    print(f'Loading: {fname}\\n')\n",
    "\n",
    "    checkpoint = torch.load(f'./saved_weights3_tmp/{fname}', map_location=device)\n",
    "    \n",
    "    # network weights load\n",
    "    net = torchvision.models.resnet152(weights='IMAGENET1K_V2').to(device)\n",
    "    \n",
    "    # for feature extraction\n",
    "    #for param in net.parameters():\n",
    "        #param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 300),\n",
    "               nn.BatchNorm1d(300),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(300, 100),\n",
    "               nn.BatchNorm1d(100),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(100, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])  \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # set start time for cnn training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ground_truths = []\n",
    "    probs = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i, testdata in enumerate(test_loader, 0):\n",
    "            \n",
    "            image, label = testdata\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = net(image)\n",
    "            \n",
    "            loss = criterion(outputs, label.unsqueeze(-1).float())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # everything saved should be on RAM\n",
    "            outputs = outputs.to(\"cpu\")\n",
    "            label = label.to(\"cpu\")\n",
    "            \n",
    "            # save for analysis\n",
    "            ground_truths.append(label)\n",
    "            \n",
    "            # # save for analysis\n",
    "            probs += outputs.squeeze(-1).tolist()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"\\t Processing Batch #{i} ... Running Time {time.time() - start_time}\")\n",
    "                print(f'\\t Current Testing Loss: {running_loss / (i+1)}\\n')\n",
    "\n",
    "                \n",
    "    print(f'******* Final Testing Loss: {running_loss / (i+1)} *******\\n')\n",
    "\n",
    "    batch_sizes.append(checkpoint['mini_batch'])\n",
    "    losses.append(running_loss / (i+1))\n",
    "                \n",
    "    # Save ground-truths and probability results\n",
    "    res = {}\n",
    "    res[\"ground_truths\"] = ground_truths\n",
    "    res[\"probs\"] = probs\n",
    "    res[\"num_batches\"] = checkpoint['mini_batch']\n",
    "    res[\"testing_loss\"] = running_loss / (i+1)\n",
    "\n",
    "    pkl_f_name = f'./saved_results3_tmp/results_ResNet152_{checkpoint[\"mini_batch\"]}b.pkl'\n",
    "    with open(pkl_f_name, 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "\n",
    "        \n",
    "plt.plot(batch_sizes, losses, 'o')\n",
    "plt.title(\"Testing Loss Over Time\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Overall Testing Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f86c7e",
   "metadata": {},
   "source": [
    "## Choose the results from the best performing model (training size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d8d6ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1525] Size Area Under the ROC Curve: 0.8572557619064345 \n",
      "\n",
      "[Batch 4575] Size Area Under the ROC Curve: 0.8714466361907438 \n",
      "\n",
      "[Batch 7625] Size Area Under the ROC Curve: 0.8655078456265713 \n",
      "\n",
      "[Batch 18300] Size Area Under the ROC Curve: 0.8639631411945157 \n",
      "\n",
      "[Batch 10675] Size Area Under the ROC Curve: 0.8671370195484144 \n",
      "\n",
      "[Batch 13725] Size Area Under the ROC Curve: 0.8738285085364366 \n",
      "\n",
      "[Batch 15250] Size Area Under the ROC Curve: 0.8699303670324782 \n",
      "\n",
      "[Batch 21350] Size Area Under the ROC Curve: 0.876193654250488 \n",
      "\n",
      "[Batch 24400] Size Area Under the ROC Curve: 0.8731728245761052 \n",
      "\n",
      "[Batch 27450] Size Area Under the ROC Curve: 0.8684784954060306 \n",
      "\n",
      "[Batch 30500] Size Area Under the ROC Curve: 0.8603878236812306 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "# in case pkl results were calculated in batch job\n",
    "# we may wont to visualize the test plot over time\n",
    "recalc_loss_plot = True\n",
    "\n",
    "if recalc_loss_plot:\n",
    "    \n",
    "    batch_sizes = []\n",
    "    losses = []\n",
    "    res_fnames = os.listdir('./saved_results3_tmp/')\n",
    "    \n",
    "    for fname in res_fnames:\n",
    "        with open(f'./saved_results3_tmp/{fname}', 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            batch_sizes.append(res[\"num_batches\"])\n",
    "            losses.append(res[\"testing_loss\"]) \n",
    "            \n",
    "            \n",
    "            gt = res[\"ground_truths\"]\n",
    "            probs = np.array(res[\"probs\"])\n",
    "\n",
    "            # match formats (shouldve done this before, forgot to check)\n",
    "            ground_truths = []\n",
    "            for i in range(len(gt)):\n",
    "                if gt[i].size() > torch.Size([1]):\n",
    "                    ground_truths += gt[i].squeeze(-1).tolist()\n",
    "                else:\n",
    "                    ground_truths.append(gt[i].squeeze(-1).tolist())\n",
    "\n",
    "            ground_truths = np.array(ground_truths)\n",
    "            print(f\"[Batch {res['num_batches']}] Size Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs), \"\\n\")\n",
    "            \n",
    "    \n",
    "    plt.plot(batch_sizes, losses, 'o')\n",
    "    plt.title(\"Testing Loss Over Time\")\n",
    "    plt.xlabel(\"Batch Number\")\n",
    "    plt.ylabel(\"Overall Testing Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b21dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_batch_size = 21350\n",
    "\n",
    "\n",
    "with open(f'./saved_results3/results_ResNet50_{best_batch_size}b.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)\n",
    "    \n",
    "    \n",
    "gt = res[\"ground_truths\"]\n",
    "probs = np.array(res[\"probs\"])\n",
    "\n",
    "\n",
    "# match formats (shouldve done this before, forgot to check)\n",
    "ground_truths = []\n",
    "for i in range(len(gt)):\n",
    "    if gt[i].size() > torch.Size([1]):\n",
    "        ground_truths += gt[i].squeeze(-1).tolist()\n",
    "    else:\n",
    "        ground_truths.append(gt[i].squeeze(-1).tolist())\n",
    "        \n",
    "ground_truths = np.array(ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc478afa",
   "metadata": {},
   "source": [
    "## Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13c8fc4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F2-Score is: 0.34424853064651556\n",
      "Max G-Mean is: 0.8065287772568284\n",
      "Max Cohen's Kappa is: 0.2412570888468809\n",
      "Area Under the ROC Curve: 0.8871136357173802 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************* USING F2-SCORE OPTIMAL THRESHOLD *************************\n",
      "The confusion matrix is:\n",
      " [[7613  521]\n",
      " [  65   82]] \n",
      "\n",
      "Recall / Sensitivity: 0.564625850340136\n",
      "Precision: 0.13598673300165837\n",
      "Specificity: 0.9359478731251537\n",
      "F2-Score: 0.34424853064651556\n",
      "G-Mean: 0.7269527933348434\n",
      "Cohen's Kappa: 0.1957086703700095\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "********************** USING G-MEAN OPTIMAL THRESHOLD **************************\n",
      "The confusion matrix is:\n",
      " [[6428 1706]\n",
      " [  27  120]] \n",
      "\n",
      "Recall / Sensitivity: 0.8231292517006803\n",
      "Precision: 0.06571741511500548\n",
      "Specificity: 0.7902630931890828\n",
      "F2-Score: 0.24855012427506212\n",
      "G-Mean: 0.8065287772568284\n",
      "Cohen's Kappa: 0.09180075725391579\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "********************** USING KAPPA OPTIMAL THRESHOLD ***************************\n",
      "The confusion matrix is:\n",
      " [[7924  210]\n",
      " [  94   53]] \n",
      "\n",
      "Recall / Sensitivity: 0.36054421768707484\n",
      "Precision: 0.20152091254752852\n",
      "Specificity: 0.9741824440619621\n",
      "F2-Score: 0.31139835487661577\n",
      "G-Mean: 0.5926515394216088\n",
      "Cohen's Kappa: 0.2412570888468809\n",
      "********************************************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(ground_truths, probs)\n",
    "recall = tpr\n",
    "\n",
    "# compute other metrics using the same thresholds\n",
    "specificity = np.zeros_like(tpr)\n",
    "precision = np.zeros_like(tpr)\n",
    "fbetascores = np.zeros_like(tpr)\n",
    "CKappas = np.zeros_like(tpr)\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    preds = probs > thresholds[i]\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(ground_truths, preds).ravel()\n",
    "    \n",
    "    specificity[i] = tn / (tn + fp)\n",
    "    precision[i] = tp / (tp + fp)\n",
    "    \n",
    "    # more attention put on recall, such as when false negatives are more important to\n",
    "    # minimize, but false positives are still important.\n",
    "    fbetascores[i] = metrics.fbeta_score(ground_truths, preds, beta = 2)\n",
    "    \n",
    "    CKappas[i] = metrics.cohen_kappa_score(ground_truths, preds,)\n",
    "    \n",
    "\n",
    "\n",
    "gmeans = np.sqrt(specificity * recall)\n",
    "\n",
    "\n",
    "print(\"Max F2-Score is:\", np.nanmax(fbetascores))\n",
    "print(\"Max G-Mean is:\", np.nanmax(gmeans))\n",
    "print(\"Max Cohen's Kappa is:\", np.nanmax(CKappas))\n",
    "\n",
    "\n",
    "print(\"Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs), \"\\n\")\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr[np.nanargmax(fbetascores)], tpr[np.nanargmax(fbetascores)], 'ro')\n",
    "plt.plot(fpr[np.nanargmax(gmeans)], tpr[np.nanargmax(gmeans)], 'go')\n",
    "plt.plot(fpr[np.nanargmax(CKappas)], tpr[np.nanargmax(CKappas)], 'yo')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(['ROC Curve', 'F2-Score Optimal Coordinates', 'G-Mean Optimal Coordinates', \n",
    "            \"Kappa's Optimal Coordinates\"], loc='lower right', prop={'size': 8}, \n",
    "           frameon=True, facecolor = 'white')\n",
    "plt.show()\n",
    "\n",
    "fb_opt_thresh = thresholds[np.nanargmax(fbetascores)]\n",
    "fb_opt_preds = probs > fb_opt_thresh\n",
    "\n",
    "print('\\n********************* USING F2-SCORE OPTIMAL THRESHOLD *************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, fb_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(fbetascores)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(fbetascores)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(fbetascores)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(fbetascores)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(fbetascores)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(fbetascores)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "gm_opt_thresh = thresholds[np.nanargmax(gmeans)]\n",
    "gm_opt_preds = probs > gm_opt_thresh\n",
    "\n",
    "print('\\n********************** USING G-MEAN OPTIMAL THRESHOLD **************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, gm_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(gmeans)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(gmeans)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(gmeans)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(gmeans)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(gmeans)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(gmeans)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "\n",
    "ck_opt_thresh = thresholds[np.nanargmax(CKappas)]\n",
    "ck_opt_preds = probs > ck_opt_thresh\n",
    "\n",
    "print('\\n********************** USING KAPPA OPTIMAL THRESHOLD ***************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, ck_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(CKappas)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(CKappas)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(CKappas)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(CKappas)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(CKappas)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(CKappas)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "\n",
    "accuracy_scores = []\n",
    "for thresh in thresholds:\n",
    "    accuracy_scores.append(metrics.accuracy_score(ground_truths, [m > thresh for m in probs]))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(thresholds, fbetascores, \"-r\")\n",
    "plt.plot(thresholds, gmeans, \"-g\")\n",
    "plt.plot(thresholds, CKappas, \"-y\")\n",
    "plt.title(\"F2-Score, G-Means, and Cohen's Kappa Curves\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Performance Metrics\")\n",
    "plt.legend(['F2-Score', 'G-Mean', \"Cohen's Kappa\"], loc='upper right',\n",
    "           frameon=True, facecolor = 'white')\n",
    "plt.show()    \n",
    "    \n",
    "\n",
    "plt.plot(thresholds, accuracy_scores)\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAS Pytorch CUDA",
   "language": "python",
   "name": "mas_pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
