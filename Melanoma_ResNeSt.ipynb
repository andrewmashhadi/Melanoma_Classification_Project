{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c030ccd3",
   "metadata": {},
   "source": [
    "# Melanoma Detection with the ResNeSt Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce87f3",
   "metadata": {},
   "source": [
    "This code was used in the Hoffman2 Linux Compute Cluster, making use of UCLA's high performance cloud computing resources like the Tesla P4 - GPU (6.1 Compute Capability, 2560 CUDA Cores, 8GB) with additional 32GB RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf460317",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad9aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1e190",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a1310",
   "metadata": {},
   "source": [
    "General histograms and bar charts for frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262eb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of positives: 0.017589052123163616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mel_df = pd.read_csv(os.path.join('train_data', 'train.csv'))\n",
    "gt = mel_df['target']\n",
    "isic_id = mel_df['image_name']\n",
    "\n",
    "# proportion of postives\n",
    "print(\"Proportion of positives:\", np.mean(gt))\n",
    "\n",
    "plt.hist(mel_df['age_approx'])\n",
    "plt.title('Histogram of Ages')\n",
    "plt.xlabel('Age')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x = mel_df['benign_malignant'],\n",
    "            y = mel_df['age_approx'])\n",
    "plt.title('Ages Grouped By Benign / Malignant Status')\n",
    "plt.xlabel('Benign / Malignant Status')\n",
    "plt.ylabel('Approximate Age')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.bar(mel_df.sex.value_counts().index,  mel_df.sex.value_counts().values)\n",
    "plt.title('Sex / Gender in Dataset')\n",
    "plt.show()\n",
    "\n",
    "plt.barh(mel_df.anatom_site_general_challenge.value_counts().index, mel_df.anatom_site_general_challenge.value_counts().values)\n",
    "plt.title('Lesion Locations in Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb11e5",
   "metadata": {},
   "source": [
    "Tests to find potential correlation between target variables and other categorical variables such as sex/gender or lesion location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101b246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************* TARGET W/ SEX INDEPENDENCE TESTS *******************\n",
      "benign_malignant  benign  malignant\n",
      "sex                                \n",
      "female             11824        170\n",
      "male               12535        267\n",
      "Chi-Squared test of independence (P-value): 7.87631386486258e-05 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************* TARGET W/ LESION LOCATION INDEPENDENCE TESTS *******************\n",
      "anatom_site_general_challenge  head/neck  lower extremity  oral/genital  \\\n",
      "sex                                                                       \n",
      "female                               629             3363            33   \n",
      "male                                 767             2966            57   \n",
      "\n",
      "anatom_site_general_challenge  palms/soles  torso  upper extremity  \n",
      "sex                                                                 \n",
      "female                                 111   5683             2001  \n",
      "male                                   169   6926             1698  \n",
      "Chi-Squared test of independence (P-value): 3.917186815096256e-37 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAEICAYAAAA3PAFIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgx0lEQVR4nO3deXyU5b338c9XQEBR3PtEXNJa3CiKiguKHrTWarVqqxbXuh2t2uqxrfrQ1tOq1SM99vXUrS5oFfcuT7XHuoFVEVwhyF53xSpYdyIUV/ydP+4rMo4zySSZZJI73/frNa/ccy/X9buvTOabe5lEEYGZmVl3t0KtCzAzM6sGB5qZmeWCA83MzHLBgWZmZrngQDMzs1xwoJmZWS440Cx3JB0maWIN+t1J0rOSlkjav5P6vELSfzaz/KeSru6MWlJ/R0l6qLP6a42Wxqq7krRBes31qnUttSZ/Ds0qIWk+8AVgGfAv4C7g5IhYUuO66oEXgT4R8XGNa7kPuD0iLqpR/6OAGyNivVr0n2o4Cvj3iBhZhbYmke1PpwVyW0g6C/hyRBzeSf3NJxvjv3VGf92Jj9CsNb4ZEQOArYFtgTOLV5DUu7OK6cy+KrQhMK/WRZj1VA40a7WIWADcDXwFQFJI+r6kZ4Fn07zjJD0n6W1Jt0tat2n7tP4pkl6Q9KakCyStkJatIOlMSS9Jel3S9ZIGpmX1adtjJf0DuB+YnJpdlE67jCg+7SVpR0nTJDWmrzsWLJsk6ZeSHpa0WNJESWuV2/dy+yXpeeBLwF9THX1LbDtf0k8k/V3SO5KuldSvgrYl6TdpPBolzZbUNPbjJZ0raeX0PVk39b9E0rqSzpJ0Y1r3Hkk/KKpplqRvp+lNJd2b+n9a0ncK1vtGqnuxpAWSTis3RqnkS1KtT0n6app5kKTpRSv+WNJfmmmrXAfHSHoyjeMESRtWOlYtjXdaFpJOUHYK+R1Jv5WkNtS5r6R5khal19pmBcvWl3SrpDckvSXp0jR/I0n3p3lvSrpJ0mpp2Q3ABix/nZ2h5T8XvdM666b9eTvt33EFfZ4l6Y/Kfq4Wp9qGt3a/uqyI8MOPFh/AfGD3NL0+2ZHIL9PzAO4F1gD6A7sBb5IdyfUFLgEmF7QVwANp/Q2AZ8hOoQAcAzxHFg4DgFuBG9Ky+rTt9cDKqa+meb0L2j8KeChNrwG8AxwB9AYOSc/XTMsnAc8DG6f2JgFjy4xBS/v16Rg1M4Zz0/itATwMnNtS28DXgenAaoCAzYC6tGx8QRujgFeK+jyL7LQdwHeBhwuWbQ4sSv2tDLwMHJ3GaetUz5C07qvAzml6dWDrMvt4FPAx8EOgDzAaaEz72xd4G9isYP0ZwAFl2prU9Loomr9/eo1slmo9E3iklWNVyWv0jtTOBsAbwJ5l6vx0jIvmb0x2ev5raSzOSHWvCPQCZgG/SWPfDxiZtvty2qYvsDbZL20XlnudUfQzADwIXJbaHJZq/2pBre8D30g1nA88Vuv3l6q9T9W6AD+6xyP9EC1Jb4AvpR+Y/mlZALsVrPs74L8Lng8APgLqC9bfs2D5ScB9afo+4KSCZZukbXsX/OB+qWD5Z36Y07yjWB5oRwBTi/blUeCoND0JOLOolnvKjEFL+/WZN5oyY3hCwfNvAM+31DbZm+8zwA7ACkVtjqfyQFuF7A12w/T8POCaND0amFK07ZXAL9L0P4DvAau28Do5ClhIuj6f5k0FjkjTlwPnpekhZL9c9C3T1iRKB9rdwLEFz1cAlpKd8q10rCp5jY4sWP5HYEyZOj8d46L5/wn8sajOBen7NIIsaHqXarOonf2BGUWvo5KBRvbL0jJglYLl5wPjC2r9W8GyzYH3WvNe0JUfPuVorbF/RKwWERtGxEkR8V7BspcLptclCz0AIrtx5C1gUJn1X0rbfG7bNN2b7IaUUtu2pLi9pjYLa/lnwfRSsje3Ftsqs18tqWi/C9uOiPuBS4HfAq9JGidp1Vb02dTmYuBO4OA062DgpjS9IbB9OjW2SNIi4DDg/6TlB5AF8EuSHpQ0opmuFkR6tyyxn9cBh6bTd0eQveF/0Mpd2RC4qKDOt8mOxlozVpV8Lyt9XZRT3McnZN//QWTB81KUuJFJ0jqSfp9O7b4L3AiUPQ1eos+30/e6SUuv937qetej28SBZtVS+Aa2kOxNB4B0fWdNst9Om6xfML1B2uZz26ZlHwOvlemrpdt0i9tranNBiXVbUsl+taSi/S5uOyIujohtyI5qNgZOL9F2Jbcs3wIckgKpP9mpX8jeaB9Mv7A0PQZExImp/2kRsR+wDvAXsiOWcgYVXW/6dD8j4jHgQ2Bn4FDghgpqLvYy8L2iWvtHxCOpj0rGqhrfy5YU9yGy7/+CtA8blAmS88m+l1tExKrA4WSB3aS57/NCYA1JqxTMa+vrvdtxoFlHuBk4WtIwZTdH/BfweETML1jndEmrS1of+A/gD2n+LcAPJX1R0oC07R9K/SabvAF8QnbNrZS7gI0lHSqpt6TRZKdZ7uig/WrJ9yWtJ2kN4Kcs3++ybUvaVtL2kvqQnTJ8n+y0UrHXgDWVbqIp4y6yN9lzyMb1kzT/DrJxOkJSn/TYVtJmklZU9tm+gRHxEfBumf6brAOckto4iOw61l0Fy68nO4r6OCJa+sxab0n9Ch59gCuAn0gaAiBpYOqHVoxVNb6XhVYoqrMvWejvLemrqZ4fAx8Aj5Cdhn0VGCtp5bTNTqmtVUin9yUN4vOB/BplXu8R8XJq//zU5hbAsSw/Es81B5pVXUTcR3b94M9kP7Qbsfw0V5P/Ibt4P5PsNNjv0vxryH5rn0z2+bL3gZOb6Wsp2bWgh9MpqB2Klr8F7EP2ZvIW2YX5fSLizQ7ar5bcDEwEXkiPcytoe1XgKrLrTS+l/fh1ifqeIvuF4IU0FuuWWOcDshttdk+1NM1fDOyR+lxIdlrqV2Q3JkB2enB+OgV2AtlRQzmPA4PJbro4DzgwfR+a3EB2h2wlR2eXA+8VPK6NiNtSbb9P9cwF9krrVzpW1fheFjqkqM7nI+JpsnG6hGwsvkn20ZcPI2JZev5lsuuTr5BdxwQ4m+xmlUayn41bi/o6HzgzfY9L3W16CNl1tYXAbWTXQe9tx751G/5gtXU6SQEMjojnal1LZ5I/EAuApP7A62R3Sj5b63osP3yEZmad7URgmsPMqi0Xd7aYWfeQjlJFdiu6WVX5lKOZmeWCTzmamVku+JRjjay11lpRX19f6zLMzLqV6dOnvxkRa5da5kCrkfr6ehoaGmpdhplZtyKp+C//fMqnHM3MLBccaGZmlgsONDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXHCgmZlZLviD1TUyZ0Ej9WPurHUZZi2aP3bvWpdgVhEfoZmZWS440MzMLBccaGZmlgsONDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXHCgmZlZLjjQzMwsF7p1oEmaJGl4wfOfSDqsyn2cI2n3NH2qpJVaW5eZmXW8Lh1oyrSmxj2AidWsISJ+HhF/S09PBVoMNDMz63w1DzRJP5I0Nz1OlVQv6UlJlwFPAOtLulxSg6R5ks4u086qwIoR8YakjSQ9JmlaOsJaUrDe6Wn+7Ka2Cvq8KvUxUVL/tGy8pAMlnQKsCzwg6YG0rMW6zMysc9Q00CRtAxwNbA/sABwHrA5sAlwfEVtFxEvAzyJiOLAF8G+StijR3O7AfWn6IuCiiNgWWFjQ3x7AYGA7YBiwjaRd0uLBwG8jYgiwCDigsPGIuDi1tWtE7JpmV1JX4f4enwKwYdnSxuYHx8zMWqXWR2gjgdsi4l8RsQS4FdgZeCkiHitY7zuSngBmAEOAzUu0tSdwd5oeAfwpTd9csM4e6TGD7OhvU7IgA3gxImam6elAfQX1V1LXpyJiXEQMj4jhvVYaWEHzZmZWqVr/PzSVmf+vT1eQvgicBmwbEe9IGg/0K7HNdsCJFfR3fkRc+ZmZUj3wQcGsZUD/ZhuqvC4zM+sEtT5CmwzsL2klSSsD3wKmFK2zKlnANUr6ArBXcSOShgBPRcSyNOsxlp8yPLhg1QnAMZIGpO0GSVqnFfUuBlaptC4zM+s8NT1Ci4gn0pHN1DTrauCdonVmSZoBzANeAB4u0dRewD0Fz08FbpT0Y+BOoDG1NVHSZsCjkgCWAIeTHZFVYhxwt6RXI2LXCuoyM7NOooiodQ3tJule4LsR8Wp6vhLwXkSEpIOBQyJiv5oWWaRv3eCoO/LCWpdh1qL5Y/eudQlmn5I0Pd2M9zm1voZWFRHxtaJZ2wCXKjsMWwQc0+lFmZlZp8pFoBWLiCnAlrWuw8zMOk+tbwoxMzOrCgeamZnlggPNzMxywYFmZma54EAzM7NccKCZmVku5PK2/e5g6KCBNPgDq2ZmVeMjNDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXPBdjjUyZ0Ej9WPurHUZViH/CxWzrs9HaGZmlgsONDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXHCgmZlZLjjQzMwsFxxoZmaWCw40MzPLhW4RaJLGSzqwSm39RNJhrdxmlKQ7qtG/mZl1jG4RaFW2BzCx1kWYmVl11STQJNVLekrSdZJmS/r/klaS9HNJ0yTNlTROkkpsO1/Sf0l6VFKDpK0lTZD0vKQT0jp1kiZLmpna2jnNXxVYMSLekHRQWjZL0uS0vJ+kayXNkTRD0q4l+l9Z0jWpzhmS9kvzh0iamvqcLWlwhw6imZl9Ri2P0DYBxkXEFsC7wEnApRGxbUR8BegP7FNm25cjYgQwBRgPHAjsAJyTlh8KTIiIYcCWwMw0f3fgvjT9c+DrEbElsG+a932AiBgKHAJcJ6lfUd8/A+6PiG2BXYELJK0MnABclPocDrxSXLSk41MINyxb2tj86JiZWavUMtBejoiH0/SNwEhgV0mPS5oD7AYMKbPt7enrHODxiFgcEW8A70taDZgGHC3pLGBoRCxO6+8J3J2mHwbGSzoO6JXmjQRuAIiIp4CXgI2L+t4DGCNpJjAJ6AdsADwK/FTS/wU2jIj3iouOiHERMTwihvdaaWCzg2NmZq1Ty0CLEs8vAw5MR0hXkYVFKR+kr58UTDc97x0Rk4FdgAXADZK+m5ZvB0wFiIgTgDOB9YGZktYEPneKswQBB0TEsPTYICKejIibyY703gMmSNqtgrbMzKxKahloG0gakaYPAR5K029KGkB2GrFNJG0IvB4RVwG/A7aWNAR4KiKWpXU2iojHI+LnwJtkwTYZOCwt35jsyOvpouYnACc3Xd+TtFX6+iXghYi4mOwIcou21m9mZq1Xy/9Y/SRwpKQrgWeBy4HVyU4jzic7bdhWo4DTJX0ELAG+CxwA3FOwzgXpxg2RXVebBTwFXJFOeX4MHBURHxTdm/JL4EJgdgq1+WTX+kYDh6c+/8ny63lmZtYJFFF85q8TOpXqgTvSzR+d1ee9wHcj4tXO6rM5fesGR92RF9a6DKvQ/LF717oEMwMkTY+I4aWW1fIIrVNFxNdqXYOZmXWcmgRaRMwHOu3ozMzM8q8n/qUQMzPLIQeamZnlggPNzMxywYFmZma54EAzM7NccKCZmVku9JjPoXU1QwcNpMEf1jUzqxofoZmZWS440MzMLBccaGZmlgsONDMzywUHmpmZ5YLvcqyROQsaqR9zZ63LqBr/exUzqzUfoZmZWS440MzMLBccaGZmlgsONDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXHCgmZlZLjjQzMwsF7pcoEmqlzS3A9qdL2mtgudXStqpiu1PkjS8Wu2ZmVnrdLlA60TbA4/VuggzM6uOrhpovSRdJWmepImS+kvaSNI9kqZLmiJpUwBJ35T0uKQZkv4m6Qtp/ppp2xmSrgTU1LikzYBnImJZOrL6laSpkp6RtHNap5ekCyRNkzRb0vcKtj9D0hxJsySNLSxc0gqSrpN0bmcMlJmZZbpqoA0GfhsRQ4BFwAHAOODkiNgGOA24LK37ELBDRGwF/B44I83/BfBQmn87sEFB+3sB9xQ87x0R2wGnpu0AjgUaI2JbYFvgOElflLQXsD+wfURsCfx3YTvATWRheWbxTkk6XlKDpIZlSxtbOSRmZtacrvrvY16MiJlpejpQD+wI/En69ECrb/q6HvAHSXXAisCLaf4uwLcBIuJOSe8UtP914OiC57cW9QWwB7CFpAPT84FkQbs7cG1ELE1tv13QzpXAHyPivFI7FRHjyIKZvnWDo+zem5lZq3XVI7QPCqaXAWsAiyJiWMFjs7T8EuDSiBgKfA/oV7Dt50JD0krAahGxsER/y1ge8iI7Imzq74sRMTHNLxdGjwC7SupXZrmZmXWQrhpoxd4FXpR0EIAyW6ZlA4EFafrIgm0mA4el9fcCVk/zdwUeqKDPCcCJkvqkNjaWtDIwETgmBSOS1ijY5nfAXWRHkl316NfMLJe6S6BBFk7HSpoFzAP2S/PPIguQKcCbBeufDewi6Qmy04f/SPOLr5+VczXwd+CJ9DGCK8mutd1Ddk2uQdJMsut5n4qI/wc8AdwgqTuNr5lZt6aInnUpJwXc9hHxUS3r6Fs3OOqOvLCWJVTV/LF717oEM+sBJE2PiJKf+e1xp8UiYuta12BmZtXnU2JmZpYLDjQzM8sFB5qZmeWCA83MzHLBgWZmZrngQDMzs1xwoJmZWS70uM+hdRVDBw2kwR9GNjOrGh+hmZlZLjjQzMwsFxxoZmaWCw40MzPLBQeamZnlgu9yrJE5CxqpH3NnrcswM+tUHfmvpnyEZmZmueBAMzOzXHCgmZlZLjjQzMwsFxxoZmaWCw40MzPLBQeamZnlggPNzMxywYFmZma54EAzM7NcyF2gSfppB7a9r6QxaXp/SZt3VF9mZtY6XSrQlGlvTSUDrRptR8TtETE2Pd0fcKCZmXURzb7BS6qXNLfg+WmSzkrTkyRdKOkRSXMlbZfmnyXpBkn3S3pW0nEF258uaZqk2ZLOLujjSUmXAU8A6xfVsI2kByVNlzRBUp2kgZKelrRJWucWScdJGgv0lzRT0k2l2m6mhqckXZ325SZJu0t6OO1D074dJelSSTsC+wIXpL42kvREQc2DJU1v6zfFzMxar71HQytHxI7AScA1BfO3APYGRgA/l7SupD2AwcB2wDBgG0m7pPU3Aa6PiK0i4qWmRiT1AS4BDoyIbVIf50VEI/ADYLykg4HVI+KqiBgDvBcRwyLisOK203S5Gr4MXJRq3xQ4FBgJnEbRUV9EPALcDpye+noeaJQ0LK1yNDC+eLAkHS+pQVLDsqWNLY2tmZm1Qnv/fcwtABExWdKqklZL8/8nIt4D3pP0AFmAjAT2AGakdQaQhcs/gJci4rES7W8CfAW4VxJAL+DV1Oe9kg4Cfgts2UyNhW3v0UwNL0bEHABJ84D7IiIkzQHqKxiLq4GjJf0IGJ32+TMiYhwwDqBv3eCooE0zM6tQS4H2MZ89iutXtLz4TTmamS/g/Ii4snCBpHrgX2X6FzAvIkZ8bkF2PWwz4D1gDeCVMm0Utt1cDR8UzPqk4PknVBb8fwZ+AdwPTI+ItyrYxszMqqSlU46vAetIWlNSX2CfouWjASSNBBrTqUCA/ST1k7QmMAqYBkwAjpE0IG0zSNI6LfT/NLC2pBFpmz6ShqRlPwSeBA4BrkmnJwE+Kpgu1pYaylkMrNL0JCLeT+1fDlzbxjbNzKyNmj3yiIiPJJ0DPA68CDxVtMo7kh4BVgWOKZg/FbgT2AD4ZUQsBBZK2gx4NJ0+XAIcDixrpv8PJR0IXCxpYKr3QkkfAf8ObBcRiyVNBs4kO0IaB8xON2n8rKi9ia2toRm/B66SdArZNb7ngZuAbwMT29CemZm1gyLadilH0iTgtIhoKJp/FrAkIn7d7uq6GUmnAQMj4j9bWrdv3eCoO/LCji/KzKwLmT9273ZtL2l6RAwvtay9N4VYIuk2YCNgt1rXYmbWE7U50CJiVJn5Z7W1ze4sIr5V6xrMzHqyLvWXQszMzNrKgWZmZrngQDMzs1xwoJmZWS440MzMLBccaGZmlgv+HFqNDB00kIZ2fsDQzMyW8xGamZnlggPNzMxywYFmZma54EAzM7NccKCZmVkuONDMzCwXfNt+jcxZ0Ej9mDtrXYZVqL3/w8nMOp6P0MzMLBccaGZmlgsONDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXHCgmZlZLjjQzMwsFyoONElLOrKQapF0qqSVOqjt4ZIuTtOjJO3YEf2YmVnrdbkjNEm92tnEqUDJQGtv2xHREBGnpKejAAeamVkX0epAU+YCSXMlzZE0Os2/TNK+afo2Sdek6WMlnZumD5c0VdJMSVc2BYykJZLOkfQ4MKKov40k3SNpuqQpkjaV1FvSNEmj0jrnSzpP0inAusADkh4o1XYLNfwq9fM3SdtJmiTphYL9GiXpDkn1wAnAD1M7O0t6UVKftN6qkuY3PTczs47XliO0bwPDgC2B3YELJNUBk4Gd0zqDgM3T9EhgiqTNgNHAThExDFgGHJbWWRmYGxHbR8RDRf2NA06OiG2A04DLIuJj4CjgcklfA/YEzo6Ii4GFwK4RsWtx28BbLdQwKfWzGDgX+BrwLeCcwoIiYj5wBfCbiBgWEVOASUDTX7A9GPhzRHxUuJ2k4yU1SGpYtrSx9OiamVmbtOWv7Y8EbomIZcBrkh4EtgWmAKdK2hz4O7B6CroRwCnAkcA2wDRJAP2B11Oby4A/F3ckaQDZab0/pW0A+gJExDxJNwB/BUZExIdl6i1s+6vN1PAhcE+angN8EBEfSZoD1FcwLlcDZwB/AY4GjiteISLGkQU0fesGRwVtmplZhdoSaCo1MyIWSFqd7GhpMrAG8B1gSUQsVpYg10XET0ps/n4KyGIrAIvS0VQpQ4FFwBeaqbew7eZq+CgimkLmE+CDtF+fSGpxnCLiYUn1kv4N6BURc1vaxszMqqctpxwnA6Ml9ZK0NrALMDUte5TspozJZEdsp6WvAPcBB0paB0DSGpI2bK6jiHgXeFHSQWkbSdoyTX8bWDP1f7Gk1dJmi4FVyjTZ6hqaUaqf64FbgGvb2KaZmbVRWwLtNmA2MAu4HzgjIv6Zlk0BekfEc8ATZEdpUwAi4u/AmcBESbOBe4G6Cvo7DDhW0ixgHrCfpLWAscCxEfEMcClwUVp/HHB3000hhdpRQyl/Bb7VdFNImncTsDpZqJmZWSfS8rNs1l6SDgT2i4gjWlq3b93gqDvywo4vyqrC/7HarGuQND0ihpda1pZraFaCpEuAvYBv1LoWM7OeyIFWJRFxcq1rMDPrybrcXwoxMzNrCweamZnlggPNzMxywYFmZma54EAzM7NccKCZmVku+Lb9Ghk6aCAN/rCumVnV+AjNzMxywYFmZma54EAzM7NccKCZmVkuONDMzCwXHGhmZpYLvm2/RuYsaKR+zJ21LsPMujj/L77K+QjNzMxywYFmZma54EAzM7NccKCZmVkuONDMzCwXHGhmZpYLDjQzM8sFB5qZmeWCA83MzHKhxwaapNUknVTrOszMrDp6bKABqwEVB5qkXh1XipmZtVdPDrSxwEaSZkq6ID3mSpojaTSApFGSHpB0MzBH0sqS7pQ0K63btN5XJc1I214jqW8td8zMrCfqyYE2Bng+IoYBjwHDgC2B3YELJNWl9bYDfhYRmwN7AgsjYsuI+Apwj6R+wHhgdEQMJfuDzyeW6lDS8ZIaJDUsW9rYcXtmZtYD9eRAKzQSuCUilkXEa8CDwLZp2dSIeDFNzwF2l/QrSTtHRCOwCfBiRDyT1rkO2KVUJxExLiKGR8TwXisN7Li9MTPrgRxoGTWz7F9NEym0tiELtvMl/byFbc3MrJP05EBbDKySpicDoyX1krQ22RHW1OINJK0LLI2IG4FfA1sDTwH1kr6cVjuC7AjPzMw6UY/9B58R8ZakhyXNBe4GZgOzgADOiIh/Stq0aLOhZNfXPgE+Ak6MiPclHQ38SVJvYBpwReftiZmZQQ8ONICIOLRo1ulFyycBkwqeTwAmlGjnPmCr6ldoZmaV6smnHM3MLEccaGZmlgsONDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXHCgmZlZLvToD1bX0tBBA2kYu3etyzAzyw0foZmZWS440MzMLBccaGZmlgsONDMzywUHmpmZ5YIDzczMcsGBZmZmueBAMzOzXHCgmZlZLigial1DjyRpMfB0revowtYC3qx1EV2Yx6d5Hp/yuvvYbBgRa5da4D99VTtPR8TwWhfRVUlq8PiU5/FpnsenvDyPjU85mplZLjjQzMwsFxxotTOu1gV0cR6f5nl8mufxKS+3Y+ObQszMLBd8hGZmZrngQDMzs1xwoHUASXtKelrSc5LGlFguSRen5bMlbV3ptnnQzvG5RtLrkuZ2btWdo61jI2l9SQ9IelLSPEn/0fnVd7x2jE8/SVMlzUrjc3bnV9/x2vOzlZb3kjRD0h2dV3UVRYQfVXwAvYDngS8BKwKzgM2L1vkGcDcgYAfg8Uq37e6P9oxPWrYLsDUwt9b70pXGBqgDtk7TqwDP+LXzmfERMCBN9wEeB3ao9T51lfEpWP4j4GbgjlrvT1sePkKrvu2A5yLihYj4EPg9sF/ROvsB10fmMWA1SXUVbtvdtWd8iIjJwNudWnHnafPYRMSrEfEEQEQsBp4EBnVm8Z2gPeMTEbEkrdMnPfJ2R1y7frYkrQfsDVzdmUVXkwOt+gYBLxc8f4XPv7GUW6eSbbu79oxP3lVlbCTVA1uRHYXkSbvGJ51Omwm8DtwbER6fz65zIXAG8EkH1dfhHGjVpxLzin8TLLdOJdt2d+0Zn7xr99hIGgD8GTg1It6tYm1dQbvGJyKWRcQwYD1gO0lfqW55Ndfm8ZG0D/B6REyvflmdx4FWfa8A6xc8Xw9YWOE6lWzb3bVnfPKuXWMjqQ9ZmN0UEbd2YJ21UpXXTkQsAiYBe1a9wtpqz/jsBOwraT7ZqcrdJN3YcaV2kFpfxMvbg+wPPr8AfJHlF2aHFK2zN5+9MDu10m27+6M941OwvJ583hTSnteOgOuBC2u9H110fNYGVkvT/YEpwD613qeuMj5F64yim94U4r+2X2UR8bGkHwATyO46uiYi5kk6IS2/AriL7G6j54ClwNHNbVuD3egw7RkfAEm3kP3ArSXpFeAXEfG7zt2LjtHOsdkJOAKYk64TAfw0Iu7qxF3oUO0cnzrgOkm9yM5M/TEiuuet6WW092crD/ynr8zMLBd8Dc3MzHLBgWZmZrngQDMzs1xwoJmZWS440MzMLBccaGZmlgsONDMzy4X/BfAHpK0CB2O9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"******************* TARGET W/ SEX INDEPENDENCE TESTS *******************\")\n",
    "\n",
    "data_crosstab = pd.crosstab(mel_df['sex'],\n",
    "                            mel_df['benign_malignant'], \n",
    "                            margins = False)\n",
    "print(data_crosstab)\n",
    "\n",
    "chi2, p, dof, ex = ss.chi2_contingency(data_crosstab)\n",
    "\n",
    "print(\"Chi-Squared test of independence (P-value):\", p, \"\\n\")\n",
    "\n",
    "g_df1 = mel_df.groupby(['sex']).mean()\n",
    "plt.bar(mel_df.sex.value_counts().index,  g_df1['target'].values)\n",
    "plt.title(\"Proportion of positives by Sex / Gender\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\n******************* TARGET W/ LESION LOCATION INDEPENDENCE TESTS *******************\")\n",
    "\n",
    "data_crosstab = pd.crosstab(mel_df['sex'],\n",
    "                            mel_df['anatom_site_general_challenge'], \n",
    "                            margins = False)\n",
    "print(data_crosstab)\n",
    "\n",
    "chi2, p, dof, ex = ss.chi2_contingency(data_crosstab)\n",
    "\n",
    "print(\"Chi-Squared test of independence (P-value):\", p, \"\\n\")\n",
    "\n",
    "g_df2 = mel_df.groupby(['anatom_site_general_challenge']).mean() \n",
    "plt.barh(mel_df.anatom_site_general_challenge.value_counts().index, g_df2['target'].values)\n",
    "plt.title(\"Proportion of positives by Lesion Location\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713af71",
   "metadata": {},
   "source": [
    "## ResNet-50 (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9efcaad",
   "metadata": {},
   "source": [
    "Set device as CPU, or GPU if available. Code will have to change if using multiple GPUs (cuda:0, cuda:1, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eba207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    n_workers = os.cpu_count()\n",
    "else:\n",
    "    n_workers = torch.cuda.device_count()\n",
    "\n",
    "# If on a CUDA machine, this should print a CUDA device:\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of devices:\", n_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cea86",
   "metadata": {},
   "source": [
    "We create a custom dataset loader class to use the ID and target information from the CSV to properly load our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1bef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset to load in with the benign \n",
    "# and malignant images in the same directory\n",
    "class ISICDatasetImages(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, patientfile, num_samples=100, start_ind=0, up_sample=False, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        mel_df = pd.read_csv(patientfile) \n",
    "        \n",
    "        if up_sample:\n",
    "            \n",
    "            # Separate majority and minority classes\n",
    "            df_benign = mel_df[mel_df['target']==0]\n",
    "            df_malignant = mel_df[mel_df['target']==1]\n",
    "            \n",
    "\n",
    "            # sample minority class\n",
    "            df_benign_sampled = resample(df_benign, \n",
    "                                         replace=True,     # sample with replacement\n",
    "                                         n_samples=num_samples//2)\n",
    "            \n",
    "\n",
    "            # Upsample minority class\n",
    "            df_malignant_upsampled = resample(df_malignant, \n",
    "                                              replace=True,     # sample with replacement\n",
    "                                              n_samples=num_samples//2)\n",
    "            \n",
    "            # Combine majority class with upsampled minority class\n",
    "            mel_df = pd.concat([df_benign_sampled, df_malignant_upsampled])\n",
    "            \n",
    "            # randomly mix them up (not necessary due to shuffling in dataloader)\n",
    "            mel_df = shuffle(mel_df)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.start_ind = start_ind\n",
    "            self.end_ind = start_ind+num_samples\n",
    "\n",
    "            if self.end_ind > len(mel_df):\n",
    "                self.end_ind = len(mel_df)\n",
    "        \n",
    "            mel_df = mel_df[self.start_ind:self.end_ind]\n",
    "            \n",
    "        self.gt = mel_df['target'].reset_index(drop=True)\n",
    "        self.isic_id = mel_df['image_name'].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.isic_id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, f\"{self.isic_id[idx]}.jpg\")\n",
    "        img = read_image(img_path).float()\n",
    "        class_id = torch.tensor([self.gt[idx]])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "    \n",
    "        \n",
    "        return img, class_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c73fa",
   "metadata": {},
   "source": [
    "We create a custom collate function to pad lower resolution images with zeros to maintain a constant high resolution of 3x4000x6000 for the CNN to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364d0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall that a CNN needs the inputs to be the same dimension so we \n",
    "# custom collate function to pad small res images with 0s if they are not 3x4000x6000\n",
    "def pad_collate2d(batch):\n",
    "    \n",
    "    # init lists\n",
    "    image_list, label_list = [], []\n",
    "   \n",
    "    for _image, _label in batch:\n",
    "        \n",
    "        image_list.append(torch.unsqueeze(_image, dim=0))\n",
    "        label_list.append(_label)\n",
    "        \n",
    "\n",
    "    image_out = torch.cat(image_list, dim=0) \n",
    "    label_out = torch.tensor(label_list, dtype=torch.int64)\n",
    "   \n",
    "    return image_out, label_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20c0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = False\n",
    "\n",
    "# set our batch size\n",
    "batch_size = 4\n",
    "\n",
    "tr_transf = transforms.Compose(\n",
    "    [transforms.Resize(416),\n",
    "     transforms.RandomHorizontalFlip(p=0.3),\n",
    "     transforms.RandomVerticalFlip(p=0.3),\n",
    "     transforms.RandomApply(torch.nn.ModuleList([transforms.GaussianBlur(kernel_size=(5, 7), sigma=(0.1, 2))]), p=0.2),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225]),\n",
    "     transforms.RandomErasing(scale=(0.02, 0.05), p=0.2)\n",
    "    ])\n",
    "\n",
    "val_transf = transforms.Compose(\n",
    "    [transforms.Resize(416),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "train_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data768x768\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data768x768\", \"train.csv\"), \n",
    "                            num_samples=5*2*24408, up_sample=True, start_ind=0, transform=tr_transf)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=pad_collate2d, \n",
    "                          num_workers=n_workers)\n",
    "\n",
    "\n",
    "val_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data768x768\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data768x768\", \"val.csv\"), \n",
    "                            num_samples=2*100, up_sample=True, start_ind=0, transform=val_transf)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=pad_collate2d, \n",
    "                        num_workers=n_workers)\n",
    "\n",
    "\n",
    "\n",
    "# test DataLoader with custom settings\n",
    "if testing:\n",
    "    for imgs, labels in train_loader:\n",
    "        print(\"Batch of images has shape: \",imgs.shape)\n",
    "        print(\"Batch of labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ef7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show the image\n",
    "def imshow(img):\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "    img = img * torch.tensor(std).view(3, 1, 1) + torch.tensor(mean).view(3, 1, 1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg.astype('int'), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "label_id = [\"Benign\", \"Malignant\"]\n",
    "\n",
    "if testing:\n",
    "    # get some random training images\n",
    "    trainiter = iter(train_loader)\n",
    "    images, labels = next(trainiter)\n",
    "    print(\"Size:\", images.shape)\n",
    "\n",
    "\n",
    "    # show images\n",
    "    imshow(images[0,])\n",
    "\n",
    "    # print labels\n",
    "    print(\"Label:\", label_id[labels[0,]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024e4f1",
   "metadata": {},
   "source": [
    "Sample and image from the data loader object to confirm it worked. Continue to run the cell for different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d401950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/zhanghang1989/ResNeSt/zipball/master\" to /u/home/a/andrewma/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new ResNeSt FC Layer weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /u/home/a/andrewma/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    }
   ],
   "source": [
    "load_weights = False\n",
    "create_new_weights = True\n",
    "PATH = './melanoma_ResNeSt.pth'\n",
    "\n",
    "# get list of models\n",
    "torch.hub.list('zhanghang1989/ResNeSt', force_reload=True)\n",
    "\n",
    "if load_weights:\n",
    "    print('Loading the pre-trained ResNeSt weights.')\n",
    "    \n",
    "    # network weights load\n",
    "    net = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True).to(device)\n",
    "    \n",
    "    # for feature extraction\n",
    "    #for param in net.parameters():\n",
    "        #param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 1024),\n",
    "               nn.BatchNorm1d(1024),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.4),\n",
    "               nn.Linear(1024, 256),\n",
    "               nn.BatchNorm1d(256),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(256, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "\n",
    "    checkpoint = torch.load(PATH, map_location=device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # optimizer state load\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.fc.parameters(), weight_decay=0)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=12204, gamma=0.5)\n",
    "    lr_sched.load_state_dict(checkpoint['lr_sched'])\n",
    "    \n",
    "    # total mini_batch state load\n",
    "    mini_batch = checkpoint['mini_batch']\n",
    "    \n",
    "    print(\"CUDA Memory Allocated:\", torch.cuda.max_memory_allocated())\n",
    "    \n",
    "elif create_new_weights:\n",
    "    print('Creating new ResNeSt FC Layer weights.')\n",
    "    \n",
    "    net = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True).to(device)\n",
    "    \n",
    "    # for feature extraction\n",
    "    #for param in net.parameters():\n",
    "        #param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 1024),\n",
    "               nn.BatchNorm1d(1024),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.4),\n",
    "               nn.Linear(1024, 256),\n",
    "               nn.BatchNorm1d(256),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(256, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "    \n",
    "    \n",
    "    mini_batch = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.fc.parameters(), weight_decay=0)\n",
    "    lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=12204, gamma=0.5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b13966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training CUDA Memory Allocation: 7312062976\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 511] Loss: 0.348335862159729\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8302\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 512] Loss: 0.49988842010498047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 513] Loss: 0.5004417498906454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 514] Loss: 0.5356898903846741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 515] Loss: 0.5113553822040557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 516] Loss: 0.46433840940395993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 517] Loss: 0.4577412498848779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 518] Loss: 0.46772209368646145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 519] Loss: 0.4989231940772798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 520] Loss: 0.47830565720796586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 521] Loss: 0.47872529923915863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 522] Loss: 0.4769507807989915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 523] Loss: 0.4668647383268063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 524] Loss: 0.4690249317458698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 525] Loss: 0.50991390645504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 526] Loss: 0.5282089496031404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 527] Loss: 0.5138426852576873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 528] Loss: 0.49878764649232227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 529] Loss: 0.4945917662821318\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 530] Loss: 0.5040149748325348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 531] Loss: 0.4900203858103071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 532] Loss: 0.47891687805002386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 533] Loss: 0.48528652087501856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 534] Loss: 0.5059257969260216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 535] Loss: 0.5185530781745911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 536] Loss: 0.515302890768418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 537] Loss: 0.5077572509094521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 538] Loss: 0.4996849128178188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 539] Loss: 0.49211653553206347\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 540] Loss: 0.49294303059577943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 541] Loss: 0.5101116222719992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 542] Loss: 0.5084840757772326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 543] Loss: 0.4997105128837354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 544] Loss: 0.5064793421941645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 545] Loss: 0.5160700559616089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 546] Loss: 0.5107627618643973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 547] Loss: 0.512954480744697\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 548] Loss: 0.5231450505946812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 549] Loss: 0.5148424192880973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 550] Loss: 0.5083044029772281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 551] Loss: 0.5118031131058205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 552] Loss: 0.5187482131378991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 553] Loss: 0.5108087055904921\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 554] Loss: 0.5148089548403566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 555] Loss: 0.5105055742793613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 556] Loss: 0.5071613140728163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 557] Loss: 0.5043759644031525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 558] Loss: 0.4989181337878108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 559] Loss: 0.4937776293681592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 560] Loss: 0.4901241084933281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 561] Loss: 0.5018085168272841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 562] Loss: 0.5022028991236136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 563] Loss: 0.49865938217010136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 564] Loss: 0.49490469611353344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 565] Loss: 0.491621180285107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 566] Loss: 0.4959406897957836\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 567] Loss: 0.4963157807002988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 568] Loss: 0.49768605956743506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 569] Loss: 0.4955092863006107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 570] Loss: 0.4918357608218988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 571] Loss: 0.48794565215462543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 572] Loss: 0.48684989036090914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 573] Loss: 0.48775610824426013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 574] Loss: 0.4944809207227081\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 575] Loss: 0.496082346026714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 576] Loss: 0.4989881459059137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 577] Loss: 0.49652678108037407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 578] Loss: 0.49262306904968095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 579] Loss: 0.4994257129188897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 580] Loss: 0.5052621698805264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 581] Loss: 0.5017205642562517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 582] Loss: 0.5062628750585847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 583] Loss: 0.5023875160984796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 584] Loss: 0.4999781800685702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 585] Loss: 0.49831487715244294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 586] Loss: 0.4970578361106546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 587] Loss: 0.5011122147371243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 588] Loss: 0.49970881927471894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 589] Loss: 0.5001280769894395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 590] Loss: 0.49731125850230457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 591] Loss: 0.4944241236389419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 592] Loss: 0.4944846889958149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 593] Loss: 0.4956910938742649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 594] Loss: 0.4930788663526376\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 595] Loss: 0.4913436986067716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 596] Loss: 0.49022891684327013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 597] Loss: 0.48985626214537126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 598] Loss: 0.48985325799069623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 599] Loss: 0.49318493534339947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 600] Loss: 0.49283232043186825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 601] Loss: 0.49037631216284994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 602] Loss: 0.489277015723612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 603] Loss: 0.48950006452298933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 604] Loss: 0.4942741883879012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 605] Loss: 0.49303419668423504\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 606] Loss: 0.49197067646309733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 607] Loss: 0.4909118083641701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 608] Loss: 0.493888183059741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 609] Loss: 0.49454324489290064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 610] Loss: 0.4926541619002819\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 611] Loss: 0.49184591566572095\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 612] Loss: 0.49043762464733687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 613] Loss: 0.48803699016571045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 614] Loss: 0.4860228282900957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 615] Loss: 0.48948811292648314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 616] Loss: 0.49106678647815055\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 617] Loss: 0.48867544490043247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 618] Loss: 0.4904557244369277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 619] Loss: 0.48806662365384057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 620] Loss: 0.48852013688195833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 621] Loss: 0.48845711175922873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 622] Loss: 0.4889254140268479\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 623] Loss: 0.49172317942159366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 624] Loss: 0.49375413449709876\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 625] Loss: 0.49148690532083095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 626] Loss: 0.49274516966322374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 627] Loss: 0.4914699946967965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 628] Loss: 0.4905337371816069\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 629] Loss: 0.4932800044282144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 630] Loss: 0.49179739244282245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 631] Loss: 0.4915594590350616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 632] Loss: 0.48999860870545026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 633] Loss: 0.4902053212489539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 634] Loss: 0.4932796165587441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 635] Loss: 0.49349808537960055\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 636] Loss: 0.4934361646809275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 637] Loss: 0.4910187473681968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 638] Loss: 0.49108728126157075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 639] Loss: 0.49419340710769327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 640] Loss: 0.4977269403063334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 641] Loss: 0.49711805365922795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 642] Loss: 0.4988869285267411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 643] Loss: 0.49709591549590115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 644] Loss: 0.5005066753990615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 645] Loss: 0.4999609873250679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 646] Loss: 0.5030140623669414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 647] Loss: 0.5004780330579647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 648] Loss: 0.4986469314798065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 649] Loss: 0.4980065626420563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 650] Loss: 0.49628246054053304\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 651] Loss: 0.49592138869119873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 652] Loss: 0.4948096511439538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 653] Loss: 0.4947377586072975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 654] Loss: 0.4931091783154342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 655] Loss: 0.49088737358307016\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 656] Loss: 0.48949057200591856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 657] Loss: 0.48959789440339924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 658] Loss: 0.4906649087127802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 659] Loss: 0.4916138533977854\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 660] Loss: 0.49053636342287066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 661] Loss: 0.4905033570449084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 662] Loss: 0.4911456312984228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 663] Loss: 0.4901034030454611\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 664] Loss: 0.4882382474742927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 665] Loss: 0.4894970050742549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 666] Loss: 0.4887731513724877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 667] Loss: 0.4934460314785599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 668] Loss: 0.4925418012315714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 669] Loss: 0.49159629867886595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 670] Loss: 0.49071913296356795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 671] Loss: 0.4895209293372883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 672] Loss: 0.4909216957511725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 673] Loss: 0.49393778682852085\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 674] Loss: 0.49349420581285547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 675] Loss: 0.49157686919877025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 676] Loss: 0.49120349715273065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 677] Loss: 0.49295815504239704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 678] Loss: 0.49544733070901464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 679] Loss: 0.49808677819353586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 680] Loss: 0.49727930549313043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 681] Loss: 0.4966882488183808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 682] Loss: 0.500122167343317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 683] Loss: 0.498816683457766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 684] Loss: 0.5006301642834455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 685] Loss: 0.5020251955304827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 686] Loss: 0.5045709979127754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 687] Loss: 0.5035173898026094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 688] Loss: 0.505327337076155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 689] Loss: 0.5042511702582823\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 690] Loss: 0.5061517482002577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 691] Loss: 0.5117645130302366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 692] Loss: 0.5134027434901877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 693] Loss: 0.5132300366795128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 694] Loss: 0.5146175624559755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 695] Loss: 0.5162509233564944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 696] Loss: 0.5193199584240554\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 697] Loss: 0.5206519711145106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 698] Loss: 0.521941195935645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 699] Loss: 0.5212568255959364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 700] Loss: 0.5199470312971818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 701] Loss: 0.5187075506330161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 702] Loss: 0.5171064058473954\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 703] Loss: 0.5154988109447796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 704] Loss: 0.5143444017343914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 705] Loss: 0.516076943049064\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 706] Loss: 0.5158413174201031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 707] Loss: 0.5163726137979382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 708] Loss: 0.5147425017573617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 709] Loss: 0.5146541939907937\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 710] Loss: 0.5133288657665253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 711] Loss: 0.5120244521406753\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 712] Loss: 0.5137357254429619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 713] Loss: 0.5129750986110988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 714] Loss: 0.5147779515268756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 715] Loss: 0.5135107646628124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 716] Loss: 0.5156928299410829\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 717] Loss: 0.5151717759272902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 718] Loss: 0.5145159806483067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 719] Loss: 0.5135364536748549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 720] Loss: 0.513494125860078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 721] Loss: 0.5133676910287396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 722] Loss: 0.5120503286708076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 723] Loss: 0.511872326963944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 724] Loss: 0.5130548759877125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 725] Loss: 0.5148738809796267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 726] Loss: 0.5159498043358326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 727] Loss: 0.515195814008537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 728] Loss: 0.5142911657554294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 729] Loss: 0.5131164808251542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 730] Loss: 0.5128318992528048\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 731] Loss: 0.5130664145245272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 732] Loss: 0.5133600119534913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 733] Loss: 0.5138683458080207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 734] Loss: 0.5131396539509296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 735] Loss: 0.5139060476091173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 736] Loss: 0.5145088853561772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 737] Loss: 0.5136682981961624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 738] Loss: 0.5152922361566309\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 739] Loss: 0.5138265952532989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 740] Loss: 0.5124364627444226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 741] Loss: 0.5120403632457122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 742] Loss: 0.5112531453627964\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 743] Loss: 0.5105277605578623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 744] Loss: 0.510335470494042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 745] Loss: 0.5103011896001531\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 746] Loss: 0.5100643095576157\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 747] Loss: 0.5086351218731594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 748] Loss: 0.5079915569234295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 749] Loss: 0.5093270780277053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 750] Loss: 0.5080420926213265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 751] Loss: 0.5076202087382558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 752] Loss: 0.509480804451241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 753] Loss: 0.5087023462042396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 754] Loss: 0.5083093466084512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 755] Loss: 0.5075138246526524\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 756] Loss: 0.5064725937639795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 757] Loss: 0.5058823455924447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 758] Loss: 0.5059894983085894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 759] Loss: 0.5047946773379682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 760] Loss: 0.5055202777385712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 761] Loss: 0.5053465270188701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 762] Loss: 0.5055172655080992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 763] Loss: 0.5052874383954663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 764] Loss: 0.5039597485713133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 765] Loss: 0.5050319224011665\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 766] Loss: 0.505462858476676\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 767] Loss: 0.5072519177360757\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 768] Loss: 0.5062157310718713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 769] Loss: 0.5064686304353839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 770] Loss: 0.5062071488453792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 771] Loss: 0.5068522831032559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 772] Loss: 0.5089538076906713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 773] Loss: 0.5081966285923135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 774] Loss: 0.5070091042328965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 775] Loss: 0.5057588593015131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 776] Loss: 0.5049806134145063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 777] Loss: 0.5037955289029896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 778] Loss: 0.5039431868649241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 779] Loss: 0.5036346839263094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 780] Loss: 0.5035151424231352\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 781] Loss: 0.5038930388834204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 782] Loss: 0.5033681857235291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 783] Loss: 0.5033309446586357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 784] Loss: 0.5030363829684084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 785] Loss: 0.5025401608510451\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 786] Loss: 0.5026194672437682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 787] Loss: 0.5021732051449993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 788] Loss: 0.5019766709787382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 789] Loss: 0.503097123142639\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 790] Loss: 0.5046240446822984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 791] Loss: 0.5044793932039119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 792] Loss: 0.5040689719484207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 793] Loss: 0.5028656223005625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 794] Loss: 0.5017503968531817\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 795] Loss: 0.502283205745513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 796] Loss: 0.5022848669986625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 797] Loss: 0.5019378895232071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 798] Loss: 0.5030162897892296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 799] Loss: 0.503519362283413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 800] Loss: 0.5045685256863462\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 801] Loss: 0.5048396241726335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 802] Loss: 0.5042211451657015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 803] Loss: 0.5044851369723525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 804] Loss: 0.5049017413842435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 805] Loss: 0.5063615826732021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 806] Loss: 0.5082689350017825\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 807] Loss: 0.5073309647113787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 808] Loss: 0.5067330706839593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 809] Loss: 0.5073774427075849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 810] Loss: 0.5084342747926712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 811] Loss: 0.5083952591664768\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 812] Loss: 0.5073444766812767\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 813] Loss: 0.5063114995905275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 814] Loss: 0.505873428706668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 815] Loss: 0.5054606200730214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 816] Loss: 0.5050305374989322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 817] Loss: 0.5045206036649231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 818] Loss: 0.5050176260630032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 819] Loss: 0.5063031592126032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 820] Loss: 0.507071435884122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 821] Loss: 0.5067321997842604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 822] Loss: 0.5060294536539377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 823] Loss: 0.5061467758858927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 824] Loss: 0.5056150705096828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 825] Loss: 0.5059163021189826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 826] Loss: 0.5052184839320334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 827] Loss: 0.5051610779593043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 828] Loss: 0.504726601612268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 829] Loss: 0.5042528044374012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 830] Loss: 0.5050334756728262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 831] Loss: 0.5049038471369729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 832] Loss: 0.5044560754058524\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 833] Loss: 0.5048046936468443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 834] Loss: 0.5041604851958928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 835] Loss: 0.5037305085934125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 836] Loss: 0.5032608875193478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 837] Loss: 0.5029131661223345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 838] Loss: 0.503571124202231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 839] Loss: 0.503753438410788\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 840] Loss: 0.5033703836076188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 841] Loss: 0.502728131017296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 842] Loss: 0.5029606088008507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 843] Loss: 0.5041517024194156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 844] Loss: 0.5034982470605902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 845] Loss: 0.5027576892678417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 846] Loss: 0.5026930107158565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 847] Loss: 0.5034389702343446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 848] Loss: 0.5025858606195309\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 849] Loss: 0.5021500128120799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 850] Loss: 0.5014708848122288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 851] Loss: 0.5018979102547917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 852] Loss: 0.5026669343225441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 853] Loss: 0.503282319656614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 854] Loss: 0.5041905810926542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 855] Loss: 0.5038342866776646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 856] Loss: 0.504563981785595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 857] Loss: 0.5052770050269383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 858] Loss: 0.5065800138674248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 859] Loss: 0.5065773811330084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 860] Loss: 0.5068304755432265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 861] Loss: 0.5061430844061735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 862] Loss: 0.5060310697775673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 863] Loss: 0.505240583090539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 864] Loss: 0.5046068794899068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 865] Loss: 0.5055871793501814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 866] Loss: 0.5055473990570963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 867] Loss: 0.5049739368572956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 868] Loss: 0.5046656898030356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 869] Loss: 0.5056287752967691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 870] Loss: 0.5048180834286743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 871] Loss: 0.5040524162331447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 872] Loss: 0.5049320944682669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 873] Loss: 0.5067967755653313\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 874] Loss: 0.5063320442952297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 875] Loss: 0.505621810931049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 876] Loss: 0.5051179271333855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 877] Loss: 0.5048512866370359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 878] Loss: 0.5048433125585966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 879] Loss: 0.5057289515972784\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 880] Loss: 0.5056922217881358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 881] Loss: 0.5049174383804804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 882] Loss: 0.5057634736100832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 883] Loss: 0.5051630694648855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 884] Loss: 0.504435589009109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 885] Loss: 0.5050200788577398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 886] Loss: 0.5058268519871413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 887] Loss: 0.5066901618115782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 888] Loss: 0.5076833605845138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 889] Loss: 0.5085956695290543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 890] Loss: 0.5080142705456206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 891] Loss: 0.5078792955697052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 892] Loss: 0.5088683986617009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 893] Loss: 0.5098572440698315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 894] Loss: 0.5094940442359075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 895] Loss: 0.5087127880229578\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 896] Loss: 0.509262820252174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 897] Loss: 0.5091901876282631\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 898] Loss: 0.5098552986902675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 899] Loss: 0.510014196479535\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 900] Loss: 0.5101906093649375\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 901] Loss: 0.5097093682383638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 902] Loss: 0.5093995978561591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 903] Loss: 0.509385145612952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 904] Loss: 0.5090337465635411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 905] Loss: 0.5103232181902173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 906] Loss: 0.5101385358532872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 907] Loss: 0.5095001249304346\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 908] Loss: 0.5089750945493204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 909] Loss: 0.5081696758667628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 910] Loss: 0.5089526787772775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 911] Loss: 0.5082755475270183\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 912] Loss: 0.5083369986334844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 913] Loss: 0.5080686463759496\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 914] Loss: 0.5088025006179763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 915] Loss: 0.5082249308809822\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 916] Loss: 0.508600861861788\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 917] Loss: 0.5096070605355341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 918] Loss: 0.5097217739504927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 919] Loss: 0.5098838739115334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 920] Loss: 0.5105529927625889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 921] Loss: 0.5100916348524628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 922] Loss: 0.5107995647828556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 923] Loss: 0.5111395308238253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 924] Loss: 0.5116832603291037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 925] Loss: 0.5112121424043035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 926] Loss: 0.5120783634483814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 927] Loss: 0.5119797906858458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 928] Loss: 0.5114147776479356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 929] Loss: 0.5117836517052889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 930] Loss: 0.5114889388283094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 931] Loss: 0.5108050159278221\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 932] Loss: 0.5108989316493414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 933] Loss: 0.5114606095050808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 934] Loss: 0.5119536373201968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 935] Loss: 0.5121234226577422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 936] Loss: 0.5120126921596102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 937] Loss: 0.5114766996903498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 938] Loss: 0.5112432371999058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 939] Loss: 0.510674737445958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 940] Loss: 0.5102358649289885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 941] Loss: 0.5104884495937078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 942] Loss: 0.5101857425782967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 943] Loss: 0.5094539538266752\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 944] Loss: 0.5091448250454143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 945] Loss: 0.5088638616019282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 946] Loss: 0.5090738929329662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 947] Loss: 0.5084265945841573\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 948] Loss: 0.5094194897369707\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 949] Loss: 0.5104162020265106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 950] Loss: 0.5106286995112896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 951] Loss: 0.5104511468048268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 952] Loss: 0.5101115595026793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 953] Loss: 0.5103397923048678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 954] Loss: 0.5099379569963292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 955] Loss: 0.5108502102032136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 956] Loss: 0.510891781593652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 957] Loss: 0.5103658827759275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 958] Loss: 0.5099635964392552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 959] Loss: 0.5106264642856699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 960] Loss: 0.5111058149735133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 961] Loss: 0.5110923886431294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 962] Loss: 0.5113654478856947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 963] Loss: 0.5106751396100779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 964] Loss: 0.5104645561869974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 965] Loss: 0.5112470076961831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 966] Loss: 0.5112568935085284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 967] Loss: 0.511306075905032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 968] Loss: 0.510674894701966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 969] Loss: 0.5106529891490936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 970] Loss: 0.5101521585298621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 971] Loss: 0.5094608990980591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 972] Loss: 0.5104318412738446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 973] Loss: 0.5101755934709091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 974] Loss: 0.5104674464412804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 975] Loss: 0.5112741851037549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 976] Loss: 0.5105122315013869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 977] Loss: 0.509838689158509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 978] Loss: 0.5097018583144388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 979] Loss: 0.5094270472015653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 980] Loss: 0.5088960086728664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 981] Loss: 0.5089132003407063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 982] Loss: 0.5084749487686461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 983] Loss: 0.5079791038346341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 984] Loss: 0.5073669974373866\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 985] Loss: 0.5075384275850496\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 986] Loss: 0.5072202734148302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 987] Loss: 0.5072620355170728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 988] Loss: 0.5068623302933063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 989] Loss: 0.5063113663228916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 990] Loss: 0.5064966195262969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 991] Loss: 0.5068697359048899\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 992] Loss: 0.5075330644907793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 993] Loss: 0.5071662241442604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 994] Loss: 0.5068166546893021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 995] Loss: 0.5065138906854945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 996] Loss: 0.5071531083174203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 997] Loss: 0.5068878717797003\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 998] Loss: 0.5064062240121306\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 999] Loss: 0.5060603445841491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1000] Loss: 0.505462398547299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1001] Loss: 0.5064546295618089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1002] Loss: 0.5061065957131909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1003] Loss: 0.5058668543854785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1004] Loss: 0.5053608955943633\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1005] Loss: 0.5051168175357761\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1006] Loss: 0.5061829285696149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1007] Loss: 0.5058518899098728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1008] Loss: 0.5067342852792108\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1009] Loss: 0.5067429028018443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1010] Loss: 0.5063012559115887\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1011] Loss: 0.506097350500063\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8261\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1012] Loss: 0.5057280550143158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1013] Loss: 0.5063837677477369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1014] Loss: 0.5061394590884447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1015] Loss: 0.5056716064120284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1016] Loss: 0.5051778616464656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1017] Loss: 0.5051061747048263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1018] Loss: 0.5053897814194518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1019] Loss: 0.505979804691725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1020] Loss: 0.505713262248273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1021] Loss: 0.5054166752939822\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1022] Loss: 0.5056452615826856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1023] Loss: 0.5056188799310148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1024] Loss: 0.5052379814161401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1025] Loss: 0.5059044528065376\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1026] Loss: 0.505604093302359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1027] Loss: 0.5060536827121067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1028] Loss: 0.5056369478695641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1029] Loss: 0.5068532335562972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1030] Loss: 0.5068124739309916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1031] Loss: 0.5063586505326566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1032] Loss: 0.506676167392411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1033] Loss: 0.5062385160711026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1034] Loss: 0.5069534393269142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1035] Loss: 0.5075228661866416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1036] Loss: 0.5071632303722458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1037] Loss: 0.5065821997072032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1038] Loss: 0.5072337781558885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1039] Loss: 0.5078371436192544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1040] Loss: 0.5076446388408823\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1041] Loss: 0.5077204665277861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1042] Loss: 0.5077087773024139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1043] Loss: 0.5079651072649303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1044] Loss: 0.5078297832541252\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1045] Loss: 0.5075078602984687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1046] Loss: 0.507929606641184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1047] Loss: 0.5077817711694724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1048] Loss: 0.5078209839955139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1049] Loss: 0.5073497416745312\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1050] Loss: 0.5074047732960295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1051] Loss: 0.5072015936791126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1052] Loss: 0.5066723427374424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1053] Loss: 0.5063742193124132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1054] Loss: 0.5071234811809572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1055] Loss: 0.5070392973105842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1056] Loss: 0.5081754282335222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1057] Loss: 0.5084738503916826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1058] Loss: 0.5082951306372229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1059] Loss: 0.5082931898276446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1060] Loss: 0.507762324620377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1061] Loss: 0.507296055852826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1062] Loss: 0.5070693363584038\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1063] Loss: 0.5064700258252418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1064] Loss: 0.5061034094878483\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1065] Loss: 0.505793764945623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1066] Loss: 0.5054773137831002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1067] Loss: 0.5053823787712323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1068] Loss: 0.5052520883232889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1069] Loss: 0.5048552745676638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1070] Loss: 0.5049072944160019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1071] Loss: 0.5045775144282935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1072] Loss: 0.5044664568748338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1073] Loss: 0.5048534804508487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1074] Loss: 0.5047590549744613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1075] Loss: 0.5046429266971825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1076] Loss: 0.504453848876296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1077] Loss: 0.5047271649568379\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1078] Loss: 0.504539179235277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1079] Loss: 0.5042632300740596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1080] Loss: 0.5042180481709932\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1081] Loss: 0.504394277382231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1082] Loss: 0.5039677425593763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1083] Loss: 0.5047338430690099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1084] Loss: 0.5043715061522527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1085] Loss: 0.5041354627712913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1086] Loss: 0.503982209548768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1087] Loss: 0.5036166120753132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1088] Loss: 0.5035154877752581\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1089] Loss: 0.5030844633649256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1090] Loss: 0.5029184359414824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1091] Loss: 0.5030312857714044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1092] Loss: 0.5025899162407184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1093] Loss: 0.5025582735268574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1094] Loss: 0.5022034437382874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1095] Loss: 0.5029789006098723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1096] Loss: 0.5027037651974187\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1097] Loss: 0.5029964184537656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1098] Loss: 0.5025858483752426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1099] Loss: 0.502485216271088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1100] Loss: 0.502695967888428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1101] Loss: 0.5033380451742969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1102] Loss: 0.5029862718908368\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1103] Loss: 0.5039350444209113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1104] Loss: 0.5038135541910275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1105] Loss: 0.5038985277925219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1106] Loss: 0.5035672308314567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1107] Loss: 0.5035524853509874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1108] Loss: 0.5036353708110924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1109] Loss: 0.5035761034548382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1110] Loss: 0.5032786675294241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1111] Loss: 0.5035981663848318\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1112] Loss: 0.5042453546856724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1113] Loss: 0.5050198390313839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1114] Loss: 0.5054566111193587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1115] Loss: 0.5049563371445522\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1116] Loss: 0.5046254565613498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1117] Loss: 0.5042576277275651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1118] Loss: 0.5043841497482437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1119] Loss: 0.5040698095984842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1120] Loss: 0.5043922556716888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1121] Loss: 0.5038489550595978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1122] Loss: 0.5044040943380275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1123] Loss: 0.505188021778477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1124] Loss: 0.5054239897172692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1125] Loss: 0.5049517349256732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1126] Loss: 0.504717150751453\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1127] Loss: 0.5045795508538111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1128] Loss: 0.504862691134118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1129] Loss: 0.5049179891825493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1130] Loss: 0.5050534813875153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1131] Loss: 0.5047197663697643\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1132] Loss: 0.5044232361809233\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1133] Loss: 0.5049410366945053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1134] Loss: 0.5046093514762245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1135] Loss: 0.5045665004968644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1136] Loss: 0.504247881234073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1137] Loss: 0.5044958155833933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1138] Loss: 0.5041284554276118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1139] Loss: 0.5041449529330567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1140] Loss: 0.5049062060694846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1141] Loss: 0.5055111679477283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1142] Loss: 0.5063266890665775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1143] Loss: 0.506586836294917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1144] Loss: 0.506117117564761\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1145] Loss: 0.506876494800012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1146] Loss: 0.5075275069981251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1147] Loss: 0.5076897077414454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1148] Loss: 0.507287750376803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1149] Loss: 0.5072645724565956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1150] Loss: 0.5067999032558873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1151] Loss: 0.5065120055271943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1152] Loss: 0.5070133473132258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1153] Loss: 0.5072358024667247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1154] Loss: 0.506735239525019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1155] Loss: 0.5063186195007591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1156] Loss: 0.5060661541234598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1157] Loss: 0.5057834667805726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1158] Loss: 0.5053440790945365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1159] Loss: 0.5049921968960431\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1160] Loss: 0.504716614301388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1161] Loss: 0.5045319404195531\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1162] Loss: 0.5043040221911267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1163] Loss: 0.5040418956582798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1164] Loss: 0.5053192516532513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1165] Loss: 0.5050356687480256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1166] Loss: 0.5054768659174442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1167] Loss: 0.5054635656841632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1168] Loss: 0.5050732190848122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1169] Loss: 0.5050984724157678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1170] Loss: 0.5066287904074698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1171] Loss: 0.5069040739951083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1172] Loss: 0.5071653676897377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1173] Loss: 0.5070667927621176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1174] Loss: 0.5068724490253322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1175] Loss: 0.5064792520793757\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1176] Loss: 0.506179228626393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1177] Loss: 0.5060281250206963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1178] Loss: 0.5059442602901044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1179] Loss: 0.5057374720749833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1180] Loss: 0.5058380350915354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1181] Loss: 0.5061894348543554\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1182] Loss: 0.5060145010329073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1183] Loss: 0.5066759889892802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1184] Loss: 0.5061930697245485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1185] Loss: 0.5059123639486454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1186] Loss: 0.5057175521623101\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1187] Loss: 0.506012903600711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1188] Loss: 0.5061371881833471\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1189] Loss: 0.5057590539307938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1190] Loss: 0.5057194464346941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1191] Loss: 0.5053917314862715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1192] Loss: 0.5050677538267678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1193] Loss: 0.5053771192297929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1194] Loss: 0.5052272952788057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1195] Loss: 0.5049013253111039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1196] Loss: 0.504590194227049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1197] Loss: 0.5055224555808905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1198] Loss: 0.5060913493900105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1199] Loss: 0.5064548989771412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1200] Loss: 0.5064461775879929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1201] Loss: 0.5062790149458928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1202] Loss: 0.5058856639645003\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1203] Loss: 0.5055759764594949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1204] Loss: 0.5055428021064066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1205] Loss: 0.5053044533129218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1206] Loss: 0.5056577860538302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1207] Loss: 0.506152488879186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1208] Loss: 0.5061121234152584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1209] Loss: 0.505678238658946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1210] Loss: 0.5052192528120109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1211] Loss: 0.5048963795077274\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1212] Loss: 0.5045449320025254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1213] Loss: 0.5048447279935202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1214] Loss: 0.5055287404790182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1215] Loss: 0.5052610959355712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1216] Loss: 0.5058450433544329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1217] Loss: 0.5055731324824113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1218] Loss: 0.5053029694084057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1219] Loss: 0.5050874721306168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1220] Loss: 0.504759590873416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1221] Loss: 0.505040963469846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1222] Loss: 0.5053051544165008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1223] Loss: 0.5053003699339791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1224] Loss: 0.5048459522655698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1225] Loss: 0.5045524173474812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1226] Loss: 0.504431015848281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1227] Loss: 0.5041160852631432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1228] Loss: 0.504381918288705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1229] Loss: 0.5039413502344005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1230] Loss: 0.5039071443801125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1231] Loss: 0.5035801455275196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1232] Loss: 0.5035167158351711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1233] Loss: 0.503186642237389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1234] Loss: 0.5030269674462837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1235] Loss: 0.502694798769622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1236] Loss: 0.5033358517141382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1237] Loss: 0.5031874124293806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1238] Loss: 0.5032146323554136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1239] Loss: 0.5033008167813998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1240] Loss: 0.5040464906047468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1241] Loss: 0.5037787220913473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1242] Loss: 0.5042311838456516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1243] Loss: 0.5038737560156098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1244] Loss: 0.5042752957920612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1245] Loss: 0.5046061312868482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1246] Loss: 0.5052099760581293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1247] Loss: 0.5050286355262857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1248] Loss: 0.5055783586248472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1249] Loss: 0.5052583213789699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1250] Loss: 0.5055108517609738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1251] Loss: 0.505406841595485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1252] Loss: 0.5052241153031025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1253] Loss: 0.5057037661446215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1254] Loss: 0.5062298577839649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1255] Loss: 0.5066178840478794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1256] Loss: 0.507236173281561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1257] Loss: 0.5069776823123296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1258] Loss: 0.5066536685481747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1259] Loss: 0.5065524226156509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1260] Loss: 0.5069179996450742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1261] Loss: 0.5065445326338119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1262] Loss: 0.5061385825157483\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1263] Loss: 0.5058125121541233\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1264] Loss: 0.5058985053900066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1265] Loss: 0.5060193504126657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1266] Loss: 0.5055774645830589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1267] Loss: 0.5052537649594336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1268] Loss: 0.505013098345583\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1269] Loss: 0.5048883964107599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1270] Loss: 0.5048023150155418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1271] Loss: 0.5051071962608458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1272] Loss: 0.505620263458237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1273] Loss: 0.5057416415308281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1274] Loss: 0.5055273871537278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1275] Loss: 0.5052302141594731\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1276] Loss: 0.5049649917814812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1277] Loss: 0.5053706160968721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1278] Loss: 0.505185290511387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1279] Loss: 0.5054611286586841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1280] Loss: 0.5051366905500363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1281] Loss: 0.5049990094743969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1282] Loss: 0.504874872265702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1283] Loss: 0.5046897937687064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1284] Loss: 0.5045120861256154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1285] Loss: 0.5045037321890554\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1286] Loss: 0.5041643848316264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1287] Loss: 0.5038866719262174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1288] Loss: 0.5034758760605802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1289] Loss: 0.5031349447212721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1290] Loss: 0.5030389116551631\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1291] Loss: 0.5030469538193201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1292] Loss: 0.5031824282101353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1293] Loss: 0.5036023454990424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1294] Loss: 0.5032451522859688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1295] Loss: 0.5035230995553315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1296] Loss: 0.5038273699069751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1297] Loss: 0.5041039985766708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1298] Loss: 0.5042721446660267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1299] Loss: 0.503919321793416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1300] Loss: 0.5038429894967924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1301] Loss: 0.5037012643330318\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1302] Loss: 0.5033710913713834\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1303] Loss: 0.5031798858698199\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1304] Loss: 0.5031052594558718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1305] Loss: 0.5029906021911393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1306] Loss: 0.5034808168496618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1307] Loss: 0.5034848365797153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1308] Loss: 0.5037523586685795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1309] Loss: 0.5034238929033876\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1310] Loss: 0.5033138685114682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1311] Loss: 0.5038521847363269\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1312] Loss: 0.5037565729453082\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1313] Loss: 0.5042117856544991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1314] Loss: 0.5041378913493595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1315] Loss: 0.5040931198722828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1316] Loss: 0.5044691543240405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1317] Loss: 0.504097908144399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1318] Loss: 0.5039544187781244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1319] Loss: 0.50422450679047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1320] Loss: 0.5047040190593696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1321] Loss: 0.504543299114954\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1322] Loss: 0.5042524063234846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1323] Loss: 0.5042002168821966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1324] Loss: 0.5040232746768056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1325] Loss: 0.5043404766752676\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1326] Loss: 0.5041057917417264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1327] Loss: 0.5037704726801946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1328] Loss: 0.5042615556374447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1329] Loss: 0.5038753161536614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1330] Loss: 0.5034788940737888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1331] Loss: 0.5031887366913994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1332] Loss: 0.5034459743766599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1333] Loss: 0.5037787185115049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1334] Loss: 0.5038782559986253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1335] Loss: 0.5041838145978523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1336] Loss: 0.5038672630961525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1337] Loss: 0.5042373495674191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1338] Loss: 0.5042575124993128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1339] Loss: 0.5044305507358797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1340] Loss: 0.5042889863791236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1341] Loss: 0.5040684237843099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1342] Loss: 0.5040147334086493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1343] Loss: 0.5037627634631485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1344] Loss: 0.5044554341361105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1345] Loss: 0.5048411951272074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1346] Loss: 0.5056607026838514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1347] Loss: 0.5054250908047495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1348] Loss: 0.505442366697481\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1349] Loss: 0.5056417285507996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1350] Loss: 0.5060391462452355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1351] Loss: 0.5065043475943996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1352] Loss: 0.5063456536719748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1353] Loss: 0.5061951929953034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1354] Loss: 0.5062615732128304\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1355] Loss: 0.5061939819500997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1356] Loss: 0.5059370130969841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1357] Loss: 0.5056152814535211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1358] Loss: 0.5054947690300222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1359] Loss: 0.5052638411943146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1360] Loss: 0.5054894847028396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1361] Loss: 0.5053109753566118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1362] Loss: 0.5052280005224994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1363] Loss: 0.5056954260657849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1364] Loss: 0.5057632863451781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1365] Loss: 0.505644952379472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1366] Loss: 0.5054724092865102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1367] Loss: 0.50516408073304\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1368] Loss: 0.5056650798012327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1369] Loss: 0.5056608269669818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1370] Loss: 0.506223748832248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1371] Loss: 0.5061165794609038\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1372] Loss: 0.5062152702197121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1373] Loss: 0.5064685762895493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1374] Loss: 0.5065860911444934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1375] Loss: 0.507014044550802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1376] Loss: 0.5067505269692071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1377] Loss: 0.50704046588868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1378] Loss: 0.5068276607289842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1379] Loss: 0.506589630187861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1380] Loss: 0.5063796529139596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1381] Loss: 0.5061573936389043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1382] Loss: 0.506033048409661\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1383] Loss: 0.5059501364536985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1384] Loss: 0.5058397171972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1385] Loss: 0.5056352413722447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1386] Loss: 0.505262299232423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1387] Loss: 0.5050714187060685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1388] Loss: 0.504784593464709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1389] Loss: 0.5046662602645681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1390] Loss: 0.5051725364374844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1391] Loss: 0.5049584532805659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1392] Loss: 0.5049707732604745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1393] Loss: 0.5055528646788537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1394] Loss: 0.5059308470205753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1395] Loss: 0.506260686388797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1396] Loss: 0.5065811145756368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1397] Loss: 0.5067439294150341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1398] Loss: 0.5068468023762778\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1399] Loss: 0.5073368303117387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1400] Loss: 0.5071603435311425\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1401] Loss: 0.5073330737357723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1402] Loss: 0.5075891846999726\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1403] Loss: 0.5073383595848671\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1404] Loss: 0.5075187848951725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1405] Loss: 0.5071522772811644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1406] Loss: 0.5077073838994173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1407] Loss: 0.507462662116208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1408] Loss: 0.5073454610190307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1409] Loss: 0.5075366326480342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1410] Loss: 0.5079639889631007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1411] Loss: 0.5078748248095782\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1412] Loss: 0.5076155879776388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1413] Loss: 0.5079983635697254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1414] Loss: 0.5078120625091601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1415] Loss: 0.5075873819339342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1416] Loss: 0.5076675836580742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1417] Loss: 0.5077948532628908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1418] Loss: 0.5080169098459939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1419] Loss: 0.5080258428427514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1420] Loss: 0.5084794422263628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1421] Loss: 0.5083958029779723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1422] Loss: 0.5088950148048369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1423] Loss: 0.509230214771932\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1424] Loss: 0.5088991282479246\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1425] Loss: 0.5086771683289054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1426] Loss: 0.509059784551635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1427] Loss: 0.5090511585854912\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1428] Loss: 0.5093951149638701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1429] Loss: 0.5096455984652626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1430] Loss: 0.5092683677764043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1431] Loss: 0.5094859458659811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1432] Loss: 0.509934202935587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1433] Loss: 0.5100112459576918\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1434] Loss: 0.5103989758597308\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1435] Loss: 0.5102566904635043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1436] Loss: 0.5099253866893941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1437] Loss: 0.5101298849302331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1438] Loss: 0.510300410526066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1439] Loss: 0.511863673657211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1440] Loss: 0.5117092681187455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1441] Loss: 0.5113862823818988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1442] Loss: 0.511382899554886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1443] Loss: 0.5118520469240053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1444] Loss: 0.5122304282268825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1445] Loss: 0.5121917396783828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1446] Loss: 0.5121019874723294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1447] Loss: 0.5120506028196473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1448] Loss: 0.5117348443502302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1449] Loss: 0.5117285367303763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1450] Loss: 0.5118932484470783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1451] Loss: 0.5122000987579162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1452] Loss: 0.5123459263497098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1453] Loss: 0.512054475595185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1454] Loss: 0.5120903883066218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1455] Loss: 0.5119489297980354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1456] Loss: 0.5118331045983457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1457] Loss: 0.5116735287994115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1458] Loss: 0.5115725837867974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1459] Loss: 0.5114074757654373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1460] Loss: 0.511480875203484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1461] Loss: 0.5113001014845354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1462] Loss: 0.5112220101183703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1463] Loss: 0.5111094598442409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1464] Loss: 0.5111106057774346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1465] Loss: 0.5112946691625405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1466] Loss: 0.5110915322869891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1467] Loss: 0.5108704065890397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1468] Loss: 0.5105499764107214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1469] Loss: 0.5106721863663358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1470] Loss: 0.5104757262859494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1471] Loss: 0.510167324502684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1472] Loss: 0.5106237088165512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1473] Loss: 0.5105549182011703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1474] Loss: 0.5101986657132499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1475] Loss: 0.5098618184655441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1476] Loss: 0.5096614479895211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1477] Loss: 0.5097377120277381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1478] Loss: 0.5097889113278429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1479] Loss: 0.5096992757113725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1480] Loss: 0.5094678899369289\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1481] Loss: 0.5098560568171598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1482] Loss: 0.5099495290299502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1483] Loss: 0.5096919116662683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1484] Loss: 0.5094431389956994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1485] Loss: 0.5092192323696919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1486] Loss: 0.5090322329006234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1487] Loss: 0.5089170837902608\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1488] Loss: 0.5091518390702812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1489] Loss: 0.5088222756662213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1490] Loss: 0.5089798888229594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1491] Loss: 0.5087900263058909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1492] Loss: 0.5088248644820784\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1493] Loss: 0.5089537360509317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1494] Loss: 0.5086437748305923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1495] Loss: 0.5088293411858796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1496] Loss: 0.5090796460093397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1497] Loss: 0.5090069594626007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1498] Loss: 0.5090341642951435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1499] Loss: 0.5089527259611867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1500] Loss: 0.5087896519387611\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1501] Loss: 0.509757812415916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1502] Loss: 0.5096263193465289\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1503] Loss: 0.5097037942898117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1504] Loss: 0.5101237802108529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1505] Loss: 0.5100705950254172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1506] Loss: 0.51024849241577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1507] Loss: 0.5100671242597469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1508] Loss: 0.5102147988691359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1509] Loss: 0.5100688628695749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1510] Loss: 0.5099113388508558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1511] Loss: 0.509661727956125\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8189\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1512] Loss: 0.5094036902644915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1513] Loss: 0.5092501646708159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1514] Loss: 0.5094121456354025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1515] Loss: 0.5091151408443404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1516] Loss: 0.5088223637364968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1517] Loss: 0.5093240233761551\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1518] Loss: 0.5096787972494013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1519] Loss: 0.5094673658533777\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1520] Loss: 0.5095668730316776\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1521] Loss: 0.5093402974712861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1522] Loss: 0.5095471297004242\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1523] Loss: 0.509819580339491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1524] Loss: 0.5097482765949455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1525] Loss: 0.5096720739098018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1526] Loss: 0.5095099672293804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1527] Loss: 0.5094066969500882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1528] Loss: 0.509215788355862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1529] Loss: 0.5091324579464208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1530] Loss: 0.5090586507875545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1531] Loss: 0.5087535241354224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1532] Loss: 0.5091057750606024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1533] Loss: 0.5092506045848859\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1534] Loss: 0.5096368778176839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1535] Loss: 0.5097345926848853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1536] Loss: 0.5096958698904538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1537] Loss: 0.5094575410547368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1538] Loss: 0.5092370340071531\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1539] Loss: 0.5090668836157801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1540] Loss: 0.5088701795461108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1541] Loss: 0.5089517433908122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1542] Loss: 0.5090206328259651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1543] Loss: 0.5088291344632253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1544] Loss: 0.5087172254574137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1545] Loss: 0.5091957541911498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1546] Loss: 0.5090191288775688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1547] Loss: 0.5093189620896958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1548] Loss: 0.509334114518354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1549] Loss: 0.5091552494149351\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1550] Loss: 0.5089717794352999\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1551] Loss: 0.5090253729154924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1552] Loss: 0.5089633470962464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1553] Loss: 0.5086455280720216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1554] Loss: 0.5085248058354946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1555] Loss: 0.5088622754983354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1556] Loss: 0.5087573114162878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1557] Loss: 0.5089316618732873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1558] Loss: 0.5087248170392886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1559] Loss: 0.5090288229364799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1560] Loss: 0.5090296948381833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1561] Loss: 0.5090474571844831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1562] Loss: 0.5090363183600595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1563] Loss: 0.5091062397444011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1564] Loss: 0.5095485344642253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1565] Loss: 0.5092669116518509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1566] Loss: 0.5093839803682358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1567] Loss: 0.5096664542196602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1568] Loss: 0.5097444250841673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1569] Loss: 0.5096057068215326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1570] Loss: 0.5093981911129547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1571] Loss: 0.5093730675314644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1572] Loss: 0.5099181060989698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1573] Loss: 0.5101573160119698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1574] Loss: 0.5102902629125611\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1575] Loss: 0.5100620109850252\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1576] Loss: 0.5105424081770386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1577] Loss: 0.5103990241340531\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1578] Loss: 0.5103872271484873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1579] Loss: 0.5101088800644407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1580] Loss: 0.5100508503267698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1581] Loss: 0.510099761904566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1582] Loss: 0.5104975709274634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1583] Loss: 0.5106374043120698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1584] Loss: 0.510632934683528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1585] Loss: 0.5107768014974372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1586] Loss: 0.511045655630335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1587] Loss: 0.5109571038071713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1588] Loss: 0.5107437022210052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1589] Loss: 0.5107423220109454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1590] Loss: 0.5107851839727826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1591] Loss: 0.5111725946249066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1592] Loss: 0.5109142874283182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1593] Loss: 0.5106343619683663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1594] Loss: 0.5103834492061631\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1595] Loss: 0.5101027287783161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1596] Loss: 0.5102644865272453\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1597] Loss: 0.5100162070254832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1598] Loss: 0.5104063164086684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1599] Loss: 0.510478897733465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1600] Loss: 0.5102639011150106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1601] Loss: 0.5102758151773572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1602] Loss: 0.5100255174350826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1603] Loss: 0.5100428540283458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1604] Loss: 0.5099061896694208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1605] Loss: 0.5102834344182384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1606] Loss: 0.5100011038627937\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1607] Loss: 0.5096696748609638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1608] Loss: 0.5095352892558649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1609] Loss: 0.5097006482357758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1610] Loss: 0.5095602344924753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1611] Loss: 0.5096767929117859\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1612] Loss: 0.5096777761371944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1613] Loss: 0.51046606890202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1614] Loss: 0.5102974831572046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1615] Loss: 0.5107988794195166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1616] Loss: 0.5107207651002498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1617] Loss: 0.5105834516352539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1618] Loss: 0.5108546909711421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1619] Loss: 0.5106999902331797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1620] Loss: 0.5105808027692743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1621] Loss: 0.5104788775169345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1622] Loss: 0.5102864116591086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1623] Loss: 0.5102227015315361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1624] Loss: 0.5103463395707697\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1625] Loss: 0.5107396538482118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1626] Loss: 0.510638780384508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1627] Loss: 0.5104481244631505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1628] Loss: 0.5103633360060895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1629] Loss: 0.510130679532819\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1630] Loss: 0.5100019074178168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1631] Loss: 0.5102426803984034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1632] Loss: 0.5100807451149997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1633] Loss: 0.5100464492585122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1634] Loss: 0.5100631586332338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1635] Loss: 0.5102670941087935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1636] Loss: 0.5103094172043656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1637] Loss: 0.5102062982960729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1638] Loss: 0.5101188906593949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1639] Loss: 0.5102540029953601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1640] Loss: 0.5105231840262371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1641] Loss: 0.5102850295978769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1642] Loss: 0.5103802771797871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1643] Loss: 0.5103371709401062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1644] Loss: 0.5106819075987242\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1645] Loss: 0.5111295059914106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1646] Loss: 0.5114613409403345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1647] Loss: 0.5111727710606869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1648] Loss: 0.5114745869315991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1649] Loss: 0.5113671633420648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1650] Loss: 0.5112081801682188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1651] Loss: 0.5110599401486119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1652] Loss: 0.510945832849384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1653] Loss: 0.5111415916801229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1654] Loss: 0.5109605579213663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1655] Loss: 0.5107513621124118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1656] Loss: 0.5108492873466036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1657] Loss: 0.51067378285561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1658] Loss: 0.5109379767904298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1659] Loss: 0.5112929119816856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1660] Loss: 0.5113141502245613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1661] Loss: 0.5117215895580272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1662] Loss: 0.5119584780962517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1663] Loss: 0.5118787139783397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1664] Loss: 0.5121223810847132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1665] Loss: 0.512157053555245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1666] Loss: 0.5121288977589161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1667] Loss: 0.5121865586772909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1668] Loss: 0.5124681804052494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1669] Loss: 0.5122317485993437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1670] Loss: 0.5120600894221972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1671] Loss: 0.5123323937950208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1672] Loss: 0.5128648537187076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1673] Loss: 0.512953417998701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1674] Loss: 0.5127405200243201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1675] Loss: 0.5133034545080856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1676] Loss: 0.5135447821143765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1677] Loss: 0.5133035021555556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1678] Loss: 0.5132343807602173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1679] Loss: 0.5135678164820268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1680] Loss: 0.513607913790605\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1681] Loss: 0.513399466375934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1682] Loss: 0.5131887043651461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1683] Loss: 0.5136163637918585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1684] Loss: 0.5138945187326184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1685] Loss: 0.5136727933173484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1686] Loss: 0.5141874676963099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1687] Loss: 0.5142444022246712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1688] Loss: 0.514046974290409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1689] Loss: 0.513975998650576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1690] Loss: 0.5141032452553006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1691] Loss: 0.5139474342628417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1692] Loss: 0.5137003901747076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1693] Loss: 0.5141877048315885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1694] Loss: 0.5142366131515922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1695] Loss: 0.5140385098598175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1696] Loss: 0.5143529937601009\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1697] Loss: 0.5145564102383351\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1698] Loss: 0.5143986052836633\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1699] Loss: 0.5143124830632575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1700] Loss: 0.5142592154380654\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1701] Loss: 0.5141219699663039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1702] Loss: 0.5140210684363874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1703] Loss: 0.513920843076626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1704] Loss: 0.5137723825085703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1705] Loss: 0.5135565495890054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1706] Loss: 0.5135683809036395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1707] Loss: 0.5138894843377964\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1708] Loss: 0.5141974403484039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1709] Loss: 0.5140823894386196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1710] Loss: 0.5142270164688428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1711] Loss: 0.5140237955378056\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1712] Loss: 0.5138356410177695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1713] Loss: 0.5137447646183464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1714] Loss: 0.5138027605869842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1715] Loss: 0.5139108351404736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1716] Loss: 0.51382249742005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1717] Loss: 0.513609581183281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1718] Loss: 0.513974378785945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1719] Loss: 0.5141906757015547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1720] Loss: 0.5141427705110597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1721] Loss: 0.5140175039762312\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1722] Loss: 0.5139503686292337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1723] Loss: 0.5138254336896064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1724] Loss: 0.5138828532748796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1725] Loss: 0.5137230981769876\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1726] Loss: 0.5135741400238323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1727] Loss: 0.5135112560814095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1728] Loss: 0.5138138159497814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1729] Loss: 0.5138655982367615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1730] Loss: 0.514132610134414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1731] Loss: 0.5140185681012299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1732] Loss: 0.5138244873763301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1733] Loss: 0.5135869837791727\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1734] Loss: 0.5135153293901799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1735] Loss: 0.5136656425923717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1736] Loss: 0.513480460906301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1737] Loss: 0.5133540786274488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1738] Loss: 0.5131468888408587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1739] Loss: 0.5132102888226024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1740] Loss: 0.5131903881706843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1741] Loss: 0.5134467178733246\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1742] Loss: 0.5133857698748251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1743] Loss: 0.5135413348094681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1744] Loss: 0.5134089460877197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1745] Loss: 0.5132741146966031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1746] Loss: 0.5131674716821766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1747] Loss: 0.512933926974561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1748] Loss: 0.5130396477384598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1749] Loss: 0.5130207750516327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1750] Loss: 0.5133249723382535\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1751] Loss: 0.5131801046449076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1752] Loss: 0.5129582451329523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1753] Loss: 0.5128838395338718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1754] Loss: 0.5131750679260473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1755] Loss: 0.5130783981946577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1756] Loss: 0.5132142531522587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1757] Loss: 0.5130073146262739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1758] Loss: 0.5135443193049958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1759] Loss: 0.513485212448695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1760] Loss: 0.513231808257103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1761] Loss: 0.513122184599618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1762] Loss: 0.5130032663004467\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1763] Loss: 0.5131019365521117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1764] Loss: 0.5129118621872183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1765] Loss: 0.5131639569166647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1766] Loss: 0.5132190196235089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1767] Loss: 0.513117008320187\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1768] Loss: 0.5131291972387386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1769] Loss: 0.5132293547544335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1770] Loss: 0.5131083644098706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1771] Loss: 0.5130424382996691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1772] Loss: 0.5129839621850314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1773] Loss: 0.5128582712684655\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1774] Loss: 0.5127895117937764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1775] Loss: 0.5126356147023529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1776] Loss: 0.5127103757613455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1777] Loss: 0.512517825956879\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1778] Loss: 0.5123850022110653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1779] Loss: 0.5125695439283072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1780] Loss: 0.5127116429993487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1781] Loss: 0.5126620554999045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1782] Loss: 0.5124961486402547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1783] Loss: 0.5127337214896179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1784] Loss: 0.512471854359239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1785] Loss: 0.5123403911964566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1786] Loss: 0.5122651658648607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1787] Loss: 0.5120829248577706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1788] Loss: 0.5122206600059366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1789] Loss: 0.5120913053602796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1790] Loss: 0.5120904074050486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1791] Loss: 0.5123310122612768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1792] Loss: 0.5126410315449635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1793] Loss: 0.5125594053385505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1794] Loss: 0.5129209730623295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1795] Loss: 0.5127788287192467\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1796] Loss: 0.5132054428383128\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1797] Loss: 0.513350288351099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1798] Loss: 0.5132467221491825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1799] Loss: 0.5131140873818734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1800] Loss: 0.5133863071600596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1801] Loss: 0.5137796950931239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1802] Loss: 0.5136702415928383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1803] Loss: 0.5134476096313865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1804] Loss: 0.513334983824486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1805] Loss: 0.5131417785256986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1806] Loss: 0.5130248746639232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1807] Loss: 0.5128482323636803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1808] Loss: 0.512900764831501\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1809] Loss: 0.5127416197224889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1810] Loss: 0.5128452450151627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1811] Loss: 0.5131216458432222\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1812] Loss: 0.5131544517596379\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1813] Loss: 0.5130867455226132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1814] Loss: 0.5130115001365269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1815] Loss: 0.513268160991285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1816] Loss: 0.5133525671560037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1817] Loss: 0.5131839223850202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1818] Loss: 0.5130026408952493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1819] Loss: 0.5129492558009575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1820] Loss: 0.51287554112327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1821] Loss: 0.5128487505524509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1822] Loss: 0.5127996331784965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1823] Loss: 0.513018953021664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1824] Loss: 0.5133393958120223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1825] Loss: 0.5135192388930702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1826] Loss: 0.5135458531282774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1827] Loss: 0.5133685658692227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1828] Loss: 0.5136313804161025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1829] Loss: 0.5135100057646576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1830] Loss: 0.513803821113525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1831] Loss: 0.5137171845806989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1832] Loss: 0.5136561065844617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1833] Loss: 0.5140071907168703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1834] Loss: 0.5139353808206915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1835] Loss: 0.5138917558485607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1836] Loss: 0.5137299877418652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1837] Loss: 0.5136582967407648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1838] Loss: 0.513916022009609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1839] Loss: 0.5137857286507604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1840] Loss: 0.5136593802867079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1841] Loss: 0.5134788127679022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1842] Loss: 0.5133898717437958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1843] Loss: 0.5136009219639837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1844] Loss: 0.5137276669693315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1845] Loss: 0.5136789240752266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1846] Loss: 0.5139241410519727\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1847] Loss: 0.5138950388803354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1848] Loss: 0.5138605514649853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1849] Loss: 0.5138382762225374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1850] Loss: 0.5141174384128692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1851] Loss: 0.5140936788807691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1852] Loss: 0.5139185986224833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1853] Loss: 0.5137213725077955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1854] Loss: 0.5135154175084262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1855] Loss: 0.5132689435464299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1856] Loss: 0.5136805267293244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1857] Loss: 0.5134807918677086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1858] Loss: 0.5137663736762562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1859] Loss: 0.5135486041995841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1860] Loss: 0.5134405129927176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1861] Loss: 0.5132191595128516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1862] Loss: 0.5131601850287273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1863] Loss: 0.5129222349268546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1864] Loss: 0.5126900842614153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1865] Loss: 0.5128300275089996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1866] Loss: 0.5127563092604851\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1867] Loss: 0.5129067862877181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1868] Loss: 0.5127814960102269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1869] Loss: 0.5127133134316662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1870] Loss: 0.5130282098536982\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1871] Loss: 0.5129061009759504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1872] Loss: 0.5129859295623068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1873] Loss: 0.5130120438857719\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1874] Loss: 0.5128174746665787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1875] Loss: 0.5126554735195943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1876] Loss: 0.5129050586487154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1877] Loss: 0.5129139158921078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1878] Loss: 0.5129119623779205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1879] Loss: 0.5132079049781607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1880] Loss: 0.5135797161472975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1881] Loss: 0.5134859373976417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1882] Loss: 0.5137652791059052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1883] Loss: 0.5140037670981163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1884] Loss: 0.5139260295089849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1885] Loss: 0.5138519558256323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1886] Loss: 0.5138207850977778\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1887] Loss: 0.513849066255226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1888] Loss: 0.5141169772482057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1889] Loss: 0.5142990893821426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1890] Loss: 0.5143448264486548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1891] Loss: 0.5143773717823277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1892] Loss: 0.5145334902228909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1893] Loss: 0.5144618700271228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1894] Loss: 0.5146068669855595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1895] Loss: 0.51442594209733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1896] Loss: 0.5141768155116884\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1897] Loss: 0.5142946389182145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1898] Loss: 0.5141534280321783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1899] Loss: 0.5143819443393388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1900] Loss: 0.5142453362187036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1901] Loss: 0.5145588384781871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1902] Loss: 0.5148600899967654\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1903] Loss: 0.5148102263067578\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1904] Loss: 0.5146788200056194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1905] Loss: 0.5147263138097674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1906] Loss: 0.5146017549799643\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1907] Loss: 0.5144070959146653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1908] Loss: 0.5144880604270702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1909] Loss: 0.5144031862964623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1910] Loss: 0.5142035271972418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1911] Loss: 0.5141022998911411\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1912] Loss: 0.5140243118333918\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1913] Loss: 0.5138887450874319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1914] Loss: 0.5142494117538644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1915] Loss: 0.5140443210924224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1916] Loss: 0.5139070223608871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1917] Loss: 0.514137402060359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1918] Loss: 0.5142470145682719\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1919] Loss: 0.5145067921168759\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1920] Loss: 0.5147421518961589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1921] Loss: 0.514506633223835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1922] Loss: 0.5144474131421901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1923] Loss: 0.5143958553830024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1924] Loss: 0.5142837859890417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1925] Loss: 0.5140560062319146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1926] Loss: 0.5139534713142673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1927] Loss: 0.514058446223751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1928] Loss: 0.5143239248664826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1929] Loss: 0.5143507372291262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1930] Loss: 0.5142354529718278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1931] Loss: 0.5144846419017974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1932] Loss: 0.5147585363118122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1933] Loss: 0.5146042280480297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1934] Loss: 0.5146451832077811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1935] Loss: 0.5147746030698743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1936] Loss: 0.5145913130103456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1937] Loss: 0.5145220456001386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1938] Loss: 0.5146416329076978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1939] Loss: 0.5144240679004294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1940] Loss: 0.5143511582192007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1941] Loss: 0.514517183584154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1942] Loss: 0.5144952847076029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1943] Loss: 0.5147975776487518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1944] Loss: 0.51461480569349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1945] Loss: 0.5144500217059763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1946] Loss: 0.5145508119094505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1947] Loss: 0.5145116353105487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1948] Loss: 0.5146415789141443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1949] Loss: 0.5149618240695931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1950] Loss: 0.5149151787679228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1951] Loss: 0.514872733221792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1952] Loss: 0.5150344993572956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1953] Loss: 0.5153286134151642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1954] Loss: 0.5152725389232431\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1955] Loss: 0.5152012499249106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1956] Loss: 0.5153408207277539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1957] Loss: 0.515519869444942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1958] Loss: 0.5153760355459887\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1959] Loss: 0.5154611877094392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1960] Loss: 0.5153593637511648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1961] Loss: 0.5153598301805848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1962] Loss: 0.515473855787647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1963] Loss: 0.5157309112082\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1964] Loss: 0.515645439238217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1965] Loss: 0.5154693353114669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1966] Loss: 0.5153201487181442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1967] Loss: 0.5151995038262114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1968] Loss: 0.5154004189273949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1969] Loss: 0.5158024130719749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1970] Loss: 0.515851847061964\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1971] Loss: 0.5161431968558904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1972] Loss: 0.5160718232739804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1973] Loss: 0.515949351762715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1974] Loss: 0.5159679557517415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1975] Loss: 0.5158622930786309\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1976] Loss: 0.5158697112500261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1977] Loss: 0.5158725777347639\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1978] Loss: 0.516129851006276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1979] Loss: 0.5165434514100735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1980] Loss: 0.5165260166031163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1981] Loss: 0.516439407101543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1982] Loss: 0.5165136469750786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1983] Loss: 0.5163299119642456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1984] Loss: 0.516208242003274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1985] Loss: 0.5164495347313962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1986] Loss: 0.5163030839628644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1987] Loss: 0.5162860458576768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1988] Loss: 0.5161255948114459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1989] Loss: 0.5161028946872657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1990] Loss: 0.5160838679687397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1991] Loss: 0.515942037990977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1992] Loss: 0.5157371163167129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1993] Loss: 0.5157140343276054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1994] Loss: 0.5155940208071969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1995] Loss: 0.5155620781861572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1996] Loss: 0.5155654212066335\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1997] Loss: 0.5155298695238789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1998] Loss: 0.5153919869372922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 1999] Loss: 0.5155732329456978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2000] Loss: 0.5155003630874941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2001] Loss: 0.5153785514239894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2002] Loss: 0.5155872699402613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2003] Loss: 0.5157263765619333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2004] Loss: 0.5156487016234212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2005] Loss: 0.5158479588087586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2006] Loss: 0.5157384198298429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2007] Loss: 0.5155282194565038\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2008] Loss: 0.5155207124428055\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2009] Loss: 0.5155742132242239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2010] Loss: 0.5155897446076075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2011] Loss: 0.515630651123916\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8007\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2012] Loss: 0.5156564625617509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2013] Loss: 0.5158215342802123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2014] Loss: 0.5157712420765707\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2015] Loss: 0.5157458868929714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2016] Loss: 0.5159483123743834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2017] Loss: 0.5157170527686958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2018] Loss: 0.5159963475495813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2019] Loss: 0.5157667744270297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2020] Loss: 0.5156444950806384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2021] Loss: 0.5154698340802852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2022] Loss: 0.5157319614023128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2023] Loss: 0.5159163341093599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2024] Loss: 0.5158087310813062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2025] Loss: 0.5157783704622351\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2026] Loss: 0.5159637111393947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2027] Loss: 0.5158907737713074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2028] Loss: 0.5160956982724437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2029] Loss: 0.5163356367108695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2030] Loss: 0.5165300857079657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2031] Loss: 0.5163996175744673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2032] Loss: 0.516285630234281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2033] Loss: 0.5162267585725904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2034] Loss: 0.5160625119378247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2035] Loss: 0.5163437939862736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2036] Loss: 0.5163598460966763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2037] Loss: 0.5162496991265265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2038] Loss: 0.5163793071363297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2039] Loss: 0.5166118048506675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2040] Loss: 0.516724068841903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2041] Loss: 0.5168984339059683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2042] Loss: 0.5169556485805126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2043] Loss: 0.5168089559672319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2044] Loss: 0.5167981007147333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2045] Loss: 0.5167013875242165\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2046] Loss: 0.516887424843541\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2047] Loss: 0.5167194103256169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2048] Loss: 0.5168288385565786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2049] Loss: 0.5167023720603395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2050] Loss: 0.5168240930546414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2051] Loss: 0.5168249412336418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2052] Loss: 0.516660601538443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2053] Loss: 0.5165124037438836\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2054] Loss: 0.5164652197124736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2055] Loss: 0.5165515072330302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2056] Loss: 0.5166961239773736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2057] Loss: 0.5165958637257275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2058] Loss: 0.5165833059014892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2059] Loss: 0.5165480349701093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2060] Loss: 0.5163400419873576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2061] Loss: 0.5163107636203925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2062] Loss: 0.516147141296839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2063] Loss: 0.5162617401084973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2064] Loss: 0.5162997672686706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2065] Loss: 0.5162512471246566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2066] Loss: 0.5160735408299073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2067] Loss: 0.5161695198784597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2068] Loss: 0.5164523994088632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2069] Loss: 0.5163407978184182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2070] Loss: 0.5163763228326271\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2071] Loss: 0.516361794988747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2072] Loss: 0.5163172963295948\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2073] Loss: 0.516631288076164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2074] Loss: 0.51663611499626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2075] Loss: 0.5168497185927992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2076] Loss: 0.5166656387396547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2077] Loss: 0.516693756003325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2078] Loss: 0.5171286366328749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2079] Loss: 0.5170747304490754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2080] Loss: 0.5168771912053133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2081] Loss: 0.5167279967659981\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2082] Loss: 0.5168360215694698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2083] Loss: 0.5169154071288736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2084] Loss: 0.5172954134198126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2085] Loss: 0.5176309381117896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2086] Loss: 0.517623044767885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2087] Loss: 0.5178273914638167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2088] Loss: 0.5176626365957605\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2089] Loss: 0.5176585360351735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2090] Loss: 0.5176124679042569\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2091] Loss: 0.5176244275450028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2092] Loss: 0.5177210330040054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2093] Loss: 0.5179264318675688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2094] Loss: 0.5177979415395495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2095] Loss: 0.5175873057793368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2096] Loss: 0.5174190907599345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2097] Loss: 0.517268968674471\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2098] Loss: 0.5171475967753594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2099] Loss: 0.517053907522621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2100] Loss: 0.5171425580322367\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2101] Loss: 0.5170766556603889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2102] Loss: 0.5170690896616659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2103] Loss: 0.5169216400504636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2104] Loss: 0.5169482694534465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2105] Loss: 0.5168536681086292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2106] Loss: 0.5167058493503204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2107] Loss: 0.5166780116431924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2108] Loss: 0.516911512182487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2109] Loss: 0.5170109954996359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2110] Loss: 0.5173401266429574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2111] Loss: 0.5171661571384891\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2112] Loss: 0.5171247114719225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2113] Loss: 0.5169708311130163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2114] Loss: 0.5168758133087223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2115] Loss: 0.5166800497384086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2116] Loss: 0.5165512599561313\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2117] Loss: 0.5164025550628198\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2118] Loss: 0.5163000619400349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2119] Loss: 0.5160893196549158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2120] Loss: 0.5164048464309354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2121] Loss: 0.5165095624440506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2122] Loss: 0.5164699397574435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2123] Loss: 0.516329156811091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2124] Loss: 0.5161509246391139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2125] Loss: 0.516117991668521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2126] Loss: 0.5160151016575718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2127] Loss: 0.5162788284049774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2128] Loss: 0.516460737381568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2129] Loss: 0.5165755474379653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2130] Loss: 0.5168061201495153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2131] Loss: 0.5167328127275492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2132] Loss: 0.5167472542489795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2133] Loss: 0.5168758391213872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2134] Loss: 0.5168580909355961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2135] Loss: 0.5171456091862459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2136] Loss: 0.5170919275023547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2137] Loss: 0.516980193907964\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2138] Loss: 0.5169509019638602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2139] Loss: 0.5171146257531797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2140] Loss: 0.5170942633704174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2141] Loss: 0.517055448736244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2142] Loss: 0.5169289331682319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2143] Loss: 0.516845494791619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2144] Loss: 0.5168326198233969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2145] Loss: 0.5168179015227414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2146] Loss: 0.5169731690514204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2147] Loss: 0.5168013018245877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2148] Loss: 0.5166037606396081\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2149] Loss: 0.516651322641265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2150] Loss: 0.516575057123129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2151] Loss: 0.5165096904122197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2152] Loss: 0.5164207743251658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2153] Loss: 0.5162425439983914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2154] Loss: 0.5161409973826286\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2155] Loss: 0.5160114945521108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2156] Loss: 0.5159652445596594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2157] Loss: 0.5158380132211506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2158] Loss: 0.5156647611988111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2159] Loss: 0.515542998774403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2160] Loss: 0.5154760759707653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2161] Loss: 0.5153566829765442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2162] Loss: 0.5154131581470117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2163] Loss: 0.5155072107469394\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2164] Loss: 0.5155381756545266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2165] Loss: 0.5154808749245012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2166] Loss: 0.5153746418834885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2167] Loss: 0.5153928422107708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2168] Loss: 0.515320264944409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2169] Loss: 0.5152706541951843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2170] Loss: 0.5154669756271753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2171] Loss: 0.5155538790991047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2172] Loss: 0.5155004634466986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2173] Loss: 0.5156724300287032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2174] Loss: 0.5155151829553338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2175] Loss: 0.5156139575683318\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2176] Loss: 0.5154685351957364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2177] Loss: 0.5155411433086613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2178] Loss: 0.5154904408706463\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2179] Loss: 0.5158159891383387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2180] Loss: 0.5156311990079766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2181] Loss: 0.5158232477416114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2182] Loss: 0.5159592378403296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2183] Loss: 0.515775267834159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2184] Loss: 0.5157523022401574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2185] Loss: 0.5156876086477024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2186] Loss: 0.5155790699623649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2187] Loss: 0.5154930218103462\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2188] Loss: 0.515538697127648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2189] Loss: 0.5154941626217054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2190] Loss: 0.5154692550855023\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2191] Loss: 0.5153768652255588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2192] Loss: 0.5152939717140266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2193] Loss: 0.5152185020245617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2194] Loss: 0.5150655469284204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2195] Loss: 0.515309120904798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2196] Loss: 0.5154789302803019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2197] Loss: 0.5155251178610021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2198] Loss: 0.5153951624064084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2199] Loss: 0.5153686860879545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2200] Loss: 0.515171865466431\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2201] Loss: 0.5150754739641086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2202] Loss: 0.5152604270541499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2203] Loss: 0.5151399239607274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2204] Loss: 0.5151563615152236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2205] Loss: 0.5151268940847532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2206] Loss: 0.5150328591413234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2207] Loss: 0.5153250859443342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2208] Loss: 0.5153547120757743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2209] Loss: 0.515271147650575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2210] Loss: 0.5156481306430172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2211] Loss: 0.5155905068078369\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2212] Loss: 0.5155279503568359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2213] Loss: 0.5154937707758341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2214] Loss: 0.5153666954802375\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2215] Loss: 0.5151921524942096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2216] Loss: 0.515094058376334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2217] Loss: 0.514941836728572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2218] Loss: 0.5147769971373349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2219] Loss: 0.5149423998217614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2220] Loss: 0.5147585669979018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2221] Loss: 0.5152161124677842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2222] Loss: 0.5151369487083404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2223] Loss: 0.5150209817354677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2224] Loss: 0.5149468360870834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2225] Loss: 0.5148729389506248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2226] Loss: 0.5147744342858419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2227] Loss: 0.5146611885373232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2228] Loss: 0.514494618398763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2229] Loss: 0.5143184903976596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2230] Loss: 0.5142411393394997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2231] Loss: 0.5143936771265232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2232] Loss: 0.5143528588713254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2233] Loss: 0.5143880323570132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2234] Loss: 0.5145872766778669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2235] Loss: 0.5145490204683248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2236] Loss: 0.514624623709109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2237] Loss: 0.5148466371547713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2238] Loss: 0.5147164596020486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2239] Loss: 0.5146797204841556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2240] Loss: 0.5145451880517723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2241] Loss: 0.5144917633425357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2242] Loss: 0.5146091355755065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2243] Loss: 0.514430940159813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2244] Loss: 0.5146539102049572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2245] Loss: 0.5145832861105372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2246] Loss: 0.5144499860583774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2247] Loss: 0.5143293433221648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2248] Loss: 0.5145160879498247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2249] Loss: 0.5143898742904328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2250] Loss: 0.514226049647249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2251] Loss: 0.514139134965362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2252] Loss: 0.5140026842172329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2253] Loss: 0.5140760166680615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2254] Loss: 0.5141185853767012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2255] Loss: 0.5142414309746215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2256] Loss: 0.5142076419152208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2257] Loss: 0.5140391458902758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2258] Loss: 0.5140359587541161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2259] Loss: 0.5139805578278978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2260] Loss: 0.5138101707356316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2261] Loss: 0.5136806168670589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2262] Loss: 0.5135438418585688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2263] Loss: 0.5137664589858776\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2264] Loss: 0.5136300242987027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2265] Loss: 0.5135238024923536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2266] Loss: 0.5136344100185298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2267] Loss: 0.5135840560878348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2268] Loss: 0.5136471744351284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2269] Loss: 0.5136776618011436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2270] Loss: 0.5136507739736276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2271] Loss: 0.5135514457952834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2272] Loss: 0.5134641237548477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2273] Loss: 0.5133188182463784\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2274] Loss: 0.5134209189859648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2275] Loss: 0.5132559563384178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2276] Loss: 0.513422977259448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2277] Loss: 0.513794682139924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2278] Loss: 0.5136612430889143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2279] Loss: 0.5136616298746699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2280] Loss: 0.5135587459230153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2281] Loss: 0.51336021841683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2282] Loss: 0.5132215252108283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2283] Loss: 0.5131742443765414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2284] Loss: 0.5130696763214156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2285] Loss: 0.5131715644749117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2286] Loss: 0.513110181618784\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2287] Loss: 0.5130427224151727\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2288] Loss: 0.5129428979080627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2289] Loss: 0.512746087046672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2290] Loss: 0.5126102190339163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2291] Loss: 0.5125228517313877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2292] Loss: 0.5127279134979419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2293] Loss: 0.5125725604781795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2294] Loss: 0.5124096570561553\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2295] Loss: 0.5122818745150953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2296] Loss: 0.5122422352683504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2297] Loss: 0.5122199539839387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2298] Loss: 0.5121006560939003\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2299] Loss: 0.5119175141986486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2300] Loss: 0.5119579296538284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2301] Loss: 0.5118760980586644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2302] Loss: 0.5120824675707679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2303] Loss: 0.5118950550209126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2304] Loss: 0.5121866775595624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2305] Loss: 0.5122473199719506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2306] Loss: 0.5124720308788635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2307] Loss: 0.512350265291179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2308] Loss: 0.5121619496846755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2309] Loss: 0.5120449766243346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2310] Loss: 0.5119830672442913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2311] Loss: 0.5119479508250373\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2312] Loss: 0.5119703180682779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2313] Loss: 0.5122118285974395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2314] Loss: 0.5122320423534599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2315] Loss: 0.5120852486413602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2316] Loss: 0.5123219502757687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2317] Loss: 0.5121684966162549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2318] Loss: 0.5120557453483343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2319] Loss: 0.512297937608411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2320] Loss: 0.5121358427224239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2321] Loss: 0.5120904343948385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2322] Loss: 0.5119104042287431\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2323] Loss: 0.5118201272433723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2324] Loss: 0.5116666703923403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2325] Loss: 0.5115343024908019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2326] Loss: 0.5114989250176278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2327] Loss: 0.5118976442710214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2328] Loss: 0.5119801704967507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2329] Loss: 0.5120276254307379\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2330] Loss: 0.5120660038112284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2331] Loss: 0.5122629050891652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2332] Loss: 0.5124901346330716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2333] Loss: 0.5124151097897703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2334] Loss: 0.5123585060957754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2335] Loss: 0.5122321154646677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2336] Loss: 0.5121033800131389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2337] Loss: 0.5122541161136804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2338] Loss: 0.5121962674271878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2339] Loss: 0.5122916644032761\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2340] Loss: 0.5121272131230662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2341] Loss: 0.5120127856536096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2342] Loss: 0.5118140800567303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2343] Loss: 0.5120081969268063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2344] Loss: 0.5119029012718923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2345] Loss: 0.512140515154324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2346] Loss: 0.5120940062264082\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2347] Loss: 0.5119891493344476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2348] Loss: 0.5118390884457138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2349] Loss: 0.511657221979133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2350] Loss: 0.5120593356535487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2351] Loss: 0.5120215234162042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2352] Loss: 0.5120454132395122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2353] Loss: 0.5121308595702109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2354] Loss: 0.5120469045516208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2355] Loss: 0.5121779704320075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2356] Loss: 0.5124605681145824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2357] Loss: 0.5126113053683532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2358] Loss: 0.5126083340189416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2359] Loss: 0.5125630066451542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2360] Loss: 0.5124267116108456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2361] Loss: 0.5124263225987691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2362] Loss: 0.5123837810355695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2363] Loss: 0.5125028875732319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2364] Loss: 0.5124496084586702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2365] Loss: 0.512510735648019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2366] Loss: 0.5125244877718645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2367] Loss: 0.5125936147790476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2368] Loss: 0.5127869070025384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2369] Loss: 0.5129468617033996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2370] Loss: 0.5131561128362533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2371] Loss: 0.5130723430577432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2372] Loss: 0.5130068214348128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2373] Loss: 0.5129215402815589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2374] Loss: 0.5129924404071124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2375] Loss: 0.5130464021066559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2376] Loss: 0.5129872238840448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2377] Loss: 0.5129492907806329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2378] Loss: 0.5128674013839288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2379] Loss: 0.5129313495402797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2380] Loss: 0.5128665121800122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2381] Loss: 0.5129436306428425\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2382] Loss: 0.513161989645316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2383] Loss: 0.5131393527831868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2384] Loss: 0.5129767935265853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2385] Loss: 0.5130858464876811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2386] Loss: 0.5129234570779526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2387] Loss: 0.5127803775768174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2388] Loss: 0.5129373704615873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2389] Loss: 0.5127696394682438\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2390] Loss: 0.5126960793628972\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2391] Loss: 0.5128675986879497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2392] Loss: 0.5128661425077573\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2393] Loss: 0.5127643217875232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2394] Loss: 0.5127273531682046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2395] Loss: 0.5127508280843259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2396] Loss: 0.5126412506210336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2397] Loss: 0.5128402517675154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2398] Loss: 0.5126964304106847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2399] Loss: 0.5126603701131693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2400] Loss: 0.5125721623105978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2401] Loss: 0.5124606899188222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2402] Loss: 0.5123397675726807\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2403] Loss: 0.5124225976586153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2404] Loss: 0.5124409099551037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2405] Loss: 0.5123880529545228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2406] Loss: 0.5124863524241412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2407] Loss: 0.5124704992780574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2408] Loss: 0.512334881305255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2409] Loss: 0.5122095514137912\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2410] Loss: 0.5123731800207966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2411] Loss: 0.512320906621478\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2412] Loss: 0.51218382595058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2413] Loss: 0.5121831523936733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2414] Loss: 0.5121106289937591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2415] Loss: 0.5120423896340873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2416] Loss: 0.5122335300412408\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2417] Loss: 0.5123286343755432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2418] Loss: 0.5122850673269301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2419] Loss: 0.5125642804855324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2420] Loss: 0.5129291602220211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2421] Loss: 0.5131109978268746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2422] Loss: 0.513345650214094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2423] Loss: 0.5131847145619096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2424] Loss: 0.5133032253247568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2425] Loss: 0.5132342962387648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2426] Loss: 0.5130948116205679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2427] Loss: 0.5130415294682786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2428] Loss: 0.5130033443785808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2429] Loss: 0.5132314433742153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2430] Loss: 0.5132086866535246\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2431] Loss: 0.5132976467878728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2432] Loss: 0.5132913042429706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2433] Loss: 0.5135040183397116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2434] Loss: 0.5133506219655957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2435] Loss: 0.5132514346265173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2436] Loss: 0.5131624691335336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2437] Loss: 0.5130817279583071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2438] Loss: 0.5130488795858201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2439] Loss: 0.5132339091545687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2440] Loss: 0.513133216382926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2441] Loss: 0.5133270370194728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2442] Loss: 0.5132154027620951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2443] Loss: 0.5133137650830228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2444] Loss: 0.5134135029801662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2445] Loss: 0.5132352901136537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2446] Loss: 0.5132285733437858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2447] Loss: 0.5131612797111473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2448] Loss: 0.5131537160222006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2449] Loss: 0.5133640285141385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2450] Loss: 0.5131900272495353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2451] Loss: 0.513113549578933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2452] Loss: 0.5130219668318756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2453] Loss: 0.5129507759291673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2454] Loss: 0.5131704897194365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2455] Loss: 0.5132484791900934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2456] Loss: 0.5131747745521145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2457] Loss: 0.5133728038349333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2458] Loss: 0.513308829924348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2459] Loss: 0.5132176351048752\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2460] Loss: 0.5134510951546523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2461] Loss: 0.5134218989626559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2462] Loss: 0.5135799768289215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2463] Loss: 0.5135184842549527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2464] Loss: 0.5136989746071177\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2465] Loss: 0.5135512488043826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2466] Loss: 0.5134520358453621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2467] Loss: 0.5135279764051516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2468] Loss: 0.513661185182456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2469] Loss: 0.5137000557580975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2470] Loss: 0.5136499375941194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2471] Loss: 0.5135069581879942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2472] Loss: 0.5134061243191407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2473] Loss: 0.5132840027538296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2474] Loss: 0.5131445633676168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2475] Loss: 0.5130589226199167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2476] Loss: 0.5131731295952476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2477] Loss: 0.5131815047465913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2478] Loss: 0.5134059879247371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2479] Loss: 0.5132898044213842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2480] Loss: 0.5131358431604913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2481] Loss: 0.5130478844900895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2482] Loss: 0.5131310900601485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2483] Loss: 0.513041854365429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2484] Loss: 0.5129732085654197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2485] Loss: 0.5131321862453146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2486] Loss: 0.5130285167395465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2487] Loss: 0.5130963096141694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2488] Loss: 0.5130042410611021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2489] Loss: 0.5129253989656867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2490] Loss: 0.5129229455960519\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2491] Loss: 0.5127771129396334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2492] Loss: 0.5129490021379396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2493] Loss: 0.5129909485623624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2494] Loss: 0.5131517192949692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2495] Loss: 0.5130957178264781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2496] Loss: 0.512995861373159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2497] Loss: 0.5131602148554946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2498] Loss: 0.5130184842292331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2499] Loss: 0.5129222434375561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2500] Loss: 0.5130361409268187\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2501] Loss: 0.5129564985490696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2502] Loss: 0.5129574342110909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2503] Loss: 0.5128409812797928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2504] Loss: 0.5128877610939675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2505] Loss: 0.512800297843185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2506] Loss: 0.5128096124317818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2507] Loss: 0.5127197100436264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2508] Loss: 0.5127716523793725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2509] Loss: 0.5127175919126427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2510] Loss: 0.5126283210888505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2511] Loss: 0.5126055217292653\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8525\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2512] Loss: 0.5124931856840997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2513] Loss: 0.5123773525267675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2514] Loss: 0.5123688479338935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2515] Loss: 0.5123012464911563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2516] Loss: 0.5121540841112584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2517] Loss: 0.5121204850088024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2518] Loss: 0.5123690435017247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2519] Loss: 0.5124314171037252\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2520] Loss: 0.512545836357335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2521] Loss: 0.5125511417277472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2522] Loss: 0.5124655609457914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2523] Loss: 0.512513938793423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2524] Loss: 0.5123539869604187\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2525] Loss: 0.5123197259737303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2526] Loss: 0.512198635790911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2527] Loss: 0.5122966037774122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2528] Loss: 0.5124698242152292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2529] Loss: 0.5124171707020826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2530] Loss: 0.5124572511504192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2531] Loss: 0.5124070486150597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2532] Loss: 0.512256164461168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2533] Loss: 0.5120920888191983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2534] Loss: 0.512053210626949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2535] Loss: 0.5119379566775428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2536] Loss: 0.5118688916289206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2537] Loss: 0.5117482557106207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2538] Loss: 0.5118603536774656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2539] Loss: 0.5117032296144063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2540] Loss: 0.5118226267727725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2541] Loss: 0.5118259454172623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2542] Loss: 0.5116968240266241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2543] Loss: 0.5117296948845728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2544] Loss: 0.5119229170366492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2545] Loss: 0.5120216525450385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2546] Loss: 0.5119691309619746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2547] Loss: 0.5118487899538695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2548] Loss: 0.5120445440456608\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2549] Loss: 0.5119244686787826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2550] Loss: 0.5118945159894579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2551] Loss: 0.5118896502276836\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2552] Loss: 0.5121151374818996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2553] Loss: 0.5121664585316817\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2554] Loss: 0.5123486773579321\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2555] Loss: 0.512336146029983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2556] Loss: 0.5124125377377923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2557] Loss: 0.5128075227279691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2558] Loss: 0.5126969573029783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2559] Loss: 0.5128279450522684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2560] Loss: 0.5127015753926301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2561] Loss: 0.5127961067006043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2562] Loss: 0.5127470335736145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2563] Loss: 0.5127734357388612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2564] Loss: 0.5127837496438765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2565] Loss: 0.5127094441811824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2566] Loss: 0.5127869840522916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2567] Loss: 0.5129142029081436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2568] Loss: 0.5128537727948875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2569] Loss: 0.5128616288977378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2570] Loss: 0.512803512739325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2571] Loss: 0.5126697071572637\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2572] Loss: 0.5125603975310821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2573] Loss: 0.5125802628807922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2574] Loss: 0.5125647475581991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2575] Loss: 0.5124003006600871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2576] Loss: 0.5123221140646981\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2577] Loss: 0.5122125527465984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2578] Loss: 0.5120550263997319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2579] Loss: 0.5120533825392998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2580] Loss: 0.5119980429033726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2581] Loss: 0.5121037183479662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2582] Loss: 0.5119648393833269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2583] Loss: 0.5120890052981039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2584] Loss: 0.5121764880510742\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2585] Loss: 0.5120428519363863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2586] Loss: 0.5120701776361649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2587] Loss: 0.5121489225756278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2588] Loss: 0.5121172207180873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2589] Loss: 0.5121246636085868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2590] Loss: 0.5121848970221785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2591] Loss: 0.5120659897328569\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2592] Loss: 0.5122734873656695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2593] Loss: 0.5121100317584856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2594] Loss: 0.5120100140886206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2595] Loss: 0.512104381774541\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2596] Loss: 0.5120984632039687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2597] Loss: 0.5119790676597437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2598] Loss: 0.5120571990037102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2599] Loss: 0.5119581608437875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2600] Loss: 0.5119648662908226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2601] Loss: 0.5119192199037726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2602] Loss: 0.5119760238332228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2603] Loss: 0.5121807624749231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2604] Loss: 0.5120788176028981\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2605] Loss: 0.5120381068642782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2606] Loss: 0.5118947808580999\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2607] Loss: 0.511900051269977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2608] Loss: 0.5120909016852838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2609] Loss: 0.5121731642827129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2610] Loss: 0.5120581024601346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2611] Loss: 0.5123518642387408\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2612] Loss: 0.5125591053381974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2613] Loss: 0.5127466835411061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2614] Loss: 0.5127399160664797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2615] Loss: 0.5126923685402315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2616] Loss: 0.5125831940688073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2617] Loss: 0.5125099233840957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2618] Loss: 0.5123926857221285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2619] Loss: 0.5124144232798776\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2620] Loss: 0.5124355215031954\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2621] Loss: 0.512262147264083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2622] Loss: 0.5121744019826028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2623] Loss: 0.5120464063514424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2624] Loss: 0.511994385603699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2625] Loss: 0.5119587343071651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2626] Loss: 0.5119077869900458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2627] Loss: 0.5119050145853112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2628] Loss: 0.5117843951168096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2629] Loss: 0.5117237725984718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2630] Loss: 0.5116294097787929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2631] Loss: 0.5117518670825337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2632] Loss: 0.5119307475703488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2633] Loss: 0.5119093832153225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2634] Loss: 0.5118702046587194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2635] Loss: 0.5121873069510741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2636] Loss: 0.5120814142088114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2637] Loss: 0.5120233109734907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2638] Loss: 0.5121606459939166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2639] Loss: 0.5123163432148825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2640] Loss: 0.5122623345801528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2641] Loss: 0.5121422724257272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2642] Loss: 0.5120156320689095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2643] Loss: 0.5119079051865024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2644] Loss: 0.5117936443394328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2645] Loss: 0.51193519460792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2646] Loss: 0.5121550305087245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2647] Loss: 0.5121003077249969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2648] Loss: 0.5119856488102502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2649] Loss: 0.5119961554505882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2650] Loss: 0.511893498786142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2651] Loss: 0.5117866839832482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2652] Loss: 0.511916975180308\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2653] Loss: 0.5118522596342565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2654] Loss: 0.5117836586797415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2655] Loss: 0.5117492841673897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2656] Loss: 0.5119798227047543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2657] Loss: 0.5120252234801393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2658] Loss: 0.5118903006518908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2659] Loss: 0.5118674966708179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2660] Loss: 0.5119367557833361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2661] Loss: 0.5121251847274577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2662] Loss: 0.5120415428106337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2663] Loss: 0.5121140522467941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2664] Loss: 0.5119639653003426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2665] Loss: 0.5118163305617817\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2666] Loss: 0.511771589227311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2667] Loss: 0.5116862305265143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2668] Loss: 0.5118563657590258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2669] Loss: 0.5118593947238091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2670] Loss: 0.511948200384224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2671] Loss: 0.5119064521425468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2672] Loss: 0.5118215505884931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2673] Loss: 0.5118509634633446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2674] Loss: 0.5119759926293562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2675] Loss: 0.5120556536227396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2676] Loss: 0.5121268142128255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2677] Loss: 0.5120334453059057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2678] Loss: 0.5121299400650707\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2679] Loss: 0.5120414338555959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2680] Loss: 0.5119483170284104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2681] Loss: 0.511962302664028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2682] Loss: 0.5118526961812218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2683] Loss: 0.5119745178371874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2684] Loss: 0.5118528860856769\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2685] Loss: 0.511724862600195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2686] Loss: 0.511648044195574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2687] Loss: 0.5115179763127206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2688] Loss: 0.5115534157737666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2689] Loss: 0.5115698161558473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2690] Loss: 0.511430549689936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2691] Loss: 0.5118069602419947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2692] Loss: 0.5118418835123999\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2693] Loss: 0.5117186492345664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2694] Loss: 0.511628813632242\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2695] Loss: 0.5118566140430197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2696] Loss: 0.5118970742818967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2697] Loss: 0.5119208432682911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2698] Loss: 0.5118359263298499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2699] Loss: 0.5119096517780699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2700] Loss: 0.5121055398085346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2701] Loss: 0.5121702401945983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2702] Loss: 0.5121123132425068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2703] Loss: 0.5120884086843283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2704] Loss: 0.5119494762655378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2705] Loss: 0.5118560069650072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2706] Loss: 0.5117819626664638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2707] Loss: 0.5116601356143674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2708] Loss: 0.5116253977775357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2709] Loss: 0.5114750229753977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2710] Loss: 0.5113428959656845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2711] Loss: 0.511422831556679\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2712] Loss: 0.5113644009146443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2713] Loss: 0.5114086424687533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2714] Loss: 0.5112874557394298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2715] Loss: 0.511227712825853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2716] Loss: 0.511120882719512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2717] Loss: 0.5111010708876956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2718] Loss: 0.5111229582352267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2719] Loss: 0.5109830831593656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2720] Loss: 0.5108697779510356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2721] Loss: 0.5108232839392731\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2722] Loss: 0.5108492288125467\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2723] Loss: 0.5107847728976501\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2724] Loss: 0.5107518388959558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2725] Loss: 0.5106400831356543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2726] Loss: 0.5104914482763635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2727] Loss: 0.510400981295738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2728] Loss: 0.5104298070879322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2729] Loss: 0.5103414472681062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2730] Loss: 0.5103534986642567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2731] Loss: 0.5102291371319435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2732] Loss: 0.5103342043358287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2733] Loss: 0.5104157267539715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2734] Loss: 0.5104345253755828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2735] Loss: 0.5103169581528461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2736] Loss: 0.5104884546440888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2737] Loss: 0.5104599248402099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2738] Loss: 0.5103068839778699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2739] Loss: 0.5102757133740703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2740] Loss: 0.5102763936725433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2741] Loss: 0.51018055725103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2742] Loss: 0.5100249096243826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2743] Loss: 0.5100403885189002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2744] Loss: 0.5099378834506072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2745] Loss: 0.5098181516398786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2746] Loss: 0.5097329083810126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2747] Loss: 0.5095876620771845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2748] Loss: 0.5095387930524893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2749] Loss: 0.5095788907557526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2750] Loss: 0.5095855450523751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2751] Loss: 0.509475280136362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2752] Loss: 0.5093697104254118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2753] Loss: 0.5097010676689675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2754] Loss: 0.5097141755934067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2755] Loss: 0.5099204460868326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2756] Loss: 0.5100969068460558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2757] Loss: 0.5101062201318287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2758] Loss: 0.5100996835269962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2759] Loss: 0.5100658116949139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2760] Loss: 0.5099559456242455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2761] Loss: 0.5100261133440543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2762] Loss: 0.5098845236810655\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2763] Loss: 0.509845379017429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2764] Loss: 0.5097783148526722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2765] Loss: 0.5097520205371925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2766] Loss: 0.5101620421632596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2767] Loss: 0.510210112192803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2768] Loss: 0.510374605220118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2769] Loss: 0.510395848326263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2770] Loss: 0.5104708176402919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2771] Loss: 0.5107949689155867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2772] Loss: 0.5106579897337314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2773] Loss: 0.5105256423242209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2774] Loss: 0.5104560280488574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2775] Loss: 0.5106726313005746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2776] Loss: 0.510552600749488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2777] Loss: 0.5105718567237804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2778] Loss: 0.5104379041261669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2779] Loss: 0.5103201373932171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2780] Loss: 0.5102762679499676\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2781] Loss: 0.5103350502509789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2782] Loss: 0.5103725531322121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2783] Loss: 0.5102730095452063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2784] Loss: 0.5101759363958578\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2785] Loss: 0.5103119928692723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2786] Loss: 0.5104326193003222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2787] Loss: 0.5103558845553002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2788] Loss: 0.5103214645890733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2789] Loss: 0.5103408181193944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2790] Loss: 0.5104449741999831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2791] Loss: 0.5105329576956499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2792] Loss: 0.510460747067966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2793] Loss: 0.5105597029390745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2794] Loss: 0.5104726619705323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2795] Loss: 0.5104193860840224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2796] Loss: 0.5103797747746227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2797] Loss: 0.5103054978165191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2798] Loss: 0.5103738062617766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2799] Loss: 0.5104871431114164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2800] Loss: 0.5104907856302491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2801] Loss: 0.5108748993424546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2802] Loss: 0.5107326435793133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2803] Loss: 0.5106466301159208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2804] Loss: 0.5106348469528615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2805] Loss: 0.5105811498913111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2806] Loss: 0.5105337845583411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2807] Loss: 0.5105574829297114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2808] Loss: 0.5107110075010644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2809] Loss: 0.5106646069343737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2810] Loss: 0.5107346901427144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2811] Loss: 0.5106976392060868\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2812] Loss: 0.5105955113181853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2813] Loss: 0.5104763859117026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2814] Loss: 0.5104336021403368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2815] Loss: 0.510859477746254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2816] Loss: 0.5109327303605915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2817] Loss: 0.5111243820094835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2818] Loss: 0.5109826835062826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2819] Loss: 0.5109365254046856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2820] Loss: 0.5108652835090955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2821] Loss: 0.5111238725164245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2822] Loss: 0.5110029579175694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2823] Loss: 0.5111745578344384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2824] Loss: 0.5112962395841435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2825] Loss: 0.5112483862859899\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2826] Loss: 0.5111463767040476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2827] Loss: 0.5110661781485039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2828] Loss: 0.5110334459759739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2829] Loss: 0.5109419865552184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2830] Loss: 0.5108475207447492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2831] Loss: 0.5107421962121395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2832] Loss: 0.5108434601873801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2833] Loss: 0.5107088577334243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2834] Loss: 0.5107091760484056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2835] Loss: 0.5105951144362009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2836] Loss: 0.5105309952386781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2837] Loss: 0.5103813555166862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2838] Loss: 0.5103349244906935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2839] Loss: 0.5102983924159168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2840] Loss: 0.51021687717141\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2841] Loss: 0.5101012155471161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2842] Loss: 0.5103301449677241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2843] Loss: 0.5102432829100739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2844] Loss: 0.5101189091929331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2845] Loss: 0.5100373582679149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2846] Loss: 0.5099648230275369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2847] Loss: 0.509932415495329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2848] Loss: 0.5098126962322063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2849] Loss: 0.5097545975864621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2850] Loss: 0.5096764874827658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2851] Loss: 0.5099135131687753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2852] Loss: 0.5098804464320359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2853] Loss: 0.50990585540153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2854] Loss: 0.5100995262340983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2855] Loss: 0.5100129385047885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2856] Loss: 0.5099330689862865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2857] Loss: 0.5097871796534729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2858] Loss: 0.5097671610825208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2859] Loss: 0.5097281682575038\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2860] Loss: 0.5096847205149366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2861] Loss: 0.5097729744922856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2862] Loss: 0.5099705187983963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2863] Loss: 0.5098757546440976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2864] Loss: 0.5099129421270896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2865] Loss: 0.5100757117420751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2866] Loss: 0.5099662779458816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2867] Loss: 0.5100260000449902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2868] Loss: 0.5101367215277585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2869] Loss: 0.5103318768886248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2870] Loss: 0.5103175749276149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2871] Loss: 0.5102804353344698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2872] Loss: 0.5103430371419239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2873] Loss: 0.5102284660581002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2874] Loss: 0.5101235885923183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2875] Loss: 0.510051862476492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2876] Loss: 0.510114488535933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2877] Loss: 0.5100432230127376\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2878] Loss: 0.5103847029228771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2879] Loss: 0.5103849573080527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2880] Loss: 0.5102670321484658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2881] Loss: 0.5101662064116497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2882] Loss: 0.5101775347960337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2883] Loss: 0.5102527867853516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2884] Loss: 0.5102574882405891\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2885] Loss: 0.5102460386000182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2886] Loss: 0.5101935364030026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2887] Loss: 0.5101748772488957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2888] Loss: 0.5102840644331716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2889] Loss: 0.5102193538708665\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2890] Loss: 0.5101030997806737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2891] Loss: 0.5100245533283395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2892] Loss: 0.50991618294835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2893] Loss: 0.5098946346575804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2894] Loss: 0.5099190239475777\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2895] Loss: 0.5100907137246502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2896] Loss: 0.5101818612652572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2897] Loss: 0.5100597451247079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2898] Loss: 0.5102390323389615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2899] Loss: 0.5101628153143991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2900] Loss: 0.5100738029537341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2901] Loss: 0.5099802852262767\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2902] Loss: 0.5099582679781527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2903] Loss: 0.5098849128179853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2904] Loss: 0.5100703068332764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2905] Loss: 0.5100038025162663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2906] Loss: 0.5098990483492464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2907] Loss: 0.5099648667851437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2908] Loss: 0.5099835091016509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2909] Loss: 0.5099262074350367\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2910] Loss: 0.5098318614127735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2911] Loss: 0.5100177536217783\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2912] Loss: 0.5099875963510919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2913] Loss: 0.5098950975185824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2914] Loss: 0.5098249867570877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2915] Loss: 0.5097036759967368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2916] Loss: 0.5096056171486205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2917] Loss: 0.5096427355260338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2918] Loss: 0.5097629691774267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2919] Loss: 0.5099722760106677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2920] Loss: 0.5099001142988561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2921] Loss: 0.5099169477937826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2922] Loss: 0.5098165234127646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2923] Loss: 0.5097825676882064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2924] Loss: 0.5099198883885476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2925] Loss: 0.5097886615473292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2926] Loss: 0.509652621519832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2927] Loss: 0.5099740288488915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2928] Loss: 0.5100268709686692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2929] Loss: 0.5098950637919492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2930] Loss: 0.5098107103966485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2931] Loss: 0.5097419885454568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2932] Loss: 0.5098611729928623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2933] Loss: 0.5098422558480828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2934] Loss: 0.510022261392067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2935] Loss: 0.5101782455518074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2936] Loss: 0.5102111278907976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2937] Loss: 0.5103663953798614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2938] Loss: 0.5104096597290707\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2939] Loss: 0.5103022054960424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2940] Loss: 0.5102969567226284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2941] Loss: 0.5104870914383123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2942] Loss: 0.5104575507636917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2943] Loss: 0.5105344288348171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2944] Loss: 0.5107279377289258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2945] Loss: 0.5107074017397432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2946] Loss: 0.5106035777794317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2947] Loss: 0.5104784368796088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2948] Loss: 0.5103740340146107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2949] Loss: 0.5102886553730032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2950] Loss: 0.510198524429417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2951] Loss: 0.5100733526829372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2952] Loss: 0.5100884753524232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2953] Loss: 0.510175832228928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2954] Loss: 0.5100645137106338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2955] Loss: 0.5102002803045792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2956] Loss: 0.5100726285798571\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2957] Loss: 0.5102075900673915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2958] Loss: 0.510079192618529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2959] Loss: 0.5099912394840603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2960] Loss: 0.5100449520957713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2961] Loss: 0.5099491860880165\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2962] Loss: 0.5099396654336332\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2963] Loss: 0.5098513214140098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2964] Loss: 0.509825443896027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2965] Loss: 0.5096904074587307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2966] Loss: 0.5097170890299816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2967] Loss: 0.5099333882525922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2968] Loss: 0.5101965021029831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2969] Loss: 0.5103249643672138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2970] Loss: 0.5103729644684287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2971] Loss: 0.5103169959257987\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2972] Loss: 0.5103075820846735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2973] Loss: 0.5104286216571473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2974] Loss: 0.5103677116794052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2975] Loss: 0.510274929799608\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2976] Loss: 0.5104435794327381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2977] Loss: 0.5104180446364979\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2978] Loss: 0.5103695755356521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2979] Loss: 0.5104195286794054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2980] Loss: 0.510332292535527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2981] Loss: 0.5102860295352836\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2982] Loss: 0.5105736263120444\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2983] Loss: 0.5104903382205115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2984] Loss: 0.5103519334847058\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2985] Loss: 0.5103361660543114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2986] Loss: 0.5102447040968835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2987] Loss: 0.5101890638681204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2988] Loss: 0.5101372671642065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2989] Loss: 0.5101782283435766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2990] Loss: 0.5100377241029374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2991] Loss: 0.5099971022626557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2992] Loss: 0.5099358700762152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2993] Loss: 0.5099292615885413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2994] Loss: 0.5098434091514148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2995] Loss: 0.5097748623827334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2996] Loss: 0.5096484942546029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2997] Loss: 0.5096296393456687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2998] Loss: 0.5098042311116909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 2999] Loss: 0.5097561589071385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3000] Loss: 0.5096398889958619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3001] Loss: 0.5097806547442769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3002] Loss: 0.5098063665147673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3003] Loss: 0.5099967719260058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3004] Loss: 0.5100059447318627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3005] Loss: 0.5098950686220655\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3006] Loss: 0.5098259568763658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3007] Loss: 0.5097900186961872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3008] Loss: 0.5097461727092131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3009] Loss: 0.5096696003550002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3010] Loss: 0.5096390772223472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3011] Loss: 0.5097948525224577\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8109\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3012] Loss: 0.5098659179979663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3013] Loss: 0.5097722669648876\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3014] Loss: 0.5099077573575722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3015] Loss: 0.5099312938972862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3016] Loss: 0.5099183617238511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3017] Loss: 0.5098375023528785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3018] Loss: 0.5098131736547373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3019] Loss: 0.5096980424255619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3020] Loss: 0.5098138474076392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3021] Loss: 0.5097772379894268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3022] Loss: 0.5097136593892411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3023] Loss: 0.5096760202025779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3024] Loss: 0.5098066518028388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3025] Loss: 0.5097231517551434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3026] Loss: 0.509907108854461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3027] Loss: 0.5099879302022283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3028] Loss: 0.5100998387119428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3029] Loss: 0.5101243979273546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3030] Loss: 0.5102988265395637\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3031] Loss: 0.5102195510347416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3032] Loss: 0.5101567892199655\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3033] Loss: 0.5101378473187079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3034] Loss: 0.5101013909582575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3035] Loss: 0.510115305604321\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3036] Loss: 0.5099869021056006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3037] Loss: 0.5101107531885031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3038] Loss: 0.510020383604201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3039] Loss: 0.5099427634109948\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3040] Loss: 0.5101086805283788\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3041] Loss: 0.5103142956990774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3042] Loss: 0.5104727883297968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3043] Loss: 0.5106096796270797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3044] Loss: 0.5106379305848289\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3045] Loss: 0.5108220888371533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3046] Loss: 0.5107232836554009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3047] Loss: 0.510660199860348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3048] Loss: 0.5105838047544538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3049] Loss: 0.5104705760825474\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3050] Loss: 0.5105729595882686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3051] Loss: 0.5104819744274119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3052] Loss: 0.5103358076628881\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3053] Loss: 0.5105273884743441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3054] Loss: 0.5105908661729323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3055] Loss: 0.5105806780001972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3056] Loss: 0.5105360682003159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3057] Loss: 0.5105108696664601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3058] Loss: 0.510377841190182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3059] Loss: 0.5104980953432336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3060] Loss: 0.5104549350282725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3061] Loss: 0.5106191862300068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3062] Loss: 0.5105464286506644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3063] Loss: 0.5105159603178945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3064] Loss: 0.510465227324186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3065] Loss: 0.5105400570393075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3066] Loss: 0.5106949117063823\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3067] Loss: 0.5105812989050026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3068] Loss: 0.5105451939334806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3069] Loss: 0.5105010949049018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3070] Loss: 0.5104211216850671\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3071] Loss: 0.5106458584867106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3072] Loss: 0.5106675026997936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3073] Loss: 0.5108223200553662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3074] Loss: 0.5107494701984427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3075] Loss: 0.5107239839916805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3076] Loss: 0.5106165013040125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3077] Loss: 0.5107620562567152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3078] Loss: 0.5106292695062369\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3079] Loss: 0.5107658487458598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3080] Loss: 0.5106522453434272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3081] Loss: 0.5106473897668184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3082] Loss: 0.5107017587609647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3083] Loss: 0.5107765650522186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3084] Loss: 0.5106428298974741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3085] Loss: 0.5105593473876564\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3086] Loss: 0.510418573226664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3087] Loss: 0.510377386471399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3088] Loss: 0.5105231175238167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3089] Loss: 0.510546187788953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3090] Loss: 0.5106605506740337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3091] Loss: 0.5105937911541782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3092] Loss: 0.5107016840489403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3093] Loss: 0.5106394389515185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3094] Loss: 0.5106484608857348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3095] Loss: 0.510610777387536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3096] Loss: 0.5105343894875981\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3097] Loss: 0.5104694598065318\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3098] Loss: 0.5103921736813679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3099] Loss: 0.5104149344426502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3100] Loss: 0.510483683507645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3101] Loss: 0.5106130758971148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3102] Loss: 0.5105680135124719\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3103] Loss: 0.5105444413156892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3104] Loss: 0.5104524652971243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3105] Loss: 0.5103291627873805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3106] Loss: 0.5103648990415645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3107] Loss: 0.510234021697542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3108] Loss: 0.5101175828563699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3109] Loss: 0.510178903471098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3110] Loss: 0.5102922601883229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3111] Loss: 0.5103130431323728\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3112] Loss: 0.5102297649486169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3113] Loss: 0.5101825569370275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3114] Loss: 0.5100551215265112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3115] Loss: 0.5100571209947344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3116] Loss: 0.5099301656746718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3117] Loss: 0.5102360071775143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3118] Loss: 0.5104073533514069\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3119] Loss: 0.5103314923812896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3120] Loss: 0.5103147784925969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3121] Loss: 0.5103170966757343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3122] Loss: 0.5102101411174121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3123] Loss: 0.5101346218330487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3124] Loss: 0.510081200617456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3125] Loss: 0.5099823064489529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3126] Loss: 0.5101387664926343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3127] Loss: 0.5101229729978556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3128] Loss: 0.5102988874484238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3129] Loss: 0.5102785867149933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3130] Loss: 0.5105486043536936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3131] Loss: 0.5104815466901538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3132] Loss: 0.5105136970497105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3133] Loss: 0.510404208599044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3134] Loss: 0.5103560117323224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3135] Loss: 0.5102705154873076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3136] Loss: 0.5104482177551339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3137] Loss: 0.5103200357315156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3138] Loss: 0.5102228322711346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3139] Loss: 0.5102103526780886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3140] Loss: 0.5101422164829965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3141] Loss: 0.5100433244899125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3142] Loss: 0.5101202203691186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3143] Loss: 0.5103696326633573\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3144] Loss: 0.5103773699172447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3145] Loss: 0.5102870078647838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3146] Loss: 0.5103030061188164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3147] Loss: 0.510373578204501\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3148] Loss: 0.5104305806306386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3149] Loss: 0.5103827806619498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3150] Loss: 0.5104716538931384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3151] Loss: 0.5106873380013913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3152] Loss: 0.5106101989475369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3153] Loss: 0.5105379299936176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3154] Loss: 0.5105402878684283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3155] Loss: 0.5104998685799835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3156] Loss: 0.5106322082239664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3157] Loss: 0.510557157940625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3158] Loss: 0.5105429455255274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3159] Loss: 0.510644362338672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3160] Loss: 0.5108334167386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3161] Loss: 0.5107610154516155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3162] Loss: 0.5107966539973946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3163] Loss: 0.5107566620487561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3164] Loss: 0.5107753855690709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3165] Loss: 0.5106558968904104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3166] Loss: 0.5106205006792064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3167] Loss: 0.5105520055556055\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3168] Loss: 0.5104228278371156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3169] Loss: 0.5104972850440274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3170] Loss: 0.5105779729895574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3171] Loss: 0.5105664548693363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3172] Loss: 0.5105594344700216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3173] Loss: 0.5105607956200222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3174] Loss: 0.5107415872304378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3175] Loss: 0.5107496242194417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3176] Loss: 0.5108831697727001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3177] Loss: 0.5108405294704849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3178] Loss: 0.5107430327052566\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3179] Loss: 0.5107595481232517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3180] Loss: 0.5106883205891994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3181] Loss: 0.510800452451722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3182] Loss: 0.510803051555005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3183] Loss: 0.5109489167389957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3184] Loss: 0.5111362701344597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3185] Loss: 0.5111973204958105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3186] Loss: 0.5111025577001774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3187] Loss: 0.5110447226028902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3188] Loss: 0.5110323086737073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3189] Loss: 0.5112258169122844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3190] Loss: 0.5111418381944965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3191] Loss: 0.5113514741769436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3192] Loss: 0.5112596953553019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3193] Loss: 0.5113267421755736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3194] Loss: 0.5114663987408008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3195] Loss: 0.5116306114851652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3196] Loss: 0.5115408836985291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3197] Loss: 0.5114544940218856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3198] Loss: 0.5116215112807584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3199] Loss: 0.5115830333437251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3200] Loss: 0.5115411578667208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3201] Loss: 0.5115192683987438\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3202] Loss: 0.5115702949508604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3203] Loss: 0.5117024237001602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3204] Loss: 0.5118985220059785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3205] Loss: 0.511805009460405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3206] Loss: 0.5117373614563698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3207] Loss: 0.5117673338133625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3208] Loss: 0.5117070490923786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3209] Loss: 0.5116128748094386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3210] Loss: 0.5116975615366741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3211] Loss: 0.5116100992010241\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3212] Loss: 0.5117169385824178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3213] Loss: 0.511628468495098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3214] Loss: 0.5115671005684155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3215] Loss: 0.5115488034574911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3216] Loss: 0.5116481786567638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3217] Loss: 0.5115880523225149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3218] Loss: 0.5115250126462831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3219] Loss: 0.5116693065885074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3220] Loss: 0.5115747786261059\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3221] Loss: 0.5114668198548221\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3222] Loss: 0.5114611596924014\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3223] Loss: 0.5114988408930575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3224] Loss: 0.5115343253227745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3225] Loss: 0.5116914221814544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3226] Loss: 0.5117079322617254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3227] Loss: 0.5116129972652166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3228] Loss: 0.5115249627232114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3229] Loss: 0.5116288371386885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3230] Loss: 0.5116323687026606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3231] Loss: 0.5115727227056609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3232] Loss: 0.511549790014159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3233] Loss: 0.511479647193286\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3234] Loss: 0.5114398532480046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3235] Loss: 0.511360524652201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3236] Loss: 0.5114098144797656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3237] Loss: 0.511479181982372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3238] Loss: 0.5113765793057743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3239] Loss: 0.5115121653629074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3240] Loss: 0.5114086643516362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3241] Loss: 0.5112985947364768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3242] Loss: 0.5112552096028192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3243] Loss: 0.5113132612318335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3244] Loss: 0.5113538403306551\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3245] Loss: 0.5112679635639382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3246] Loss: 0.5112024499756986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3247] Loss: 0.511137456850584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3248] Loss: 0.5111193459091873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3249] Loss: 0.5111876203929825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3250] Loss: 0.5113176033499032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3251] Loss: 0.5113648275407463\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3252] Loss: 0.5114490024468309\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3253] Loss: 0.5113270780301815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3254] Loss: 0.5114689765568639\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3255] Loss: 0.5116644388068134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3256] Loss: 0.5115533234240882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3257] Loss: 0.511512259412645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3258] Loss: 0.5114698915578305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3259] Loss: 0.5115686949652817\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3260] Loss: 0.5116313450499015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3261] Loss: 0.5115746445100726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3262] Loss: 0.5114857944020959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3263] Loss: 0.5115859926320664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3264] Loss: 0.5117041189588752\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3265] Loss: 0.5116109237101896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3266] Loss: 0.5116079231488947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3267] Loss: 0.5117408644095858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3268] Loss: 0.5116588160979134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3269] Loss: 0.511564835657586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3270] Loss: 0.5116162072219278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3271] Loss: 0.5115591665565946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3272] Loss: 0.5115085354831493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3273] Loss: 0.5114979129196983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3274] Loss: 0.5115977694831692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3275] Loss: 0.5115628088846345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3276] Loss: 0.5116760158454777\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3277] Loss: 0.5117367875082725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3278] Loss: 0.5116839187241272\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3279] Loss: 0.5118527212602303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3280] Loss: 0.5119696403614881\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3281] Loss: 0.5119363447521534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3282] Loss: 0.5121954203694844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3283] Loss: 0.5123431151077887\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3284] Loss: 0.5123132118161582\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3285] Loss: 0.5122454910503852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3286] Loss: 0.5121768909299838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3287] Loss: 0.5122875350443288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3288] Loss: 0.5122126881180692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3289] Loss: 0.5123294330932289\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3290] Loss: 0.5123389861620159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3291] Loss: 0.5122488768130316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3292] Loss: 0.5124705053067824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3293] Loss: 0.5124029396111872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3294] Loss: 0.5124917019576091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3295] Loss: 0.5126782733137775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3296] Loss: 0.5125712413937175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3297] Loss: 0.5125016483100974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3298] Loss: 0.5124490091809669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3299] Loss: 0.5124392797172005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3300] Loss: 0.5124036854793949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3301] Loss: 0.5123567586569152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3302] Loss: 0.5123243081225963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3303] Loss: 0.5123668429706841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3304] Loss: 0.5123733184330362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3305] Loss: 0.5122888744357234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3306] Loss: 0.5124791804034908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3307] Loss: 0.5126245501885722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3308] Loss: 0.5127265820115954\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3309] Loss: 0.5126899176035322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3310] Loss: 0.5126929684781603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3311] Loss: 0.5126360324409254\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3312] Loss: 0.5126552557802728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3313] Loss: 0.512622672730882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3314] Loss: 0.512568643558425\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3315] Loss: 0.512509988468811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3316] Loss: 0.5124216690999392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3317] Loss: 0.5124169134259181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3318] Loss: 0.5124106895539411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3319] Loss: 0.5124236843039787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3320] Loss: 0.5123453900642242\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3321] Loss: 0.5126119543069709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3322] Loss: 0.5125541817635341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3323] Loss: 0.5125476360861364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3324] Loss: 0.5124913171653893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3325] Loss: 0.5126393755788702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3326] Loss: 0.5125287057969465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3327] Loss: 0.5126665840398178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3328] Loss: 0.512656737555207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3329] Loss: 0.512827158825285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3330] Loss: 0.5127450761378657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3331] Loss: 0.5128092530590556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3332] Loss: 0.5129014096842599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3333] Loss: 0.5128704892683232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3334] Loss: 0.5128048270170394\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3335] Loss: 0.5127215213216512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3336] Loss: 0.5126825968160835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3337] Loss: 0.5126090255049711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3338] Loss: 0.5125575787223128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3339] Loss: 0.5126037773741728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3340] Loss: 0.5127063796029495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3341] Loss: 0.5126300855276231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3342] Loss: 0.5125451452745977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3343] Loss: 0.5124829821196538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3344] Loss: 0.5124123314199687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3345] Loss: 0.5125402745378501\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3346] Loss: 0.5127124706984659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3347] Loss: 0.5128131647333949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3348] Loss: 0.5129435244761276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3349] Loss: 0.5130934183101664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3350] Loss: 0.5130456885768914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3351] Loss: 0.5129724056976273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3352] Loss: 0.5128693364869953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3353] Loss: 0.5128890592794875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3354] Loss: 0.5129882775289013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3355] Loss: 0.5129664979050155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3356] Loss: 0.5129587946280324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3357] Loss: 0.5129666111376732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3358] Loss: 0.5129623525106254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3359] Loss: 0.5130195928617878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3360] Loss: 0.5129502948595767\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3361] Loss: 0.5128436134922342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3362] Loss: 0.5127493485777024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3363] Loss: 0.512805583531966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3364] Loss: 0.5127840523869739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3365] Loss: 0.5127394407778821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3366] Loss: 0.5127261215951346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3367] Loss: 0.5127199409691905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3368] Loss: 0.5127034684769585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3369] Loss: 0.5126292036990262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3370] Loss: 0.5125396841837393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3371] Loss: 0.5126174749329306\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3372] Loss: 0.5125757074862156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3373] Loss: 0.5125294431882682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3374] Loss: 0.5124208653949243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3375] Loss: 0.5123493097475478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3376] Loss: 0.5125030814530296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3377] Loss: 0.5125175455502612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3378] Loss: 0.5125558011420851\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3379] Loss: 0.5125607785269872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3380] Loss: 0.51243908141029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3381] Loss: 0.512489428314877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3382] Loss: 0.5124714841241209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3383] Loss: 0.5124268767220028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3384] Loss: 0.5124077717452806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3385] Loss: 0.5124349321437919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3386] Loss: 0.5124375375145749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3387] Loss: 0.5123283709757105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3388] Loss: 0.5123901199640045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3389] Loss: 0.5125024639388347\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3390] Loss: 0.5126292929674189\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3391] Loss: 0.5126935906684965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3392] Loss: 0.5128650142243139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3393] Loss: 0.5128646667037537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3394] Loss: 0.5130615652062195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3395] Loss: 0.5131771557252577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3396] Loss: 0.5131425935199339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3397] Loss: 0.5131408960339434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3398] Loss: 0.5131600365620571\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3399] Loss: 0.5132517193006619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3400] Loss: 0.5132419921947599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3401] Loss: 0.5131246632500447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3402] Loss: 0.5129930436260802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3403] Loss: 0.5130049014870287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3404] Loss: 0.5129301600214854\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3405] Loss: 0.5128062570445072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3406] Loss: 0.5127758544288288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3407] Loss: 0.5127333339263869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3408] Loss: 0.5126221631161915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3409] Loss: 0.5127226935815877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3410] Loss: 0.5127072804909328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3411] Loss: 0.5126720860585062\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3412] Loss: 0.5128870428707335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3413] Loss: 0.5128204681638845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3414] Loss: 0.5127793775188939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3415] Loss: 0.5128463023244412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3416] Loss: 0.5127588165879086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3417] Loss: 0.5128969832419283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3418] Loss: 0.5128506013490721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3419] Loss: 0.5129419704069997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3420] Loss: 0.5130016136568846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3421] Loss: 0.5131314686332852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3422] Loss: 0.5132247568926886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3423] Loss: 0.5131854822381517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3424] Loss: 0.5131407430678013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3425] Loss: 0.5130956535938469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3426] Loss: 0.513061839964532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3427] Loss: 0.51297289743524\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3428] Loss: 0.5130910964464389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3429] Loss: 0.513008956342129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3430] Loss: 0.5129680183397173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3431] Loss: 0.5128886398031303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3432] Loss: 0.5127810958994732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3433] Loss: 0.512820694892528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3434] Loss: 0.5128109137261371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3435] Loss: 0.5128431141631216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3436] Loss: 0.5127701487236861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3437] Loss: 0.5127839581741177\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3438] Loss: 0.512908137179504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3439] Loss: 0.5128772066300051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3440] Loss: 0.5128311799909068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3441] Loss: 0.5129410792326448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3442] Loss: 0.5129622833760097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3443] Loss: 0.5131661511152383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3444] Loss: 0.513094750766902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3445] Loss: 0.5131977592406118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3446] Loss: 0.5130925509540684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3447] Loss: 0.513189417075088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3448] Loss: 0.513117355319251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3449] Loss: 0.5132009503832476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3450] Loss: 0.5131545069570445\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3451] Loss: 0.5132661056798237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3452] Loss: 0.5135718435454255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3453] Loss: 0.513714743488871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3454] Loss: 0.5136504316419039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3455] Loss: 0.5136535879913939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3456] Loss: 0.513692619864502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3457] Loss: 0.5135744067827321\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3458] Loss: 0.5135322348739108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3459] Loss: 0.5137161828676051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3460] Loss: 0.5137252308453544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3461] Loss: 0.5136691440024322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3462] Loss: 0.5136520001724322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3463] Loss: 0.5137845948817888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3464] Loss: 0.5139320295104806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3465] Loss: 0.5138927749570856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3466] Loss: 0.514017817918438\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3467] Loss: 0.513964247635018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3468] Loss: 0.5138768824151112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3469] Loss: 0.5138274695007895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3470] Loss: 0.5138366316520685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3471] Loss: 0.5139571341237117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3472] Loss: 0.5138899849464892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3473] Loss: 0.513855385078275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3474] Loss: 0.5137752833174147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3475] Loss: 0.5137955728811519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3476] Loss: 0.5138651040443599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3477] Loss: 0.5138222069341726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3478] Loss: 0.5139238629741167\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3479] Loss: 0.513873317399062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3480] Loss: 0.5138742076628136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3481] Loss: 0.5139078072368477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3482] Loss: 0.513882964949749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3483] Loss: 0.5139409667548209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3484] Loss: 0.5140853008730105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3485] Loss: 0.5139714800109383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3486] Loss: 0.5138784591759485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3487] Loss: 0.5138127154748553\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3488] Loss: 0.5138182359915839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3489] Loss: 0.513741593571588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3490] Loss: 0.5137439350403795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3491] Loss: 0.5137111076879566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3492] Loss: 0.513640870185885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3493] Loss: 0.513591275608184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3494] Loss: 0.5137142388364825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3495] Loss: 0.5136999700287079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3496] Loss: 0.5138580856940846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3497] Loss: 0.5138448580106765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3498] Loss: 0.5140274500541658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3499] Loss: 0.5139370475297151\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3500] Loss: 0.5139040022789435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3501] Loss: 0.5138042068668769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3502] Loss: 0.5138761735556119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3503] Loss: 0.5138381073457359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3504] Loss: 0.5137428841892687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3505] Loss: 0.5138379484962342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3506] Loss: 0.5137964991308023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3507] Loss: 0.5137936466429128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3508] Loss: 0.5138357436263299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3509] Loss: 0.5138050636854836\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3510] Loss: 0.5138346243798733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3511] Loss: 0.5139342313387203\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8449\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3512] Loss: 0.513884306897091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3513] Loss: 0.5138651526970661\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3514] Loss: 0.5139265316383499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3515] Loss: 0.5138604816303476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3516] Loss: 0.5140729109128951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3517] Loss: 0.5139977425525146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3518] Loss: 0.5139393166123711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3519] Loss: 0.5139707023517441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3520] Loss: 0.5138643083085254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3521] Loss: 0.5138943598678998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3522] Loss: 0.5140767505820371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3523] Loss: 0.514052521883749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3524] Loss: 0.5140123987878168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3525] Loss: 0.5139472757208209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3526] Loss: 0.5140294729082274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3527] Loss: 0.5139301902051789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3528] Loss: 0.5138651554585688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3529] Loss: 0.5139196611775512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3530] Loss: 0.5139275427438961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3531] Loss: 0.5140517635708098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3532] Loss: 0.5140059692951562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3533] Loss: 0.5140416318568575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3534] Loss: 0.5139722931174138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3535] Loss: 0.5138557610383704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3536] Loss: 0.5137847776365296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3537] Loss: 0.5137249652873497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3538] Loss: 0.5136493108576088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3539] Loss: 0.5137561200307753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3540] Loss: 0.5137365460248277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3541] Loss: 0.5138865274953905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3542] Loss: 0.513831306558525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3543] Loss: 0.5138847023405889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3544] Loss: 0.5139237143536421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3545] Loss: 0.5140082645701026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3546] Loss: 0.5139714664042153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3547] Loss: 0.5139510020157362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3548] Loss: 0.5140097687036839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3549] Loss: 0.5140175841707036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3550] Loss: 0.5139667252715873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3551] Loss: 0.5140259682357958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3552] Loss: 0.5139650793735092\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3553] Loss: 0.5139837393459425\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3554] Loss: 0.5141661991848661\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3555] Loss: 0.5141418867070099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3556] Loss: 0.5140979092908156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3557] Loss: 0.5140838641679439\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3558] Loss: 0.5140064961455391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3559] Loss: 0.5140510483789381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3560] Loss: 0.5139462833668365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3561] Loss: 0.514058760246149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3562] Loss: 0.513982611311646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3563] Loss: 0.5139052099598613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3564] Loss: 0.5138701570477061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3565] Loss: 0.513879377844876\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3566] Loss: 0.5138366105698088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3567] Loss: 0.5138895484281593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3568] Loss: 0.5138767062204581\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3569] Loss: 0.5139929333195729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3570] Loss: 0.514012078673037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3571] Loss: 0.5141871471599363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3572] Loss: 0.5141235185235953\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3573] Loss: 0.5140176586103797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3574] Loss: 0.5139160196323619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3575] Loss: 0.513891381939317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3576] Loss: 0.5138522467617886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3577] Loss: 0.5138756327299485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3578] Loss: 0.5138001841489308\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3579] Loss: 0.5137360112583182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3580] Loss: 0.5138847226331599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3581] Loss: 0.5137923659689455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3582] Loss: 0.513755244013737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3583] Loss: 0.5137054663438809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3584] Loss: 0.5138611166884004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3585] Loss: 0.5138749972252342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3586] Loss: 0.5137831594538239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3587] Loss: 0.5138839973202766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3588] Loss: 0.5138516735099135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3589] Loss: 0.5139222004102708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3590] Loss: 0.5138450712013941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3591] Loss: 0.5139419443661112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3592] Loss: 0.5138460270610821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3593] Loss: 0.5138335735707789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3594] Loss: 0.5137989189809167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3595] Loss: 0.5139379778837655\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3596] Loss: 0.5139710458595491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3597] Loss: 0.5140930133698864\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3598] Loss: 0.5141108965481888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3599] Loss: 0.5140453053788484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3600] Loss: 0.5139905628697\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3601] Loss: 0.5141239161872817\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3602] Loss: 0.5141114499492216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3603] Loss: 0.5142586613805634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3604] Loss: 0.514196779475377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3605] Loss: 0.5141380686896683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3606] Loss: 0.5142331462719258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3607] Loss: 0.5143470028340142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3608] Loss: 0.5143534847955461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3609] Loss: 0.5143912177954847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3610] Loss: 0.514389448872497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3611] Loss: 0.5145457920884747\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3612] Loss: 0.5147026082216195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3613] Loss: 0.5147277844929903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3614] Loss: 0.5148085127584636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3615] Loss: 0.5146976776505054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3616] Loss: 0.5146566709384792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3617] Loss: 0.5146679551099941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3618] Loss: 0.5147252549756525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3619] Loss: 0.5146540671529476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3620] Loss: 0.5148161002054475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3621] Loss: 0.5147689664020909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3622] Loss: 0.5150047924446186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3623] Loss: 0.5150028457174032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3624] Loss: 0.5149832682524955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3625] Loss: 0.5149085990929106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3626] Loss: 0.5149485109677593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3627] Loss: 0.5149764261463341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3628] Loss: 0.5150101286018025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3629] Loss: 0.5149472392964111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3630] Loss: 0.514846529544164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3631] Loss: 0.5150424312109062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3632] Loss: 0.5150222643532102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3633] Loss: 0.5150380451405159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3634] Loss: 0.5150053936811629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3635] Loss: 0.5149655029582977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3636] Loss: 0.5149125747110931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3637] Loss: 0.515046347214995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3638] Loss: 0.5152206018929134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3639] Loss: 0.5151350245560729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3640] Loss: 0.5150478606359266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3641] Loss: 0.5149697316878903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3642] Loss: 0.5149256106261473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3643] Loss: 0.5149852936873982\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3644] Loss: 0.5149026399985879\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3645] Loss: 0.5149323419568261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3646] Loss: 0.5150414843121733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3647] Loss: 0.5150399692059703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3648] Loss: 0.5151860798911504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3649] Loss: 0.5152045862856622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3650] Loss: 0.5151907710465277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3651] Loss: 0.515120674298281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3652] Loss: 0.5151140173241486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3653] Loss: 0.5150871523722622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3654] Loss: 0.5150114272984158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3655] Loss: 0.5149452502710452\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3656] Loss: 0.5148434154523533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3657] Loss: 0.5148038128784508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3658] Loss: 0.514769230601218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3659] Loss: 0.5146956179644729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3660] Loss: 0.5146751059520812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3661] Loss: 0.5147528307602846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3662] Loss: 0.5148774273788113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3663] Loss: 0.5147758315205385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3664] Loss: 0.5147074865875323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3665] Loss: 0.5148703998034427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3666] Loss: 0.5147872214006078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3667] Loss: 0.5147845575542891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3668] Loss: 0.5148436736275381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3669] Loss: 0.5148367730263254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3670] Loss: 0.5147879919485201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3671] Loss: 0.5149741047396383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3672] Loss: 0.5149144807018712\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3673] Loss: 0.5149059931650743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3674] Loss: 0.5148005692333021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3675] Loss: 0.5147561776835772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3676] Loss: 0.5146741144363819\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3677] Loss: 0.5147440443335929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3678] Loss: 0.5148371792624168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3679] Loss: 0.5147831272809955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3680] Loss: 0.5148584788577037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3681] Loss: 0.5148161746930414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3682] Loss: 0.5147088230652136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3683] Loss: 0.5148487810997643\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3684] Loss: 0.514927680932206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3685] Loss: 0.5148924478114121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3686] Loss: 0.5149766267828587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3687] Loss: 0.5149072156336685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3688] Loss: 0.5148474123209359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3689] Loss: 0.5147933158420024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3690] Loss: 0.5147813679474704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3691] Loss: 0.5149212052371659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3692] Loss: 0.5149463367941692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3693] Loss: 0.5150168933023495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3694] Loss: 0.5150541238599087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3695] Loss: 0.5149568607893245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3696] Loss: 0.5148571361714538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3697] Loss: 0.5150280691751089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3698] Loss: 0.5150838770869759\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3699] Loss: 0.5150698623516299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3700] Loss: 0.515031839234515\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3701] Loss: 0.5150390473354651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3702] Loss: 0.5152402717694827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3703] Loss: 0.5154724385002508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3704] Loss: 0.5153733462179237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3705] Loss: 0.5152735691908193\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3706] Loss: 0.5152277211345108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3707] Loss: 0.515389222338269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3708] Loss: 0.5152923134927305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3709] Loss: 0.5153770997407623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3710] Loss: 0.5154133218573407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3711] Loss: 0.5153401915643856\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3712] Loss: 0.5153600814648154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3713] Loss: 0.5154658441027365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3714] Loss: 0.5155874262895254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3715] Loss: 0.5155899984462771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3716] Loss: 0.515505732263131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3717] Loss: 0.515526620057241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3718] Loss: 0.5155921972907169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3719] Loss: 0.5155348210189528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3720] Loss: 0.5154480692521434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3721] Loss: 0.5154404050880264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3722] Loss: 0.5153997785285357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3723] Loss: 0.5154789083895563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3724] Loss: 0.5155589897336245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3725] Loss: 0.5154624680384506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3726] Loss: 0.5153856473043561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3727] Loss: 0.5153385635383165\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3728] Loss: 0.5153681558850317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3729] Loss: 0.5152693458200056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3730] Loss: 0.5152374004493959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3731] Loss: 0.515218466238108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3732] Loss: 0.5151602968869559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3733] Loss: 0.5151911131064417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3734] Loss: 0.5152406537068763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3735] Loss: 0.5153082601830017\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3736] Loss: 0.5154527838573423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3737] Loss: 0.5156344664855974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3738] Loss: 0.515576901281301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3739] Loss: 0.5155398449585068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3740] Loss: 0.5154923808786891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3741] Loss: 0.5154565066671046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3742] Loss: 0.5154329856147639\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3743] Loss: 0.5155553651315082\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3744] Loss: 0.5154954405915022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3745] Loss: 0.5155332099571479\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3746] Loss: 0.5154281768565641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3747] Loss: 0.5153980475491051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3748] Loss: 0.5154734783063265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3749] Loss: 0.515571991573443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3750] Loss: 0.5155509838305505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3751] Loss: 0.5155534127020829\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3752] Loss: 0.5155020640734388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3753] Loss: 0.5155611060217432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3754] Loss: 0.5154799060108262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3755] Loss: 0.5154275833136863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3756] Loss: 0.5153400731799196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3757] Loss: 0.5154300138955488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3758] Loss: 0.5153377527104956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3759] Loss: 0.5152410014239265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3760] Loss: 0.5151839993550227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3761] Loss: 0.5153057008514181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3762] Loss: 0.515282724213043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3763] Loss: 0.5152272394518467\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3764] Loss: 0.5152764390254211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3765] Loss: 0.51528123385529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3766] Loss: 0.5152735090299672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3767] Loss: 0.5153546291221898\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3768] Loss: 0.5154724743667741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3769] Loss: 0.5154767369850899\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3770] Loss: 0.5153711775146379\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3771] Loss: 0.5153181080217195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3772] Loss: 0.5153475272955155\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3773] Loss: 0.5153094028041949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3774] Loss: 0.5153488173039958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3775] Loss: 0.515301953513991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3776] Loss: 0.5153479985550584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3777] Loss: 0.5153089843742849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3778] Loss: 0.5153232005902428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3779] Loss: 0.5153024254241971\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3780] Loss: 0.5152566761325259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3781] Loss: 0.5151852278471066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3782] Loss: 0.5152426238073842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3783] Loss: 0.5152093737434256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3784] Loss: 0.5152119975502927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3785] Loss: 0.5151339393808642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3786] Loss: 0.5152002112059803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3787] Loss: 0.515123922212989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3788] Loss: 0.5152213343983383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3789] Loss: 0.5151458261452204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3790] Loss: 0.5152054795132177\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3791] Loss: 0.5153014801853643\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3792] Loss: 0.5152213007829162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3793] Loss: 0.5153086968957742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3794] Loss: 0.5154345272682906\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3795] Loss: 0.5153790489451526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3796] Loss: 0.5154385202208889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3797] Loss: 0.5153599049151397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3798] Loss: 0.5153866207494497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3799] Loss: 0.5153236328903105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3800] Loss: 0.5153059327765439\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3801] Loss: 0.5153271580260418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3802] Loss: 0.5154933411886017\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3803] Loss: 0.5155804983088212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3804] Loss: 0.5154747068610565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3805] Loss: 0.5153917267036727\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3806] Loss: 0.515313269534302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3807] Loss: 0.5153371448729448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3808] Loss: 0.5153522046679363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3809] Loss: 0.5153102100202624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3810] Loss: 0.5152458329363303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3811] Loss: 0.5153607340680221\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3812] Loss: 0.5152711234332431\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3813] Loss: 0.5152859799687949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3814] Loss: 0.5152291625413421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3815] Loss: 0.5154705905517544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3816] Loss: 0.5154902764110946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3817] Loss: 0.5155202435120593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3818] Loss: 0.515567053708517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3819] Loss: 0.5155203878861534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3820] Loss: 0.5155069012746349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3821] Loss: 0.5155773488048051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3822] Loss: 0.5155442690089849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3823] Loss: 0.5154446036730006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3824] Loss: 0.5154137047986777\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3825] Loss: 0.5154506428299267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3826] Loss: 0.5155070798945657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3827] Loss: 0.5156169007674088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3828] Loss: 0.5156087202433723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3829] Loss: 0.5158470145332127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3830] Loss: 0.5158207803426018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3831] Loss: 0.5157208594257162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3832] Loss: 0.5157184630883594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3833] Loss: 0.5157115851813973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3834] Loss: 0.5157347582684096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3835] Loss: 0.5157772359901801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3836] Loss: 0.5159247480937679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3837] Loss: 0.5160175728249701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3838] Loss: 0.5160433752014517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3839] Loss: 0.5161493019088678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3840] Loss: 0.5160856351032629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3841] Loss: 0.5161578720824134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3842] Loss: 0.5161970871783533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3843] Loss: 0.5161313965912163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3844] Loss: 0.5160724285053649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3845] Loss: 0.5159903824642145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3846] Loss: 0.5159890966565847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3847] Loss: 0.5161247013043528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3848] Loss: 0.5162333175450688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3849] Loss: 0.5163463793291093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3850] Loss: 0.5163766935423105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3851] Loss: 0.5164886244049475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3852] Loss: 0.5166141596661222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3853] Loss: 0.5166247023005989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3854] Loss: 0.5167005700972733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3855] Loss: 0.5166193446637269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3856] Loss: 0.5167159309919124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3857] Loss: 0.5167141984185426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3858] Loss: 0.5167277598867066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3859] Loss: 0.516666188471244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3860] Loss: 0.516605361660025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3861] Loss: 0.5165825720395091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3862] Loss: 0.516516520723985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3863] Loss: 0.5164596925351003\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3864] Loss: 0.5164173484763789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3865] Loss: 0.5163329374106204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3866] Loss: 0.5163088901916406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3867] Loss: 0.5164221895879175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3868] Loss: 0.5165391207903961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3869] Loss: 0.5164737157672245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3870] Loss: 0.5166950437595093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3871] Loss: 0.516631080272722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3872] Loss: 0.5166128024653903\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3873] Loss: 0.5166740015729566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3874] Loss: 0.5166660468453955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3875] Loss: 0.5165888415270362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3876] Loss: 0.5166662564402666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3877] Loss: 0.5166119108231584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3878] Loss: 0.5165437193467965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3879] Loss: 0.5164915021828496\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3880] Loss: 0.5164465506679934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3881] Loss: 0.5163922954944148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3882] Loss: 0.5164827686443942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3883] Loss: 0.5164803547540052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3884] Loss: 0.5166379488611547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3885] Loss: 0.5165729010767407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3886] Loss: 0.5166885241980872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3887] Loss: 0.516746133243787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3888] Loss: 0.5166542616211288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3889] Loss: 0.5167074708384358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3890] Loss: 0.5166272798586173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3891] Loss: 0.5166022598620034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3892] Loss: 0.516736287238117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3893] Loss: 0.5166814717409915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3894] Loss: 0.5167654189182088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3895] Loss: 0.516725438463459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3896] Loss: 0.5167756987987576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3897] Loss: 0.5167144757915747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3898] Loss: 0.5167252049025988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3899] Loss: 0.5168408460323703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3900] Loss: 0.5168692106254684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3901] Loss: 0.5168195294001503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3902] Loss: 0.5168878457790135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3903] Loss: 0.5168181959942594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3904] Loss: 0.516914157882886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3905] Loss: 0.5169056089707432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3906] Loss: 0.5169341227893133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3907] Loss: 0.5169037437751288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3908] Loss: 0.5168918947688687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3909] Loss: 0.5168791418217252\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3910] Loss: 0.516917834790314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3911] Loss: 0.517031755089164\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3912] Loss: 0.516940449000386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3913] Loss: 0.5168771520454044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3914] Loss: 0.5169209271459265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3915] Loss: 0.5168557803973928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3916] Loss: 0.5168464170069955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3917] Loss: 0.5169201453462667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3918] Loss: 0.5168187455776991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3919] Loss: 0.5167397304071153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3920] Loss: 0.5168331366873556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3921] Loss: 0.5169219845281742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3922] Loss: 0.5168814838046136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3923] Loss: 0.5168407286738078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3924] Loss: 0.5170433019109476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3925] Loss: 0.5170481079101213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3926] Loss: 0.5170328459403624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3927] Loss: 0.517015953200199\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3928] Loss: 0.5171344363991108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3929] Loss: 0.5170611202516344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3930] Loss: 0.5169814825275837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3931] Loss: 0.5169679214690589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3932] Loss: 0.5169452684036059\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3933] Loss: 0.5169253117929172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3934] Loss: 0.516917114280965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3935] Loss: 0.5168487063681122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3936] Loss: 0.5168181319100705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3937] Loss: 0.5168021694934504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3938] Loss: 0.5167769159786968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3939] Loss: 0.5167359045075758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3940] Loss: 0.5166370736657704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3941] Loss: 0.5166015402850658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3942] Loss: 0.5165490757344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3943] Loss: 0.5166595909682649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3944] Loss: 0.5167032177920736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3945] Loss: 0.5168060125091711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3946] Loss: 0.5167265279875576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3947] Loss: 0.5166821905674891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3948] Loss: 0.5166906006179138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3949] Loss: 0.5166061562649834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3950] Loss: 0.5166986459866166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3951] Loss: 0.5167075532383781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3952] Loss: 0.516660954385459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3953] Loss: 0.5166399681157855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3954] Loss: 0.5166215593434447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3955] Loss: 0.5166232885271095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3956] Loss: 0.5166136077105203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3957] Loss: 0.5166905641356931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3958] Loss: 0.5168269604591441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3959] Loss: 0.51672874914715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3960] Loss: 0.5166801827498104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3961] Loss: 0.5168080068012832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3962] Loss: 0.5167422789350791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3963] Loss: 0.5166845958655101\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3964] Loss: 0.5166211223362427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3965] Loss: 0.5166592677189886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3966] Loss: 0.5169014351106145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3967] Loss: 0.5168699657887712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3968] Loss: 0.5168071986580598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3969] Loss: 0.5169083963165259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3970] Loss: 0.517046963923067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3971] Loss: 0.5169661241273803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3972] Loss: 0.5170287689477084\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3973] Loss: 0.5169919014725767\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3974] Loss: 0.5170175623079852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3975] Loss: 0.5170736272570718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3976] Loss: 0.5170168087092769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3977] Loss: 0.5171394626439941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3978] Loss: 0.5170715481446757\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3979] Loss: 0.5170269873917842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3980] Loss: 0.5170814628173364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3981] Loss: 0.5170928913272447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3982] Loss: 0.517038447855358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3983] Loss: 0.5171132702074716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3984] Loss: 0.517105414112306\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3985] Loss: 0.5170590032562078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3986] Loss: 0.5170631933018514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3987] Loss: 0.5170767464778668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3988] Loss: 0.5171407196004546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3989] Loss: 0.5171711939090146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3990] Loss: 0.5171228130958203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3991] Loss: 0.5171149534480184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3992] Loss: 0.5171069349496615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3993] Loss: 0.5170268445967464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3994] Loss: 0.5171599524602483\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3995] Loss: 0.517382835379803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3996] Loss: 0.5174902001426812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3997] Loss: 0.5174454617054167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3998] Loss: 0.5175314299856235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 3999] Loss: 0.5174702184483533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4000] Loss: 0.5174043668503406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4001] Loss: 0.517365209154296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4002] Loss: 0.5173974450792849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4003] Loss: 0.5173773740863786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4004] Loss: 0.5173943787088173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4005] Loss: 0.5174762484895995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4006] Loss: 0.5174105963808243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4007] Loss: 0.517322032923592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4008] Loss: 0.5173848583919447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4009] Loss: 0.5173861431620672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4010] Loss: 0.5172987187377044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4011] Loss: 0.5172856695794678\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8258000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4012] Loss: 0.5172379842992852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4013] Loss: 0.5172278692926164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4014] Loss: 0.5173361784561652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4015] Loss: 0.5172671121426894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4016] Loss: 0.5172212250228957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4017] Loss: 0.5171466246480986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4018] Loss: 0.5170744345857928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4019] Loss: 0.5171179800257447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4020] Loss: 0.5172349092075628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4021] Loss: 0.5171386055697135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4022] Loss: 0.517124436889056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4023] Loss: 0.5171104482002079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4024] Loss: 0.5170958931380263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4025] Loss: 0.5172593308122535\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4026] Loss: 0.5172329076965243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4027] Loss: 0.5171766259858714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4028] Loss: 0.517165915982299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4029] Loss: 0.517127531407807\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4030] Loss: 0.5171244062567977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4031] Loss: 0.5170852524233419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4032] Loss: 0.5171219694435021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4033] Loss: 0.5171066412880378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4034] Loss: 0.5171062290313295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4035] Loss: 0.517145517793953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4036] Loss: 0.5172255706513259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4037] Loss: 0.5172198121342201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4038] Loss: 0.5171368700745695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4039] Loss: 0.5171196215650656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4040] Loss: 0.5171143062307206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4041] Loss: 0.5170702241578476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4042] Loss: 0.5169893758179512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4043] Loss: 0.5169172203561047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4044] Loss: 0.5168196399104278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4045] Loss: 0.516774091352162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4046] Loss: 0.5167056743805697\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4047] Loss: 0.5166190810120305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4048] Loss: 0.5166382184847053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4049] Loss: 0.5167435824357375\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4050] Loss: 0.5167048814480251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4051] Loss: 0.5167876166270231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4052] Loss: 0.5166913670866857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4053] Loss: 0.5167116492927798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4054] Loss: 0.5168338238469768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4055] Loss: 0.5170297924066632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4056] Loss: 0.5169848396458063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4057] Loss: 0.5169715226486363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4058] Loss: 0.5169142839638464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4059] Loss: 0.5169115885273441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4060] Loss: 0.5168640293034029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4061] Loss: 0.5168318071858173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4062] Loss: 0.5168049327508958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4063] Loss: 0.5169291083003446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4064] Loss: 0.5168401319858528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4065] Loss: 0.5168128106832169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4066] Loss: 0.5167638541055745\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4067] Loss: 0.5166958784383436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4068] Loss: 0.5168302062942849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4069] Loss: 0.5169052415582095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4070] Loss: 0.5169061642600579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4071] Loss: 0.516859899249113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4072] Loss: 0.5168506055568324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4073] Loss: 0.5168083022559522\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4074] Loss: 0.516900092263013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4075] Loss: 0.5169725133192322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4076] Loss: 0.5169645162927121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4077] Loss: 0.5169540278327408\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4078] Loss: 0.5169390646548683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4079] Loss: 0.5171210940610999\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4080] Loss: 0.517238637821681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4081] Loss: 0.5171830408114474\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4082] Loss: 0.5172497430213489\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4083] Loss: 0.5174453635526711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4084] Loss: 0.5175351449145107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4085] Loss: 0.5176333514126864\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4086] Loss: 0.5176023600385493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4087] Loss: 0.517685062814632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4088] Loss: 0.5175908495921406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4089] Loss: 0.5176176161331716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4090] Loss: 0.5175680017504612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4091] Loss: 0.5176328801968949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4092] Loss: 0.5176496857341477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4093] Loss: 0.5176125945008203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4094] Loss: 0.5175520541878151\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4095] Loss: 0.5175175542445695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4096] Loss: 0.5174779442522822\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4097] Loss: 0.5174563872239235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4098] Loss: 0.5174628137844463\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4099] Loss: 0.5174682694213508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4100] Loss: 0.5174413999475145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4101] Loss: 0.517380840356421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4102] Loss: 0.5173597304536266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4103] Loss: 0.5174050473367275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4104] Loss: 0.5173858578379578\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4105] Loss: 0.5172974994849098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4106] Loss: 0.5173687804262418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4107] Loss: 0.5172998201591889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4108] Loss: 0.5172627276019828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4109] Loss: 0.5172770595047069\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4110] Loss: 0.5172612722549174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4111] Loss: 0.5173379430106135\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4112] Loss: 0.51745277754073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4113] Loss: 0.517514394334115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4114] Loss: 0.5175517707004663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4115] Loss: 0.517541817611861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4116] Loss: 0.5175215045297007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4117] Loss: 0.5174281216614591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4118] Loss: 0.5175230023213673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4119] Loss: 0.5174565497454463\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4120] Loss: 0.5174090985322263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4121] Loss: 0.517472453314292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4122] Loss: 0.5175740811953183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4123] Loss: 0.5176092669710108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4124] Loss: 0.517569518375443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4125] Loss: 0.5175144390487737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4126] Loss: 0.5176214819271282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4127] Loss: 0.5176450946374199\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4128] Loss: 0.5176996438006725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4129] Loss: 0.5176045397519407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4130] Loss: 0.517540677159366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4131] Loss: 0.5176601621721766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4132] Loss: 0.5176002595641546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4133] Loss: 0.5175794307702883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4134] Loss: 0.5175669430148588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4135] Loss: 0.5176852105280448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4136] Loss: 0.517595419634526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4137] Loss: 0.5175444371926413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4138] Loss: 0.5174587459283929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4139] Loss: 0.5174027239653847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4140] Loss: 0.5173491936864604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4141] Loss: 0.5172635133364614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4142] Loss: 0.5173648678234036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4143] Loss: 0.5174166068840775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4144] Loss: 0.5173670239193006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4145] Loss: 0.5173703436551429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4146] Loss: 0.5174167431989322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4147] Loss: 0.517401335054713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4148] Loss: 0.5174213062814135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4149] Loss: 0.5173718419359031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4150] Loss: 0.5174138819901171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4151] Loss: 0.5173658326255919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4152] Loss: 0.517447496368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4153] Loss: 0.5173889497929538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4154] Loss: 0.5173258208245465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4155] Loss: 0.517239126742949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4156] Loss: 0.5172465075414323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4157] Loss: 0.5172206881415985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4158] Loss: 0.5172176145613521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4159] Loss: 0.5171635533341253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4160] Loss: 0.5171184806668595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4161] Loss: 0.5171409043807652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4162] Loss: 0.5170580150737036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4163] Loss: 0.5170084428222677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4164] Loss: 0.5170936280658615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4165] Loss: 0.5170298985841336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4166] Loss: 0.5169536012104616\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4167] Loss: 0.5170980869729499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4168] Loss: 0.5170670255749782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4169] Loss: 0.5171628089819686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4170] Loss: 0.5171274407558103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4171] Loss: 0.5171549801242316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4172] Loss: 0.5171226977341686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4173] Loss: 0.5170460115714441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4174] Loss: 0.5170176396772721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4175] Loss: 0.5169408810984389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4176] Loss: 0.5169012326232996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4177] Loss: 0.5168896514403257\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4178] Loss: 0.5168575946903801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4179] Loss: 0.5168856948849679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4180] Loss: 0.5168379573065188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4181] Loss: 0.5168590647214377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4182] Loss: 0.5168414465222743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4183] Loss: 0.5167686109837544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4184] Loss: 0.5167852705369906\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4185] Loss: 0.5168455793744042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4186] Loss: 0.5167978507617612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4187] Loss: 0.5168518059468314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4188] Loss: 0.516815403173513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4189] Loss: 0.5167480633290574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4190] Loss: 0.5168796715529068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4191] Loss: 0.5168109704896957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4192] Loss: 0.516847003710652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4193] Loss: 0.516911956867769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4194] Loss: 0.5168823718895224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4195] Loss: 0.5168225997823873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4196] Loss: 0.5167567378240504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4197] Loss: 0.5167962613550666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4198] Loss: 0.5167672877278736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4199] Loss: 0.5167298105051279\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4200] Loss: 0.5167538301973809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4201] Loss: 0.5166882569875151\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4202] Loss: 0.5166095135368222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4203] Loss: 0.5165223482660314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4204] Loss: 0.5165874688252282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4205] Loss: 0.516577383912142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4206] Loss: 0.5167187607052909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4207] Loss: 0.5166854584478255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4208] Loss: 0.5166392709642826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4209] Loss: 0.5166030476289235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4210] Loss: 0.5165344448508443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4211] Loss: 0.5165415447133839\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4212] Loss: 0.5166126474665668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4213] Loss: 0.5166825166013863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4214] Loss: 0.5165922546328787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4215] Loss: 0.5166681790158816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4216] Loss: 0.5166291779784724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4217] Loss: 0.5166835631358195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4218] Loss: 0.5167186573838982\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4219] Loss: 0.516928873299331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4220] Loss: 0.5169969975948334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4221] Loss: 0.5169794413960549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4222] Loss: 0.5170988225988273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4223] Loss: 0.5173137417836168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4224] Loss: 0.5173004294395062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4225] Loss: 0.5173068842127377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4226] Loss: 0.5173001133539064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4227] Loss: 0.5174070141842714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4228] Loss: 0.5173781597210297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4229] Loss: 0.5174064774383746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4230] Loss: 0.5174203021231518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4231] Loss: 0.5174410861366957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4232] Loss: 0.5174773742696536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4233] Loss: 0.5174275126519332\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4234] Loss: 0.517403127147419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4235] Loss: 0.5173393861159382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4236] Loss: 0.5174510436821419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4237] Loss: 0.5174181861390624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4238] Loss: 0.5175191435739078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4239] Loss: 0.5176203243772093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4240] Loss: 0.5175274633929813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4241] Loss: 0.5174930107734121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4242] Loss: 0.5175258884683555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4243] Loss: 0.5176671848854685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4244] Loss: 0.5177002860842762\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4245] Loss: 0.5176121346842014\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4246] Loss: 0.5176378248510702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4247] Loss: 0.5176386091442455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4248] Loss: 0.5177651297972762\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4249] Loss: 0.5177275384690361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4250] Loss: 0.5176486761413793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4251] Loss: 0.5176148387605702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4252] Loss: 0.5177659371729396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4253] Loss: 0.517681664864615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4254] Loss: 0.5176514981465781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4255] Loss: 0.5176087287263336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4256] Loss: 0.5175757725060273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4257] Loss: 0.5175970972164936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4258] Loss: 0.5175932414368543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4259] Loss: 0.5175355511026148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4260] Loss: 0.5174863340735435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4261] Loss: 0.5175220587394359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4262] Loss: 0.517470121459126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4263] Loss: 0.517469162845529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4264] Loss: 0.5174037339359315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4265] Loss: 0.5174107686141518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4266] Loss: 0.517505221120434\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4267] Loss: 0.5174623707451654\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4268] Loss: 0.5174184678010385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4269] Loss: 0.5173696083261726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4270] Loss: 0.51745087461982\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4271] Loss: 0.5173848863097779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4272] Loss: 0.5174564811054979\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4273] Loss: 0.5173745813195197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4274] Loss: 0.5174969906832599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4275] Loss: 0.5175832472672343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4276] Loss: 0.5177021099771836\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4277] Loss: 0.5176830320347833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4278] Loss: 0.5177346110905495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4279] Loss: 0.5176837708841555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4280] Loss: 0.517624089750276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4281] Loss: 0.5175592097362641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4282] Loss: 0.5175934474745396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4283] Loss: 0.5175544491638532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4284] Loss: 0.5174982025666596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4285] Loss: 0.5174439038385619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4286] Loss: 0.5174854666318239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4287] Loss: 0.5174270670447743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4288] Loss: 0.5173872335640888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4289] Loss: 0.517323101972991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4290] Loss: 0.5173805469518932\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4291] Loss: 0.5174840471749897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4292] Loss: 0.5174466292302423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4293] Loss: 0.5173945613687958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4294] Loss: 0.5173379446854363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4295] Loss: 0.5174298815281577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4296] Loss: 0.5173508208176162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4297] Loss: 0.517423397925116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4298] Loss: 0.5174091000782588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4299] Loss: 0.5175336345491449\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4300] Loss: 0.5175346700096193\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4301] Loss: 0.5174904197328671\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4302] Loss: 0.5174245548862886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4303] Loss: 0.5173651374151673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4304] Loss: 0.5173688724982657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4305] Loss: 0.5174016009716805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4306] Loss: 0.5174263150983729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4307] Loss: 0.5173383730554192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4308] Loss: 0.5173395257472615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4309] Loss: 0.5174099850588706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4310] Loss: 0.5173940059385802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4311] Loss: 0.5175343219056565\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4312] Loss: 0.5176210628566712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4313] Loss: 0.5176852626484568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4314] Loss: 0.5178162535783746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4315] Loss: 0.5177368679905063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4316] Loss: 0.5178470770371818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4317] Loss: 0.5178599498201174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4318] Loss: 0.5177718820754721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4319] Loss: 0.5177613431839656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4320] Loss: 0.5177882171678418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4321] Loss: 0.51772446194148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4322] Loss: 0.5177183588585599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4323] Loss: 0.5177341169207983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4324] Loss: 0.517746326494542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4325] Loss: 0.5177200198876593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4326] Loss: 0.5177205724708064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4327] Loss: 0.5176882169416571\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4328] Loss: 0.517807998032455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4329] Loss: 0.5178359739363396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4330] Loss: 0.5178726086675809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4331] Loss: 0.5178739675533955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4332] Loss: 0.5178197938646714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4333] Loss: 0.5178675912622691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4334] Loss: 0.5178358846394712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4335] Loss: 0.5178818298866547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4336] Loss: 0.5179379946560493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4337] Loss: 0.5180213033436545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4338] Loss: 0.5180304072010978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4339] Loss: 0.517994731306625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4340] Loss: 0.5180034236481544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4341] Loss: 0.5180921703849316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4342] Loss: 0.5180845512368858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4343] Loss: 0.5181308483645153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4344] Loss: 0.5180591824748706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4345] Loss: 0.5180336943117239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4346] Loss: 0.5181354378992002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4347] Loss: 0.5180835819800509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4348] Loss: 0.5181070502115576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4349] Loss: 0.5180592897209099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4350] Loss: 0.5180043059013163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4351] Loss: 0.5179351532608232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4352] Loss: 0.5180080496337127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4353] Loss: 0.517977986458593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4354] Loss: 0.5178889315982391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4355] Loss: 0.517851754825354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4356] Loss: 0.5178444668596385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4357] Loss: 0.5179234427122256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4358] Loss: 0.5179012590426789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4359] Loss: 0.517892295980057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4360] Loss: 0.5179894004162255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4361] Loss: 0.5179224870741074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4362] Loss: 0.5180657009206333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4363] Loss: 0.5180235649358753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4364] Loss: 0.5180215474680167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4365] Loss: 0.5179743667443594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4366] Loss: 0.5180270001833246\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4367] Loss: 0.5180506673244296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4368] Loss: 0.5181593988969451\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4369] Loss: 0.5180981091790077\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4370] Loss: 0.5180219890308504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4371] Loss: 0.5180847084034419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4372] Loss: 0.5181148169433929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4373] Loss: 0.5180826388413259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4374] Loss: 0.518161600212688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4375] Loss: 0.5181148030548169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4376] Loss: 0.5181785019965475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4377] Loss: 0.5181239360997866\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4378] Loss: 0.5180834399646046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4379] Loss: 0.5180126983671418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4380] Loss: 0.5179359328300146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4381] Loss: 0.5179444771424586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4382] Loss: 0.5178982451142482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4383] Loss: 0.5178822600084345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4384] Loss: 0.5178970192733596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4385] Loss: 0.5178352055318894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4386] Loss: 0.5178092812928992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4387] Loss: 0.5177610992447507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4388] Loss: 0.5178713951223348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4389] Loss: 0.5178576628705158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4390] Loss: 0.5178130441028432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4391] Loss: 0.5178438868835334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4392] Loss: 0.5178564075785528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4393] Loss: 0.5179389505895363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4394] Loss: 0.5178518393887428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4395] Loss: 0.5178349527689787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4396] Loss: 0.517845675119161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4397] Loss: 0.5178819529756287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4398] Loss: 0.5179628122812558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4399] Loss: 0.5180075182173546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4400] Loss: 0.5179707133371603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4401] Loss: 0.5179015519272175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4402] Loss: 0.5178736167953168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4403] Loss: 0.5178552759955298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4404] Loss: 0.51780648131553\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4405] Loss: 0.5178469463475096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4406] Loss: 0.5178146646176398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4407] Loss: 0.5177864590638229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4408] Loss: 0.5177322243760341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4409] Loss: 0.5178121666276598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4410] Loss: 0.5177273019307699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4411] Loss: 0.5178562134267489\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4412] Loss: 0.5177898431728584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4413] Loss: 0.5177638526760613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4414] Loss: 0.5176971820030423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4415] Loss: 0.5176817327470694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4416] Loss: 0.5177357301260958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4417] Loss: 0.5177044227189166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4418] Loss: 0.5176844395613232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4419] Loss: 0.5176313050905131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4420] Loss: 0.5176860041020777\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4421] Loss: 0.5176579532294711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4422] Loss: 0.517738977694743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4423] Loss: 0.5177269735864566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4424] Loss: 0.5176590149903334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4425] Loss: 0.5176153015892472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4426] Loss: 0.5176025793808717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4427] Loss: 0.5175708876083419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4428] Loss: 0.5174814117516164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4429] Loss: 0.5174797919863309\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4430] Loss: 0.5175094757343129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4431] Loss: 0.5175151189419117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4432] Loss: 0.5174874563695276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4433] Loss: 0.5174154460111079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4434] Loss: 0.5173497888716232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4435] Loss: 0.5174932887144149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4436] Loss: 0.5174687814633578\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4437] Loss: 0.5174987889714359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4438] Loss: 0.5174215075819774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4439] Loss: 0.5174919272248877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4440] Loss: 0.5174783417135098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4441] Loss: 0.5174891762805027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4442] Loss: 0.5174559255112587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4443] Loss: 0.5175577752271318\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4444] Loss: 0.5176338253663043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4445] Loss: 0.5175677457561638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4446] Loss: 0.5175071906343829\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4447] Loss: 0.5174482594385407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4448] Loss: 0.5173958470565283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4449] Loss: 0.5173702918127364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4450] Loss: 0.5173642609476438\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4451] Loss: 0.5173605446644889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4452] Loss: 0.5174534827818549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4453] Loss: 0.517378728412566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4454] Loss: 0.5173867091377838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4455] Loss: 0.5173774802223963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4456] Loss: 0.5175046766893368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4457] Loss: 0.5174415685049576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4458] Loss: 0.517514010538416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4459] Loss: 0.5176340257307945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4460] Loss: 0.5175847917455662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4461] Loss: 0.5175667254155691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4462] Loss: 0.5177176802905525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4463] Loss: 0.517677322874454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4464] Loss: 0.5175854224347984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4465] Loss: 0.517508833839378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4466] Loss: 0.5174528390048608\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4467] Loss: 0.5174806559962518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4468] Loss: 0.5174471534536366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4469] Loss: 0.5174101369418288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4470] Loss: 0.5173574578506176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4471] Loss: 0.5173251291878505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4472] Loss: 0.5172614825683552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4473] Loss: 0.5172190860067989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4474] Loss: 0.5172872939847192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4475] Loss: 0.5172341610639724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4476] Loss: 0.5171977392709202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4477] Loss: 0.5172286575060513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4478] Loss: 0.5172205792003942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4479] Loss: 0.5172206960012162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4480] Loss: 0.5172438954751497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4481] Loss: 0.5172470809154923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4482] Loss: 0.5172721997713755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4483] Loss: 0.517249469496938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4484] Loss: 0.5172296812186964\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4485] Loss: 0.5171962118748599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4486] Loss: 0.5171332744585436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4487] Loss: 0.5170607628393988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4488] Loss: 0.5170310638946106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4489] Loss: 0.5170769721614143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4490] Loss: 0.5170773740577039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4491] Loss: 0.5170414968932043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4492] Loss: 0.5170937922770567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4493] Loss: 0.517052125895619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4494] Loss: 0.5170492650342574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4495] Loss: 0.5170118968003666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4496] Loss: 0.5169452024138698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4497] Loss: 0.5169730654352136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4498] Loss: 0.5169453899616164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4499] Loss: 0.5170577998469426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4500] Loss: 0.5170228989836865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4501] Loss: 0.5170641757475049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4502] Loss: 0.5170474543697191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4503] Loss: 0.5171233985564657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4504] Loss: 0.5170873435522892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4505] Loss: 0.5171835051795867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4506] Loss: 0.5171316343511607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4507] Loss: 0.517163891407708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4508] Loss: 0.5171490604986901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4509] Loss: 0.5172069351705619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4510] Loss: 0.5171624173410236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4511] Loss: 0.5172913618138122\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8234\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4512] Loss: 0.5173468891417664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4513] Loss: 0.5173382525879712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4514] Loss: 0.5173194528809496\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4515] Loss: 0.517308187838202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4516] Loss: 0.5172406070369574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4517] Loss: 0.5173032952010974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4518] Loss: 0.5172964517146111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4519] Loss: 0.5173342475371635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4520] Loss: 0.5174169428962424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4521] Loss: 0.5173622190662526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4522] Loss: 0.5174849173713361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4523] Loss: 0.5175493816254949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4524] Loss: 0.5175829085437409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4525] Loss: 0.5176960516057603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4526] Loss: 0.517637401941674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4527] Loss: 0.5176139079555516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4528] Loss: 0.517558142524296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4529] Loss: 0.5175725754093667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4530] Loss: 0.5175463103944092\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4531] Loss: 0.5175298309443276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4532] Loss: 0.5174785972360294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4533] Loss: 0.5174401998705097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4534] Loss: 0.5174913948094454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4535] Loss: 0.5175162663852206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4536] Loss: 0.5174299449846333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4537] Loss: 0.5174561690752815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4538] Loss: 0.5173890343474181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4539] Loss: 0.5174382186743721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4540] Loss: 0.5174026678942569\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4541] Loss: 0.5174215064469297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4542] Loss: 0.5174020766428421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4543] Loss: 0.517434498666978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4544] Loss: 0.5173673704229609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4545] Loss: 0.5174614171260943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4546] Loss: 0.5174711394180037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4547] Loss: 0.5174197155501484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4548] Loss: 0.5173495441948264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4549] Loss: 0.5173115432151741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4550] Loss: 0.5172776347966772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4551] Loss: 0.5173105449590669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4552] Loss: 0.5172196682883571\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4553] Loss: 0.5171569808230316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4554] Loss: 0.5171532984178155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4555] Loss: 0.5172499199401316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4556] Loss: 0.5172117839922307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4557] Loss: 0.5171681768862677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4558] Loss: 0.5171316597765319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4559] Loss: 0.5171004281121437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4560] Loss: 0.5170442711387152\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4561] Loss: 0.5171406773270515\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4562] Loss: 0.5171145668027137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4563] Loss: 0.517083488719098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4564] Loss: 0.5171337771552446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4565] Loss: 0.5171189440200714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4566] Loss: 0.5170919142215268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4567] Loss: 0.5170344385844838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4568] Loss: 0.5169943856225042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4569] Loss: 0.5169572057682001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4570] Loss: 0.5170147131265972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4571] Loss: 0.5169826545017572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4572] Loss: 0.517050925420312\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4573] Loss: 0.5170565949023548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4574] Loss: 0.5170053065842473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4575] Loss: 0.5171398806212719\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4576] Loss: 0.5171335644129457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4577] Loss: 0.5172208582436039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4578] Loss: 0.5172531456158166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4579] Loss: 0.5172183724762504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4580] Loss: 0.5171371237505273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4581] Loss: 0.5172054355017415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4582] Loss: 0.5172212174224479\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4583] Loss: 0.517139450399758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4584] Loss: 0.5170894682070878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4585] Loss: 0.5171044745108833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4586] Loss: 0.5170145256698891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4587] Loss: 0.516972919334405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4588] Loss: 0.5169744888844006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4589] Loss: 0.5169728328913384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4590] Loss: 0.5169192529338248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4591] Loss: 0.5170071719178263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4592] Loss: 0.5170693770014498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4593] Loss: 0.5170045390509812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4594] Loss: 0.5170751129816141\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4595] Loss: 0.5170023916400923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4596] Loss: 0.5169422703371641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4597] Loss: 0.5170004402392354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4598] Loss: 0.5170232914461081\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4599] Loss: 0.5169674528056177\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4600] Loss: 0.5168884581852076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4601] Loss: 0.5168857446468071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4602] Loss: 0.5169479543323158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4603] Loss: 0.5170477179670695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4604] Loss: 0.5170783803602167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4605] Loss: 0.5170018515016279\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4606] Loss: 0.5169545071839821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4607] Loss: 0.5170314048364857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4608] Loss: 0.5171084967603213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4609] Loss: 0.5171854376909237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4610] Loss: 0.5171030041357366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4611] Loss: 0.5170775432611088\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4612] Loss: 0.5170956912969509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4613] Loss: 0.5170822075640548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4614] Loss: 0.5172103445388769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4615] Loss: 0.5171532122139227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4616] Loss: 0.5171013359348773\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4617] Loss: 0.5170812800631838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4618] Loss: 0.5171017846221353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4619] Loss: 0.5170763187757749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4620] Loss: 0.5170760753380991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4621] Loss: 0.5171000533833523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4622] Loss: 0.5171791371610378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4623] Loss: 0.5171306731128182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4624] Loss: 0.5171379973123997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4625] Loss: 0.5172571618168785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4626] Loss: 0.5173497253520264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4627] Loss: 0.5173080935983772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4628] Loss: 0.5173529165137908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4629] Loss: 0.5172956996923166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4630] Loss: 0.5174217316638497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4631] Loss: 0.51751373904592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4632] Loss: 0.5174350591176693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4633] Loss: 0.5173829992780864\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4634] Loss: 0.5173705374509472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4635] Loss: 0.5173399906158447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4636] Loss: 0.5174165771704418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4637] Loss: 0.5174202049272472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4638] Loss: 0.517403298880645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4639] Loss: 0.5175065814322783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4640] Loss: 0.5174552856793415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4641] Loss: 0.5173997700329815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4642] Loss: 0.5174515956989332\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4643] Loss: 0.5174286652643918\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4644] Loss: 0.5173861304515217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4645] Loss: 0.5174211594905646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4646] Loss: 0.5173866855596666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4647] Loss: 0.517353813775588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4648] Loss: 0.5174542643010991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4649] Loss: 0.5174352964879119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4650] Loss: 0.5175271274674917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4651] Loss: 0.5176337423085072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4652] Loss: 0.5176145610876672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4653] Loss: 0.5175979646786439\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4654] Loss: 0.5176596323740184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4655] Loss: 0.5177424794442691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4656] Loss: 0.5178275021719461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4657] Loss: 0.517836289281353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4658] Loss: 0.5178349192470686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4659] Loss: 0.5179015664443478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4660] Loss: 0.5179436340461294\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4661] Loss: 0.5180079549357163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4662] Loss: 0.5180067488841124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4663] Loss: 0.5180194169577134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4664] Loss: 0.5179978303815748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4665] Loss: 0.5179868148050297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4666] Loss: 0.5179688266915008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4667] Loss: 0.5180321844518256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4668] Loss: 0.5181076547284147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4669] Loss: 0.5180798119565965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4670] Loss: 0.5182144747115671\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4671] Loss: 0.518157885463385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4672] Loss: 0.5181140950140273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4673] Loss: 0.5180526505345884\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4674] Loss: 0.5180851294659973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4675] Loss: 0.5181172612477608\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4676] Loss: 0.5180676848454596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4677] Loss: 0.5181298705637241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4678] Loss: 0.5180691961420704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4679] Loss: 0.5180174276665207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4680] Loss: 0.5180527440292372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4681] Loss: 0.5181051617230594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4682] Loss: 0.5181074123811744\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4683] Loss: 0.5182227710939956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4684] Loss: 0.5182255044629486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4685] Loss: 0.5181553096935421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4686] Loss: 0.5182297849899222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4687] Loss: 0.5182235767025396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4688] Loss: 0.5183098343690941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4689] Loss: 0.5182347334894617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4690] Loss: 0.518216429971622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4691] Loss: 0.5183243778327298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4692] Loss: 0.5183117174321188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4693] Loss: 0.5183835722115973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4694] Loss: 0.51835398183669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4695] Loss: 0.5182877444807916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4696] Loss: 0.5182618945196263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4697] Loss: 0.5182010253545689\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4698] Loss: 0.5181842434229801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4699] Loss: 0.5181129909153627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4700] Loss: 0.5180828803692342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4701] Loss: 0.5180749055935926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4702] Loss: 0.5180456034042908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4703] Loss: 0.518129446641261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4704] Loss: 0.5180646579465698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4705] Loss: 0.5181097640184168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4706] Loss: 0.5180671724949824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4707] Loss: 0.5180609588441832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4708] Loss: 0.5179760147555207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4709] Loss: 0.517916713145041\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4710] Loss: 0.5178537616488479\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4711] Loss: 0.517857554845088\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4712] Loss: 0.5178307017248509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4713] Loss: 0.5178218003528163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4714] Loss: 0.5178102654760276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4715] Loss: 0.5177334155750047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4716] Loss: 0.5177669601317423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4717] Loss: 0.5178358006752215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4718] Loss: 0.517849736248529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4719] Loss: 0.5177855324464786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4720] Loss: 0.5178750144543387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4721] Loss: 0.5178258173385332\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4722] Loss: 0.5179305207143482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4723] Loss: 0.5178612901073382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4724] Loss: 0.5179348185651382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4725] Loss: 0.5179209477253798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4726] Loss: 0.5180397388831035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4727] Loss: 0.5179972937385816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4728] Loss: 0.518050307075013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4729] Loss: 0.5181424055627297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4730] Loss: 0.5181235182836157\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4731] Loss: 0.5180606404784956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4732] Loss: 0.518043258287849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4733] Loss: 0.5179901038078852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4734] Loss: 0.5180490322157063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4735] Loss: 0.5180563393429186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4736] Loss: 0.5179739235576385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4737] Loss: 0.5180269523393976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4738] Loss: 0.5179621352829117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4739] Loss: 0.5180531613622102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4740] Loss: 0.5180176460010222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4741] Loss: 0.5179843708038104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4742] Loss: 0.5179345380648661\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4743] Loss: 0.5180275448453417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4744] Loss: 0.5179617452696071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4745] Loss: 0.5179131021993642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4746] Loss: 0.518037079523007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4747] Loss: 0.5180185807498243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4748] Loss: 0.5180023277302349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4749] Loss: 0.5179558848647133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4750] Loss: 0.5178975497952328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4751] Loss: 0.517943307441154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4752] Loss: 0.5179453145138276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4753] Loss: 0.5178884553222366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4754] Loss: 0.5179627158632546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4755] Loss: 0.5180788010499222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4756] Loss: 0.5181144153312361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4757] Loss: 0.518161197336163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4758] Loss: 0.5181298212318378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4759] Loss: 0.5182109112408785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4760] Loss: 0.5183342311697847\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4761] Loss: 0.5183029500625185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4762] Loss: 0.518305611068616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4763] Loss: 0.5182540770133972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4764] Loss: 0.5183042355046398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4765] Loss: 0.5183905636766962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4766] Loss: 0.5183961957910175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4767] Loss: 0.5183158986531207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4768] Loss: 0.5182940789746421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4769] Loss: 0.518344843830545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4770] Loss: 0.5183046411495533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4771] Loss: 0.518381023768583\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4772] Loss: 0.5183825221442382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4773] Loss: 0.5183344002993991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4774] Loss: 0.5183014045408306\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4775] Loss: 0.5182360250987593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4776] Loss: 0.5182978262049851\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4777] Loss: 0.5182918563663247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4778] Loss: 0.518257399987789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4779] Loss: 0.5182342879852888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4780] Loss: 0.5183229743282745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4781] Loss: 0.5184093747079526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4782] Loss: 0.5184190987612814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4783] Loss: 0.5184557531879012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4784] Loss: 0.5183964201785662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4785] Loss: 0.518449404975127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4786] Loss: 0.5184070673728514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4787] Loss: 0.5184149142073392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4788] Loss: 0.5184956985922438\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4789] Loss: 0.518612868894091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4790] Loss: 0.5185456902833185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4791] Loss: 0.5184814930797542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4792] Loss: 0.5184409579402206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4793] Loss: 0.5185205032411869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4794] Loss: 0.5184947471825683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4795] Loss: 0.5184867377250587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4796] Loss: 0.518472868895286\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4797] Loss: 0.5184543416554037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4798] Loss: 0.518462569689128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4799] Loss: 0.5185250478570181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4800] Loss: 0.5185577701577495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4801] Loss: 0.5185127129534746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4802] Loss: 0.5185067797112665\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4803] Loss: 0.5185027240651653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4804] Loss: 0.5184443316239782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4805] Loss: 0.5185365756537779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4806] Loss: 0.5184991375319452\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4807] Loss: 0.5185796203920112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4808] Loss: 0.5185671734665648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4809] Loss: 0.5186123652745469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4810] Loss: 0.5186597462171731\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4811] Loss: 0.518767666910948\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4812] Loss: 0.5187269948502575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4813] Loss: 0.5187106212363419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4814] Loss: 0.5187700618833968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4815] Loss: 0.5187577003593644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4816] Loss: 0.5186867934538772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4817] Loss: 0.518686751890188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4818] Loss: 0.5187230615002365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4819] Loss: 0.5186434808221985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4820] Loss: 0.5187140631253924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4821] Loss: 0.5186726889313581\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4822] Loss: 0.5186695716756782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4823] Loss: 0.5186972662429923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4824] Loss: 0.5187601962704929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4825] Loss: 0.5188734532749391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4826] Loss: 0.5188467321715441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4827] Loss: 0.5188328349047688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4828] Loss: 0.5189108933775499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4829] Loss: 0.5188987141000255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4830] Loss: 0.5188293976433299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4831] Loss: 0.5187946816984357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4832] Loss: 0.5187611181887142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4833] Loss: 0.5186886913159255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4834] Loss: 0.5187832931613172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4835] Loss: 0.5187394753494704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4836] Loss: 0.5187358066157175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4837] Loss: 0.5187923243862943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4838] Loss: 0.5187468988314794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4839] Loss: 0.518760619041798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4840] Loss: 0.5187409760357325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4841] Loss: 0.5188835938707341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4842] Loss: 0.518830123728112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4843] Loss: 0.5189889916750786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4844] Loss: 0.5189347735915142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4845] Loss: 0.5190723968143419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4846] Loss: 0.5191130388967995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4847] Loss: 0.5191754040518526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4848] Loss: 0.5191516426575244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4849] Loss: 0.5190836786124859\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4850] Loss: 0.5190814152703307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4851] Loss: 0.5191189250043119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4852] Loss: 0.5192012714796053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4853] Loss: 0.5191756341552339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4854] Loss: 0.5192449658038867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4855] Loss: 0.519217559122518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4856] Loss: 0.5192102870933482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4857] Loss: 0.5192081070450879\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4858] Loss: 0.5191698058164766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4859] Loss: 0.5191293862475064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4860] Loss: 0.5192363656115258\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4861] Loss: 0.5191982027251977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4862] Loss: 0.5192063493681524\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4863] Loss: 0.5191758334801001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4864] Loss: 0.5192653176143294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4865] Loss: 0.5192438977388782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4866] Loss: 0.5193866393276592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4867] Loss: 0.5193728636869028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4868] Loss: 0.5194557902872042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4869] Loss: 0.519413706092119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4870] Loss: 0.5194804053856146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4871] Loss: 0.5194030852014114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4872] Loss: 0.5193437930813135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4873] Loss: 0.5193615497463263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4874] Loss: 0.5193712599370646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4875] Loss: 0.5193574322765225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4876] Loss: 0.5193053068361914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4877] Loss: 0.5193169728409434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4878] Loss: 0.5193498957008397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4879] Loss: 0.519276006642237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4880] Loss: 0.519351249871865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4881] Loss: 0.5192836975715875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4882] Loss: 0.519250341414151\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4883] Loss: 0.5192889631094388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4884] Loss: 0.5192308590517886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4885] Loss: 0.5192824707303728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4886] Loss: 0.5192825136879898\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4887] Loss: 0.5192472284001413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4888] Loss: 0.5193398549315482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4889] Loss: 0.5193261547798053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4890] Loss: 0.5192996257584389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4891] Loss: 0.519251992001269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4892] Loss: 0.5192510725387097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4893] Loss: 0.5192321571086929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4894] Loss: 0.5192139235930178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4895] Loss: 0.5192982615943373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4896] Loss: 0.5192641901048525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4897] Loss: 0.5192146338836625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4898] Loss: 0.5192127174739416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4899] Loss: 0.5191748628083436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4900] Loss: 0.5191139973920679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4901] Loss: 0.5191294578994442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4902] Loss: 0.5190561904634717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4903] Loss: 0.519146173968555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4904] Loss: 0.5191704393328457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4905] Loss: 0.5192994139842748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4906] Loss: 0.5193917179264949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4907] Loss: 0.5194253592781786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4908] Loss: 0.5193604941921595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4909] Loss: 0.5194212157134441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4910] Loss: 0.5195683253691955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4911] Loss: 0.5195066441706272\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4912] Loss: 0.5195832362720935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4913] Loss: 0.5195095300633826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4914] Loss: 0.5196108834633602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4915] Loss: 0.519615689248151\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4916] Loss: 0.5197184551414229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4917] Loss: 0.5198011589761707\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4918] Loss: 0.5197569898611946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4919] Loss: 0.5197046927298297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4920] Loss: 0.5196601626689201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4921] Loss: 0.5197554461352275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4922] Loss: 0.5197267591520969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4923] Loss: 0.5198101511137734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4924] Loss: 0.5197710859853863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4925] Loss: 0.5198346857778888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4926] Loss: 0.5197789839981799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4927] Loss: 0.5197920239036483\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4928] Loss: 0.5198417028902126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4929] Loss: 0.5199246072383401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4930] Loss: 0.5199964639355694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4931] Loss: 0.519955896011334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4932] Loss: 0.5200406263676342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4933] Loss: 0.5199720531419044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4934] Loss: 0.5200131249800998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4935] Loss: 0.5201026258933342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4936] Loss: 0.5201349809792749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4937] Loss: 0.5201166410605849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4938] Loss: 0.5201070615895557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4939] Loss: 0.5201792636266759\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4940] Loss: 0.5201484382455289\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4941] Loss: 0.5202223755178804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4942] Loss: 0.5201908490765127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4943] Loss: 0.520227341583478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4944] Loss: 0.5202440268447875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4945] Loss: 0.5202047476605874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4946] Loss: 0.5201904064899259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4947] Loss: 0.5201908990871753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4948] Loss: 0.5201667330020878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4949] Loss: 0.5201843830044137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4950] Loss: 0.520117148532121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4951] Loss: 0.520122653153989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4952] Loss: 0.5201852254431991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4953] Loss: 0.520163795944943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4954] Loss: 0.5201274142856269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4955] Loss: 0.5200494502674206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4956] Loss: 0.5199953524301141\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4957] Loss: 0.5199416789186653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4958] Loss: 0.519916430041998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4959] Loss: 0.5198609408608285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4960] Loss: 0.5198273116752004\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4961] Loss: 0.5199006594679003\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4962] Loss: 0.5198257898331117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4963] Loss: 0.5198623268973177\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4964] Loss: 0.5199292071019795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4965] Loss: 0.5199512377331152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4966] Loss: 0.5199671386067273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4967] Loss: 0.5199056071953734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4968] Loss: 0.5199149263151134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4969] Loss: 0.5198829137950126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4970] Loss: 0.5199565089470602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4971] Loss: 0.5199169284277662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4972] Loss: 0.5198523141059679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4973] Loss: 0.5199112662039288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4974] Loss: 0.5198692304873338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4975] Loss: 0.5199268278189229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4976] Loss: 0.5198817307832686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4977] Loss: 0.5199863306756037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4978] Loss: 0.51995206683634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4979] Loss: 0.5199109693185999\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4980] Loss: 0.519895668117792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4981] Loss: 0.519916426679534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4982] Loss: 0.5199113778889926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4983] Loss: 0.5199067170471253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4984] Loss: 0.5198392608553957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4985] Loss: 0.5198304266270313\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4986] Loss: 0.5197952250126913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4987] Loss: 0.519726750069573\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4988] Loss: 0.5197710616380149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4989] Loss: 0.5198209238128222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4990] Loss: 0.5198166668714423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4991] Loss: 0.5197717120201294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4992] Loss: 0.5197600369798397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4993] Loss: 0.5197432245091086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4994] Loss: 0.5197872043712718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4995] Loss: 0.5197904516987445\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4996] Loss: 0.5197672164744424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4997] Loss: 0.5197280753667088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4998] Loss: 0.5197593202837701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 4999] Loss: 0.5198536546206681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5000] Loss: 0.5198346100730992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5001] Loss: 0.519857680381499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5002] Loss: 0.5198081999068428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5003] Loss: 0.5198577212917229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5004] Loss: 0.5198255309733116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5005] Loss: 0.5198297802313284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5006] Loss: 0.519868998344179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5007] Loss: 0.5197981200252397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5008] Loss: 0.5198412650111994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5009] Loss: 0.5197953453002228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5010] Loss: 0.519735153267781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5011] Loss: 0.519697794432085\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8353999999999999\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5012] Loss: 0.5196746313395844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5013] Loss: 0.5197080917492618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5014] Loss: 0.5196512209989302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5015] Loss: 0.5196460970994767\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5016] Loss: 0.5195740272885844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5017] Loss: 0.5196132586679201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5018] Loss: 0.5196403225630971\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5019] Loss: 0.5196456962088889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5020] Loss: 0.5196111560297383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5021] Loss: 0.5196275735801246\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5022] Loss: 0.5196536238609106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5023] Loss: 0.519717563107963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5024] Loss: 0.5196810520964225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5025] Loss: 0.5197194624217774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5026] Loss: 0.5198314753389918\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5027] Loss: 0.5198721059008143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5028] Loss: 0.5198948071752569\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5029] Loss: 0.5198263844331299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5030] Loss: 0.5197704339812024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5031] Loss: 0.5197865930384857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5032] Loss: 0.5197285808019805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5033] Loss: 0.519700548437589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5034] Loss: 0.5196440948896394\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5035] Loss: 0.5196006026682932\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5036] Loss: 0.5196069139580626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5037] Loss: 0.519530069562729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5038] Loss: 0.5195335404513002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5039] Loss: 0.5195551186694718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5040] Loss: 0.5196210072168188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5041] Loss: 0.5196371367799344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5042] Loss: 0.5196573541640236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5043] Loss: 0.5196140629584337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5044] Loss: 0.5195833256441063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5045] Loss: 0.5195608618733901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5046] Loss: 0.5194965079985231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5047] Loss: 0.5194315463994236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5048] Loss: 0.5194183333414459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5049] Loss: 0.5194020677529444\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5050] Loss: 0.5194081059833455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5051] Loss: 0.5194456585242186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5052] Loss: 0.5194009672317249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5053] Loss: 0.5193976076681026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5054] Loss: 0.5193556402547573\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5055] Loss: 0.5193788124324202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5056] Loss: 0.5193556252485112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5057] Loss: 0.519407611083848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5058] Loss: 0.5194095932569751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5059] Loss: 0.5193961332213305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5060] Loss: 0.5194154863239645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5061] Loss: 0.5193487531836021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5062] Loss: 0.5193630162109285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5063] Loss: 0.5193149151054288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5064] Loss: 0.5193080618211874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5065] Loss: 0.5192795097762484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5066] Loss: 0.5193409875257692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5067] Loss: 0.5192767472049503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5068] Loss: 0.5192834003000941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5069] Loss: 0.5192612109868823\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5070] Loss: 0.5192849388182686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5071] Loss: 0.5193342863990664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5072] Loss: 0.5192956114967682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5073] Loss: 0.519358359242959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5074] Loss: 0.5193485528700817\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5075] Loss: 0.5192775958926628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5076] Loss: 0.5192675775621107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5077] Loss: 0.5193564230382429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5078] Loss: 0.5193179155032814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5079] Loss: 0.5193254924311005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5080] Loss: 0.519404613019601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5081] Loss: 0.5195160892828109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5082] Loss: 0.5195684974796398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5083] Loss: 0.5195341047926803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5084] Loss: 0.5195415953152244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5085] Loss: 0.5195062605149108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5086] Loss: 0.5195377810721422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5087] Loss: 0.519613407026453\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5088] Loss: 0.5195850424953505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5089] Loss: 0.519532250567903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5090] Loss: 0.5195096935897936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5091] Loss: 0.5194867218303202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5092] Loss: 0.5194644686021642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5093] Loss: 0.519536596496319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5094] Loss: 0.5194893055522317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5095] Loss: 0.5194328462544548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5096] Loss: 0.5194686894535343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5097] Loss: 0.5195720637704855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5098] Loss: 0.5195176006662606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5099] Loss: 0.5195110255556445\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5100] Loss: 0.5194527466117946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5101] Loss: 0.5194002262285606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5102] Loss: 0.5193435143913022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5103] Loss: 0.5192821813861244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5104] Loss: 0.5194157010408967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5105] Loss: 0.5193862349638093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5106] Loss: 0.5194090568501862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5107] Loss: 0.519357513108797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5108] Loss: 0.5193134878039775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5109] Loss: 0.5192607672357331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5110] Loss: 0.5193148185150779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5111] Loss: 0.5193967529449274\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5112] Loss: 0.5193675935611628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5113] Loss: 0.5193211416168211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5114] Loss: 0.5193498284627985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5115] Loss: 0.5193234233300926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5116] Loss: 0.5192950247935352\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5117] Loss: 0.5193057819402008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5118] Loss: 0.519273015259791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5119] Loss: 0.5192310375921844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5120] Loss: 0.5192162915029909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5121] Loss: 0.5192029970512037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5122] Loss: 0.5191424878480532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5123] Loss: 0.5191134798366543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5124] Loss: 0.5190860602534794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5125] Loss: 0.5190535765879849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5126] Loss: 0.5190602692241169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5127] Loss: 0.5190041070767287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5128] Loss: 0.5190059162673107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5129] Loss: 0.5189883786379246\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5130] Loss: 0.5189797950126392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5131] Loss: 0.5190451961527447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5132] Loss: 0.5191271204625513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5133] Loss: 0.5190518772462837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5134] Loss: 0.5191110711493191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5135] Loss: 0.5192206008949796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5136] Loss: 0.5192080439103407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5137] Loss: 0.519177624460222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5138] Loss: 0.5192263736136019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5139] Loss: 0.519228808197148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5140] Loss: 0.5191926876618075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5141] Loss: 0.5191488467226067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5142] Loss: 0.5191432821704316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5143] Loss: 0.5191136425806898\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5144] Loss: 0.5192034598255034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5145] Loss: 0.5192800291019222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5146] Loss: 0.5192926056020932\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5147] Loss: 0.5192751182560162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5148] Loss: 0.5192321773526893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5149] Loss: 0.5191821482230887\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5150] Loss: 0.5191959601473706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5151] Loss: 0.5192377107959193\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5152] Loss: 0.5192819894040673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5153] Loss: 0.5193020157418117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5154] Loss: 0.5192502011945287\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5155] Loss: 0.5192287176302098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5156] Loss: 0.519263855667141\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5157] Loss: 0.5193932476173505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5158] Loss: 0.5193554239985557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5159] Loss: 0.519304589953159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5160] Loss: 0.519260748836302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5161] Loss: 0.5192269756893424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5162] Loss: 0.5192618000345914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5163] Loss: 0.5193566774171628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5164] Loss: 0.5192995464015427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5165] Loss: 0.5193168595001085\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5166] Loss: 0.51929637255896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5167] Loss: 0.5193050678052282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5168] Loss: 0.5192308782309153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5169] Loss: 0.5192973347923279\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5170] Loss: 0.5192551833269679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5171] Loss: 0.5193001876935456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5172] Loss: 0.5192452187109331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5173] Loss: 0.5192047794687628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5174] Loss: 0.5192300747428249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5175] Loss: 0.5193897779444023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5176] Loss: 0.5193255788048244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5177] Loss: 0.519279882522551\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5178] Loss: 0.5193718445631768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5179] Loss: 0.5193198787646468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5180] Loss: 0.5193061232854008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5181] Loss: 0.51932067059585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5182] Loss: 0.5194056263761212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5183] Loss: 0.5193754847309152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5184] Loss: 0.5193351471257046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5185] Loss: 0.5192695617006424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5186] Loss: 0.5192940941031057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5187] Loss: 0.5193810789681205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5188] Loss: 0.5194034490506585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5189] Loss: 0.5194840829904631\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5190] Loss: 0.5195684797966328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5191] Loss: 0.5195219956530996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5192] Loss: 0.5195514666141734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5193] Loss: 0.5195398372658648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5194] Loss: 0.5194748778923722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5195] Loss: 0.519435022737962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5196] Loss: 0.5196147334185157\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5197] Loss: 0.5196527357399782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5198] Loss: 0.5196241855195409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5199] Loss: 0.5196505761249509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5200] Loss: 0.5196040231948976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5201] Loss: 0.519718029499181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5202] Loss: 0.5196704826232773\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5203] Loss: 0.5196058276954196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5204] Loss: 0.5196095448563492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5205] Loss: 0.5196391413164849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5206] Loss: 0.5197251301157261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5207] Loss: 0.5198487634636733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5208] Loss: 0.519885612963336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5209] Loss: 0.5198433954117272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5210] Loss: 0.5197692222734716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5211] Loss: 0.519766872657205\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5212] Loss: 0.5198559385887971\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5213] Loss: 0.5199043018283577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5214] Loss: 0.5198772337705809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5215] Loss: 0.5198093714166777\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5216] Loss: 0.5197825352283321\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5217] Loss: 0.519742418239647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5218] Loss: 0.5197049747636587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5219] Loss: 0.5196867892192311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5220] Loss: 0.5197532106673388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5221] Loss: 0.5196913985139725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5222] Loss: 0.5196534897467985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5223] Loss: 0.5196338448584168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5224] Loss: 0.5196015919966873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5225] Loss: 0.5195550687302967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5226] Loss: 0.519593063245849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5227] Loss: 0.5195671000267353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5228] Loss: 0.5195853424279454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5229] Loss: 0.5196818264138524\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5230] Loss: 0.5196199587498933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5231] Loss: 0.5195742630177043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5232] Loss: 0.5195913111119642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5233] Loss: 0.5196563055454461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5234] Loss: 0.5196659554278669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5235] Loss: 0.5196636693345176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5236] Loss: 0.5196609585638577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5237] Loss: 0.5197662296199587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5238] Loss: 0.5196986245070214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5239] Loss: 0.5196601557073696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5240] Loss: 0.5196110828293302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5241] Loss: 0.5195480224711274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5242] Loss: 0.5195573013549827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5243] Loss: 0.5196583735534814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5244] Loss: 0.5196487076004497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5245] Loss: 0.5196581511839894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5246] Loss: 0.5196664627213534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5247] Loss: 0.519625648557322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5248] Loss: 0.5195845612220595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5249] Loss: 0.5195513194461089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5250] Loss: 0.5195359144595605\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5251] Loss: 0.5195505264451096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5252] Loss: 0.5195210669763176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5253] Loss: 0.5194922422787169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5254] Loss: 0.5195793670870703\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5255] Loss: 0.5196535220198938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5256] Loss: 0.5196070036604793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5257] Loss: 0.519587863852708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5258] Loss: 0.5195614495193044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5259] Loss: 0.5195198868997325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5260] Loss: 0.5196748036208906\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5261] Loss: 0.5196091669580857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5262] Loss: 0.5196009750794682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5263] Loss: 0.519647639787952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5264] Loss: 0.5196576566193652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5265] Loss: 0.5196639510860703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5266] Loss: 0.5196782707591534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5267] Loss: 0.5197257903134945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5268] Loss: 0.5196795424101182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5269] Loss: 0.519645115812678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5270] Loss: 0.5195992620990557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5271] Loss: 0.5195501400089344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5272] Loss: 0.5196045388485994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5273] Loss: 0.5195662482568704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5274] Loss: 0.5196590047927087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5275] Loss: 0.5195928498242609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5276] Loss: 0.5195986117965976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5277] Loss: 0.5195547496969904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5278] Loss: 0.5196086418933716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5279] Loss: 0.5196414791480328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5280] Loss: 0.519575655069866\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5281] Loss: 0.5195610823652025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5282] Loss: 0.5195306286446857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5283] Loss: 0.5194945280909813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5284] Loss: 0.5195618176409077\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5285] Loss: 0.519526914386225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5286] Loss: 0.5195178746447042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5287] Loss: 0.5194871991199007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5288] Loss: 0.519500529503488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5289] Loss: 0.5194887745596171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5290] Loss: 0.5194987041400317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5291] Loss: 0.5194729845928013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5292] Loss: 0.5195328590480766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5293] Loss: 0.5194737972816662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5294] Loss: 0.5194156267508915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5295] Loss: 0.5193625341014803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5296] Loss: 0.519332448184814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5297] Loss: 0.5194082265983974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5298] Loss: 0.5193732536600945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5299] Loss: 0.5192995116746258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5300] Loss: 0.5193062274372652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5301] Loss: 0.5193822452834642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5302] Loss: 0.5193846886246799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5303] Loss: 0.5193867410103513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5304] Loss: 0.5193561856348563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5305] Loss: 0.5193283976212035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5306] Loss: 0.5193450657108657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5307] Loss: 0.5192991421853649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5308] Loss: 0.5194005007921979\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5309] Loss: 0.5193822787693079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5310] Loss: 0.5194555434864014\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5311] Loss: 0.5195785246679272\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5312] Loss: 0.5195362584441218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5313] Loss: 0.5195146113907126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5314] Loss: 0.5195530696394391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5315] Loss: 0.5194980657466617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5316] Loss: 0.5194347640480095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5317] Loss: 0.5194818116143262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5318] Loss: 0.5196417413571363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5319] Loss: 0.5195858821653978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5320] Loss: 0.5195997940558406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5321] Loss: 0.5196626525619277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5322] Loss: 0.5196230240252894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5323] Loss: 0.5195946505708504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5324] Loss: 0.519544461544713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5325] Loss: 0.519488176480632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5326] Loss: 0.5194613249613548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5327] Loss: 0.5195731782822879\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5328] Loss: 0.5196478023395196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5329] Loss: 0.5196428945368072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5330] Loss: 0.5196863564125482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5331] Loss: 0.5198026128396086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5332] Loss: 0.5198083269409449\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5333] Loss: 0.519770636765289\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5334] Loss: 0.5197821990254754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5335] Loss: 0.5198297722629932\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5336] Loss: 0.5199133906091984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5337] Loss: 0.5199085537699999\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5338] Loss: 0.5199029827775942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5339] Loss: 0.5198717229847435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5340] Loss: 0.5198740421830991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5341] Loss: 0.5198337888370955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5342] Loss: 0.519816005457538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5343] Loss: 0.5198135757462322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5344] Loss: 0.5199573942493643\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5345] Loss: 0.519970713643648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5346] Loss: 0.520032656020843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5347] Loss: 0.5200178620580901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5348] Loss: 0.5199948075223975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5349] Loss: 0.5199884086243746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5350] Loss: 0.5200614599557208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5351] Loss: 0.5200067939953577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5352] Loss: 0.5199935976192755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5353] Loss: 0.5200649523554128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5354] Loss: 0.5201303741374014\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5355] Loss: 0.520186892705436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5356] Loss: 0.5201472847073274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5357] Loss: 0.5202281318185303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5358] Loss: 0.5202102818658495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5359] Loss: 0.5203357911258659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5360] Loss: 0.5204222962690382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5361] Loss: 0.5204096724601953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5362] Loss: 0.5203814530700617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5363] Loss: 0.5204127549652264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5364] Loss: 0.5205207188058449\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5365] Loss: 0.5205018429168098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5366] Loss: 0.5204586836323756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5367] Loss: 0.5204342471116213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5368] Loss: 0.5204107654547436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5369] Loss: 0.520506635240798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5370] Loss: 0.5205228247790915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5371] Loss: 0.5204856330274039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5372] Loss: 0.5205152095596225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5373] Loss: 0.5205144814820587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5374] Loss: 0.5205622241789426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5375] Loss: 0.5206491419277848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5376] Loss: 0.5207171244290303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5377] Loss: 0.5206764229231359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5378] Loss: 0.520778532477437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5379] Loss: 0.5207273141299461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5380] Loss: 0.520681026470857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5381] Loss: 0.5206390315962386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5382] Loss: 0.5206338082316327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5383] Loss: 0.520677419703109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5384] Loss: 0.5206638879074295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5385] Loss: 0.5207000647661013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5386] Loss: 0.5206473668222793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5387] Loss: 0.5206707135065917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5388] Loss: 0.5206180838146333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5389] Loss: 0.520591926268682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5390] Loss: 0.5205403876200807\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5391] Loss: 0.5205465734426273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5392] Loss: 0.5205252138483432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5393] Loss: 0.5205331305249802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5394] Loss: 0.5206036706600737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5395] Loss: 0.5206524102232717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5396] Loss: 0.5206102630671692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5397] Loss: 0.5207866609724651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5398] Loss: 0.5207672882276403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5399] Loss: 0.5207347575111071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5400] Loss: 0.5207545441261099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5401] Loss: 0.5207265124252007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5402] Loss: 0.520768070092405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5403] Loss: 0.5207638253183567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5404] Loss: 0.5208626127560917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5405] Loss: 0.5209209198514579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5406] Loss: 0.520887283063524\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5407] Loss: 0.5208419221157635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5408] Loss: 0.5207793402666946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5409] Loss: 0.5207824081054729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5410] Loss: 0.5208117592942958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5411] Loss: 0.5208036109479781\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5412] Loss: 0.5207654224128833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5413] Loss: 0.5207639185598911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5414] Loss: 0.5206985164306097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5415] Loss: 0.5206905004324413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5416] Loss: 0.5207784484587443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5417] Loss: 0.5207603784304227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5418] Loss: 0.5208197006410763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5419] Loss: 0.5208895923552724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5420] Loss: 0.5208782171219769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5421] Loss: 0.5209930625607054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5422] Loss: 0.5210051271846027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5423] Loss: 0.5209972946204164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5424] Loss: 0.5209579911775885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5425] Loss: 0.5210384797892275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5426] Loss: 0.5210177468043503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5427] Loss: 0.520992922030112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5428] Loss: 0.5209903386718813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5429] Loss: 0.5209696085612011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5430] Loss: 0.5209895732380995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5431] Loss: 0.5210067009361409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5432] Loss: 0.5209630094180985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5433] Loss: 0.5209033880598439\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5434] Loss: 0.5209602448063396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5435] Loss: 0.520917876256904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5436] Loss: 0.5209227840359792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5437] Loss: 0.5208834439352751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5438] Loss: 0.5208532413915283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5439] Loss: 0.5208429103933742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5440] Loss: 0.5208667902385488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5441] Loss: 0.5208162978862657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5442] Loss: 0.5208161535573895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5443] Loss: 0.5207512436992325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5444] Loss: 0.5207383940045438\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5445] Loss: 0.5207370039962587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5446] Loss: 0.5207326154147593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5447] Loss: 0.520694729816776\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5448] Loss: 0.5206966377333042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5449] Loss: 0.5206940645123088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5450] Loss: 0.5206962385158307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5451] Loss: 0.5207219020216795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5452] Loss: 0.5206854168532205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5453] Loss: 0.5206650659824802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5454] Loss: 0.5206473705307565\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5455] Loss: 0.5206960210424101\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5456] Loss: 0.5206329325041548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5457] Loss: 0.5205756308895326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5458] Loss: 0.5206631615896473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5459] Loss: 0.5206705527905142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5460] Loss: 0.5206743404630458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5461] Loss: 0.5206457279596972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5462] Loss: 0.5206423982983885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5463] Loss: 0.5206081646784325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5464] Loss: 0.5206779817064346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5465] Loss: 0.5206335199859621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5466] Loss: 0.5205801844242073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5467] Loss: 0.5206081616540625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5468] Loss: 0.5205418743695065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5469] Loss: 0.5205356797177505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5470] Loss: 0.5205308276228606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5471] Loss: 0.5206427015146404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5472] Loss: 0.5206535368863492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5473] Loss: 0.5206189441141668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5474] Loss: 0.5205907304527344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5475] Loss: 0.5205879402574816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5476] Loss: 0.5206092019001659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5477] Loss: 0.5206926660306597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5478] Loss: 0.5207788225851245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5479] Loss: 0.5208594694112862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5480] Loss: 0.5208497416955125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5481] Loss: 0.520787250237838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5482] Loss: 0.5209346833074658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5483] Loss: 0.5210211573053648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5484] Loss: 0.5209748893017552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5485] Loss: 0.5209515111350534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5486] Loss: 0.5210321920825066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5487] Loss: 0.5209828977403282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5488] Loss: 0.5209652514175604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5489] Loss: 0.5209892399640513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5490] Loss: 0.5210759371100181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5491] Loss: 0.521082592732072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5492] Loss: 0.5210859970018022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5493] Loss: 0.5210565057727627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5494] Loss: 0.5210606630347417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5495] Loss: 0.5210361525849808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5496] Loss: 0.5210407153160468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5497] Loss: 0.5210550175309301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5498] Loss: 0.5211023666388814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5499] Loss: 0.5211561893468201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5500] Loss: 0.5211973892722674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5501] Loss: 0.5211591325346489\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5502] Loss: 0.5211914125292634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5503] Loss: 0.521145048676047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5504] Loss: 0.5211393604660015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5505] Loss: 0.5211229815437748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5506] Loss: 0.521183346961144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5507] Loss: 0.5213590624369644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5508] Loss: 0.5214060965646692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5509] Loss: 0.5213597128035474\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5510] Loss: 0.5213411454439163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5511] Loss: 0.5212993341263616\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8284\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5512] Loss: 0.5213382222422692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5513] Loss: 0.5213826245220808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5514] Loss: 0.5214521734477233\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5515] Loss: 0.5214992464720072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5516] Loss: 0.5214624739245706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5517] Loss: 0.52157515962371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5518] Loss: 0.5215504118345511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5519] Loss: 0.5215276079005076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5520] Loss: 0.5215106937759175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5521] Loss: 0.5215489531117651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5522] Loss: 0.5215647732519381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5523] Loss: 0.5215424534602957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5524] Loss: 0.5214862733615743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5525] Loss: 0.5215358701802917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5526] Loss: 0.5215113717569498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5527] Loss: 0.5215300848361908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5528] Loss: 0.5216033646978673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5529] Loss: 0.5215933188234911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5530] Loss: 0.5215649415594173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5531] Loss: 0.5216408859173441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5532] Loss: 0.5216226269943882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5533] Loss: 0.5216021382568985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5534] Loss: 0.5216375616420607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5535] Loss: 0.5215942844170243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5536] Loss: 0.5215360333055696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5537] Loss: 0.5215869980077039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5538] Loss: 0.5215876196631858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5539] Loss: 0.5216030258797205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5540] Loss: 0.5215623904708365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5541] Loss: 0.5215565213873239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5542] Loss: 0.5215803031381723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5543] Loss: 0.5216484776422894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5544] Loss: 0.5216128535281584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5545] Loss: 0.5216400645991423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5546] Loss: 0.5216958568186586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5547] Loss: 0.5217547826062173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5548] Loss: 0.5217713029517593\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5549] Loss: 0.5217034800654958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5550] Loss: 0.5217510783364848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5551] Loss: 0.5217088547769319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5552] Loss: 0.521705351506893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5553] Loss: 0.5216611682683164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5554] Loss: 0.5217363538544013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5555] Loss: 0.521735218453809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5556] Loss: 0.5216817793170941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5557] Loss: 0.5216331343816629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5558] Loss: 0.5216186003030385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5559] Loss: 0.5215612000924664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5560] Loss: 0.5214985145820249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5561] Loss: 0.5216073298030645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5562] Loss: 0.5216015586489406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5563] Loss: 0.5215810542970796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5564] Loss: 0.5216082498419412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5565] Loss: 0.5215537883790853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5566] Loss: 0.5216165400151447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5567] Loss: 0.521599477842459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5568] Loss: 0.5216076809746468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5569] Loss: 0.521554852099281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5570] Loss: 0.5214835808652899\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5571] Loss: 0.5214355103649468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5572] Loss: 0.5214211873560025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5573] Loss: 0.5213933631192077\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5574] Loss: 0.52134147402058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5575] Loss: 0.5213098120153951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5576] Loss: 0.5213898611907615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5577] Loss: 0.5213409176392976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5578] Loss: 0.5213054945191108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5579] Loss: 0.5212823454827852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5580] Loss: 0.5213276125711097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5581] Loss: 0.5213132053907513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5582] Loss: 0.5214334200356708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5583] Loss: 0.5214073592457291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5584] Loss: 0.5213465000619134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5585] Loss: 0.521298872587716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5586] Loss: 0.5213405459127011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5587] Loss: 0.5213338716136486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5588] Loss: 0.5213423402508256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5589] Loss: 0.5214126455306208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5590] Loss: 0.5213656545712019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5591] Loss: 0.5214188706772789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5592] Loss: 0.5214197769444414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5593] Loss: 0.5214118020150598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5594] Loss: 0.5213679987336358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5595] Loss: 0.5213476100942026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5596] Loss: 0.5213004434895103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5597] Loss: 0.5212884180427934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5598] Loss: 0.5213157983362066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5599] Loss: 0.5214122127698385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5600] Loss: 0.5214317808304648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5601] Loss: 0.5214138110589943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5602] Loss: 0.5214224980247133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5603] Loss: 0.521406347443705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5604] Loss: 0.5213777558782262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5605] Loss: 0.5213119387421922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5606] Loss: 0.5212661028990239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5607] Loss: 0.521232778020459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5608] Loss: 0.5212179345592839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5609] Loss: 0.5211873479446119\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5610] Loss: 0.5212611083335736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5611] Loss: 0.5212162503966442\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5612] Loss: 0.5212125021662024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5613] Loss: 0.521266505951487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5614] Loss: 0.5212149456651374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5615] Loss: 0.5212030410153625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5616] Loss: 0.5212334156170622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5617] Loss: 0.5212348529374924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5618] Loss: 0.5211982535651959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5619] Loss: 0.5211661499052698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5620] Loss: 0.5211399614723461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5621] Loss: 0.5211184302396304\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5622] Loss: 0.5210833880607277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5623] Loss: 0.5210942007658091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5624] Loss: 0.5211888036873129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5625] Loss: 0.5211728014874901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5626] Loss: 0.5211816978807604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5627] Loss: 0.5211631966209141\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5628] Loss: 0.5211579375472699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5629] Loss: 0.5211457525800043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5630] Loss: 0.5211352693702793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5631] Loss: 0.5211530430426101\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5632] Loss: 0.5211157611731625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5633] Loss: 0.5211274837571477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5634] Loss: 0.5211117146892952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5635] Loss: 0.5211881379645045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5636] Loss: 0.5212529831904943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5637] Loss: 0.5213074739858935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5638] Loss: 0.5213088405909897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5639] Loss: 0.5212760763742185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5640] Loss: 0.5212937776369658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5641] Loss: 0.5212632706031258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5642] Loss: 0.5212681109778182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5643] Loss: 0.52126803928861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5644] Loss: 0.5212348365236871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5645] Loss: 0.5212192991431059\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5646] Loss: 0.5211974166571584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5647] Loss: 0.5212566207471422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5648] Loss: 0.5212228949308256\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5649] Loss: 0.521230615685598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5650] Loss: 0.521321835287698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5651] Loss: 0.5212642622322128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5652] Loss: 0.5212628180411065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5653] Loss: 0.5212501771001994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5654] Loss: 0.5213171286801018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5655] Loss: 0.5213029654108284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5656] Loss: 0.5212664526264306\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5657] Loss: 0.5212839786748598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5658] Loss: 0.5212396141156126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5659] Loss: 0.5212269657750712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5660] Loss: 0.5211869603135053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5661] Loss: 0.5212403003699894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5662] Loss: 0.5213031194300061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5663] Loss: 0.5212902527554808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5664] Loss: 0.5212523109277043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5665] Loss: 0.5213314857995128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5666] Loss: 0.5212961973938903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5667] Loss: 0.5212340629739958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5668] Loss: 0.5211680415539651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5669] Loss: 0.5211428289416687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5670] Loss: 0.5211472521283368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5671] Loss: 0.5211973628305322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5672] Loss: 0.5212271469812991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5673] Loss: 0.5211699251320019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5674] Loss: 0.5212338887599436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5675] Loss: 0.5212172140398884\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5676] Loss: 0.5212731679661349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5677] Loss: 0.5213606045593117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5678] Loss: 0.5214271645231789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5679] Loss: 0.5213803943685509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5680] Loss: 0.5213346214774037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5681] Loss: 0.5213684501903382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5682] Loss: 0.5214285797627509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5683] Loss: 0.5214791709602905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5684] Loss: 0.5214593011265239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5685] Loss: 0.5214631398346113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5686] Loss: 0.5214292785380226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5687] Loss: 0.5214234826036096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5688] Loss: 0.5213949901399672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5689] Loss: 0.521488363464543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5690] Loss: 0.5215861586127503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5691] Loss: 0.5215436137624515\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5692] Loss: 0.5214968351197952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5693] Loss: 0.5214732697507761\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5694] Loss: 0.5215016191100909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5695] Loss: 0.5214695486751104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5696] Loss: 0.521529922120322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5697] Loss: 0.5215790047221774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5698] Loss: 0.5215528886549824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5699] Loss: 0.5215199124286988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5700] Loss: 0.5216015442028671\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5701] Loss: 0.5216189776282621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5702] Loss: 0.5215728034728848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5703] Loss: 0.52151148678043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5704] Loss: 0.5214832198358197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5705] Loss: 0.5214628099706795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5706] Loss: 0.5215247380586658\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5707] Loss: 0.5215731934902873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5708] Loss: 0.5215195283242123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5709] Loss: 0.5215046633594562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5710] Loss: 0.5215337886603979\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5711] Loss: 0.5215240670176474\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5712] Loss: 0.5214657799205795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5713] Loss: 0.5214353394856619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5714] Loss: 0.521383312220852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5715] Loss: 0.5214149412916442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5716] Loss: 0.5213501839610507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5717] Loss: 0.5213973878331759\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5718] Loss: 0.5213749917701871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5719] Loss: 0.5213767995810779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5720] Loss: 0.5213365525665072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5721] Loss: 0.5212998618762293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5722] Loss: 0.5213112425736666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5723] Loss: 0.5212674101511988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5724] Loss: 0.5212173928811398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5725] Loss: 0.521167021329771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5726] Loss: 0.5211033918866259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5727] Loss: 0.5210722810021986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5728] Loss: 0.521078579102087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5729] Loss: 0.5210444229068051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5730] Loss: 0.5210248404764124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5731] Loss: 0.5210415427654188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5732] Loss: 0.5211199308802277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5733] Loss: 0.5210686262918801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5734] Loss: 0.5210871850732275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5735] Loss: 0.5211280487827137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5736] Loss: 0.5212526854955963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5737] Loss: 0.5212135094517113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5738] Loss: 0.5212054640601953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5739] Loss: 0.5212294498213073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5740] Loss: 0.5213230345490786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5741] Loss: 0.5212701507278756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5742] Loss: 0.5212329808987153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5743] Loss: 0.5212270807415765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5744] Loss: 0.5211859013818355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5745] Loss: 0.5212043205213865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5746] Loss: 0.521214494776234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5747] Loss: 0.5212877387874837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5748] Loss: 0.521412749675541\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5749] Loss: 0.5213713004550854\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5750] Loss: 0.5213708663316629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5751] Loss: 0.5213697213033437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5752] Loss: 0.5214609636725109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5753] Loss: 0.5214481076824967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5754] Loss: 0.5214369343231418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5755] Loss: 0.5214840311510888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5756] Loss: 0.5214778826100837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5757] Loss: 0.5214331981873135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5758] Loss: 0.5214551883329432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5759] Loss: 0.5214262349443405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5760] Loss: 0.5214721696830931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5761] Loss: 0.5214527014027865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5762] Loss: 0.5215392223951568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5763] Loss: 0.5215409699903314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5764] Loss: 0.5216189734850449\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5765] Loss: 0.5215713100133909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5766] Loss: 0.5216182441744086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5767] Loss: 0.5215759997996445\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5768] Loss: 0.5215610676378114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5769] Loss: 0.5216710063175322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5770] Loss: 0.5217257796528222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5771] Loss: 0.5217422319272017\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5772] Loss: 0.5218104950344549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5773] Loss: 0.5217597006267861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5774] Loss: 0.5217896634514662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5775] Loss: 0.5217845906898846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5776] Loss: 0.5217345894073471\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5777] Loss: 0.5217218382129503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5778] Loss: 0.5217177301996028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5779] Loss: 0.5216865062566562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5780] Loss: 0.521691496316803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5781] Loss: 0.5216609915855193\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5782] Loss: 0.5216326047475583\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5783] Loss: 0.521609922689074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5784] Loss: 0.5215549493970679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5785] Loss: 0.521497418329614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5786] Loss: 0.5214862496739676\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5787] Loss: 0.5214395317131989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5788] Loss: 0.5214547833802574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5789] Loss: 0.5213952388647584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5790] Loss: 0.5214091060598466\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5791] Loss: 0.5214810922270674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5792] Loss: 0.5215926567421904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5793] Loss: 0.5215716281584576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5794] Loss: 0.5215244813992118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5795] Loss: 0.521585930857099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5796] Loss: 0.5215658131582332\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5797] Loss: 0.5215879617086628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5798] Loss: 0.5215638715818908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5799] Loss: 0.5215159783005647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5800] Loss: 0.5215727391591144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5801] Loss: 0.5215389942614799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5802] Loss: 0.521528842754082\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5803] Loss: 0.521597496482503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5804] Loss: 0.5215491869138195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5805] Loss: 0.5216156835174426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5806] Loss: 0.5215752384346822\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5807] Loss: 0.5215612412629903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5808] Loss: 0.5216306085297863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5809] Loss: 0.5217080873759383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5810] Loss: 0.5217291992930871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5811] Loss: 0.5216999357234889\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5812] Loss: 0.521773658254402\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5813] Loss: 0.5217377158458247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5814] Loss: 0.5218036547387482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5815] Loss: 0.5217895888669889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5816] Loss: 0.5217757036736045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5817] Loss: 0.5217900385773742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5818] Loss: 0.5217415549467405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5819] Loss: 0.5218313127857163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5820] Loss: 0.5218861789100587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5821] Loss: 0.521965599140302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5822] Loss: 0.5220200672874743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5823] Loss: 0.522094559996518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5824] Loss: 0.522087162994984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5825] Loss: 0.5220595022444442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5826] Loss: 0.5220567736833548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5827] Loss: 0.5220458943250373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5828] Loss: 0.5220067735289263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5829] Loss: 0.5219817006441184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5830] Loss: 0.5220209156408122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5831] Loss: 0.522111276442945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5832] Loss: 0.522102284315541\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5833] Loss: 0.5221245028085555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5834] Loss: 0.5221790025958121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5835] Loss: 0.5221176925287561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5836] Loss: 0.5220775795170923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5837] Loss: 0.5220853906311549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5838] Loss: 0.5222347050390623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5839] Loss: 0.5223038376306006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5840] Loss: 0.5223202238163402\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5841] Loss: 0.5223114848237529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5842] Loss: 0.5223632038075586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5843] Loss: 0.5223672626647988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5844] Loss: 0.5223444728680126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5845] Loss: 0.5223103730446172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5846] Loss: 0.5222577578381173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5847] Loss: 0.5222119462105896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5848] Loss: 0.5221553624748245\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5849] Loss: 0.5222024640065271\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5850] Loss: 0.5222203345669343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5851] Loss: 0.5222206192167482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5852] Loss: 0.5221673710678276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5853] Loss: 0.5222397914054174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5854] Loss: 0.522254481610513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5855] Loss: 0.5222453333430473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5856] Loss: 0.522232853642707\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5857] Loss: 0.5222170378351648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5858] Loss: 0.5221698535808272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5859] Loss: 0.5222177557197324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5860] Loss: 0.5222051715405188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5861] Loss: 0.5221855747623992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5862] Loss: 0.5221543993372109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5863] Loss: 0.5221848955460403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5864] Loss: 0.5221978736081851\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5865] Loss: 0.5221536664902663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5866] Loss: 0.522115502225498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5867] Loss: 0.5220590433551234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5868] Loss: 0.5220904477571943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5869] Loss: 0.5220588779639656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5870] Loss: 0.5220324845317362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5871] Loss: 0.5220930499914666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5872] Loss: 0.5221517599454926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5873] Loss: 0.5221015509470127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5874] Loss: 0.5220867320422936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5875] Loss: 0.5221361141308156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5876] Loss: 0.5221095295665293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5877] Loss: 0.5221618420283222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5878] Loss: 0.5222508641634408\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5879] Loss: 0.5222237103282684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5880] Loss: 0.5223911913663315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5881] Loss: 0.5225757833461347\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5882] Loss: 0.522529311691725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5883] Loss: 0.5224980117859575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5884] Loss: 0.5224464179737931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5885] Loss: 0.5224615504187207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5886] Loss: 0.5224642156944832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5887] Loss: 0.5225126844689996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5888] Loss: 0.5225442808028622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5889] Loss: 0.5225259412906276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5890] Loss: 0.5225019843826507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5891] Loss: 0.5225062908968547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5892] Loss: 0.5224772983326517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5893] Loss: 0.5224518437445795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5894] Loss: 0.5224633114453942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5895] Loss: 0.5225180407752565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5896] Loss: 0.5225001046682352\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5897] Loss: 0.522552543067525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5898] Loss: 0.5225876165967388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5899] Loss: 0.522593763348432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5900] Loss: 0.5225581242003113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5901] Loss: 0.5226080499118168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5902] Loss: 0.5226481730776124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5903] Loss: 0.5226100923487633\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5904] Loss: 0.5225962106170237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5905] Loss: 0.5225789046928327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5906] Loss: 0.5225281274046961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5907] Loss: 0.5224906611170883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5908] Loss: 0.5226373832318111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5909] Loss: 0.5226199516688791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5910] Loss: 0.5225919247666995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5911] Loss: 0.5226002615417645\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5912] Loss: 0.5226523550803112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5913] Loss: 0.5226178671968175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5914] Loss: 0.522559971196355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5915] Loss: 0.522510648959219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5916] Loss: 0.5225399951393676\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5917] Loss: 0.5225627591930962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5918] Loss: 0.5225348745544369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5919] Loss: 0.5226640627077402\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5920] Loss: 0.5227676310127192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5921] Loss: 0.5228005473670139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5922] Loss: 0.5227650717693791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5923] Loss: 0.5227676621926771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5924] Loss: 0.522740218509023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5925] Loss: 0.5226981522413943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5926] Loss: 0.522630957300545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5927] Loss: 0.5226287956615865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5928] Loss: 0.522625918794752\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5929] Loss: 0.5226195463561818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5930] Loss: 0.522670773589963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5931] Loss: 0.522699837902555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5932] Loss: 0.5227491789717641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5933] Loss: 0.5227109249581627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5934] Loss: 0.5227321230017945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5935] Loss: 0.5227750520936905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5936] Loss: 0.5227693164926414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5937] Loss: 0.52279275226861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5938] Loss: 0.5227845489111295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5939] Loss: 0.5227336921641831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5940] Loss: 0.5226827786442984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5941] Loss: 0.5227626684817835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5942] Loss: 0.522747458947688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5943] Loss: 0.5227677961442871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5944] Loss: 0.5227119022507987\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5945] Loss: 0.522712573052659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5946] Loss: 0.522780492295997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5947] Loss: 0.5227354430929799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5948] Loss: 0.5227969243256605\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5949] Loss: 0.5228635311417116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5950] Loss: 0.5229289827567032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5951] Loss: 0.5228846405325206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5952] Loss: 0.5228380782475632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5953] Loss: 0.5228013212209279\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5954] Loss: 0.5227678486561399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5955] Loss: 0.5227591748271545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5956] Loss: 0.5227456549692145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5957] Loss: 0.5228022865517459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5958] Loss: 0.5227622043658677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5959] Loss: 0.5228001180346415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5960] Loss: 0.5227677234679187\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5961] Loss: 0.5227562682226456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5962] Loss: 0.5227071627651998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5963] Loss: 0.5227072791437392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5964] Loss: 0.522669756479717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5965] Loss: 0.5227284507697925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5966] Loss: 0.5226657124826155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5967] Loss: 0.522615091328724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5968] Loss: 0.5226050395997682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5969] Loss: 0.5225732823003668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5970] Loss: 0.5225418529258324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5971] Loss: 0.5225140296879148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5972] Loss: 0.5224647722900984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5973] Loss: 0.5224347481786735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5974] Loss: 0.5225099170795952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5975] Loss: 0.5225170977679503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5976] Loss: 0.5225042259866529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5977] Loss: 0.522465185006903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5978] Loss: 0.5224807216283738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5979] Loss: 0.5225038804804566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5980] Loss: 0.5224916435383138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5981] Loss: 0.5225209048849598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5982] Loss: 0.5225151788078423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5983] Loss: 0.522478697987921\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5984] Loss: 0.5224911597797938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5985] Loss: 0.5225434755108672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5986] Loss: 0.5225207082589801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5987] Loss: 0.5225916212142026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5988] Loss: 0.5225525942677649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5989] Loss: 0.5225185773101334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5990] Loss: 0.522615186138636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5991] Loss: 0.5226082287741021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5992] Loss: 0.5226336951887073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5993] Loss: 0.5225922104277664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5994] Loss: 0.5225982180260594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5995] Loss: 0.5226086415452313\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5996] Loss: 0.5225983076245734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5997] Loss: 0.5225528094918215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5998] Loss: 0.5226386531117663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 5999] Loss: 0.5226985399195092\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6000] Loss: 0.5226788461235485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6001] Loss: 0.522728092608893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6002] Loss: 0.5226879387209429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6003] Loss: 0.5226432200953237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6004] Loss: 0.5225943039803015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6005] Loss: 0.5225719884946414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6006] Loss: 0.5225874766452561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6007] Loss: 0.5226226893933184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6008] Loss: 0.5226317461147747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6009] Loss: 0.5226118848619298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6010] Loss: 0.5226545176749879\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6011] Loss: 0.5226724190731262\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8508\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6012] Loss: 0.5226501818002895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6013] Loss: 0.5226045726576264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6014] Loss: 0.522683373866278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6015] Loss: 0.5226468191674143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6016] Loss: 0.522643778761365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6017] Loss: 0.5226156050131329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6018] Loss: 0.5226256576493199\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6019] Loss: 0.522599604435875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6020] Loss: 0.5225712708817852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6021] Loss: 0.5225285445820764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6022] Loss: 0.522512885481011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6023] Loss: 0.5224784130050465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6024] Loss: 0.5224557024010247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6025] Loss: 0.5224841362248738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6026] Loss: 0.5224517502020328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6027] Loss: 0.5224328511825869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6028] Loss: 0.5224182905521934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6029] Loss: 0.5223676216766484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6030] Loss: 0.5224044144315564\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6031] Loss: 0.5224185537339725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6032] Loss: 0.522437320260499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6033] Loss: 0.5224470525046481\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6034] Loss: 0.5224677966577173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6035] Loss: 0.522514405169638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6036] Loss: 0.5224697632048383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6037] Loss: 0.5224559776699303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6038] Loss: 0.5225133285864476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6039] Loss: 0.5224602961328927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6040] Loss: 0.5224191284416813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6041] Loss: 0.5224637782200834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6042] Loss: 0.5224219456325815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6043] Loss: 0.5224744379251102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6044] Loss: 0.5225373642148203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6045] Loss: 0.5225407512853463\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6046] Loss: 0.5225232845075385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6047] Loss: 0.5226691476549937\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6048] Loss: 0.5226597661477386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6049] Loss: 0.5226132284058284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6050] Loss: 0.5225712582576576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6051] Loss: 0.5225880783111089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6052] Loss: 0.5225509376473825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6053] Loss: 0.5225982452297004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6054] Loss: 0.5226026277876261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6055] Loss: 0.522543975498571\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6056] Loss: 0.5225164281131127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6057] Loss: 0.5224704651633146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6058] Loss: 0.5224889870827027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6059] Loss: 0.5225215723308749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6060] Loss: 0.5225485519138542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6061] Loss: 0.522509752716208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6062] Loss: 0.522492708434332\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6063] Loss: 0.5224505280098055\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6064] Loss: 0.5224024383475294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6065] Loss: 0.5223965352821951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6066] Loss: 0.5223473942831234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6067] Loss: 0.5223286120190517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6068] Loss: 0.5223103205627484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6069] Loss: 0.5223477239509263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6070] Loss: 0.5223710980989951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6071] Loss: 0.5223555080733311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6072] Loss: 0.5223084859541962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6073] Loss: 0.5222660411439224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6074] Loss: 0.5222815527365758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6075] Loss: 0.5222699272075646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6076] Loss: 0.5223071606166492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6077] Loss: 0.5222883652651441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6078] Loss: 0.522266096569298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6079] Loss: 0.5222552138077847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6080] Loss: 0.5222596407685717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6081] Loss: 0.5222554177586126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6082] Loss: 0.5222724358479667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6083] Loss: 0.5223333578541401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6084] Loss: 0.5223737683963895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6085] Loss: 0.5223703238408127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6086] Loss: 0.5224604031800625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6087] Loss: 0.5224339116457566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6088] Loss: 0.5223941871687348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6089] Loss: 0.5223812780557172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6090] Loss: 0.5223862817103717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6091] Loss: 0.5223451588577191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6092] Loss: 0.5223195522446993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6093] Loss: 0.5223251336335751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6094] Loss: 0.5223676240856207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6095] Loss: 0.5223575862262968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6096] Loss: 0.5223362629308181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6097] Loss: 0.522328798787334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6098] Loss: 0.5222847131197591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6099] Loss: 0.5222747224209923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6100] Loss: 0.5222151757166296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6101] Loss: 0.5221792531741623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6102] Loss: 0.5221834818943163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6103] Loss: 0.5221472201029985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6104] Loss: 0.5221454885881162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6105] Loss: 0.5222099850575129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6106] Loss: 0.5222628664803173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6107] Loss: 0.522321689045278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6108] Loss: 0.5223258236591498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6109] Loss: 0.5223963698051214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6110] Loss: 0.522458464990237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6111] Loss: 0.5224553656496123\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6112] Loss: 0.5225831283384755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6113] Loss: 0.5226088552262854\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6114] Loss: 0.5225778972220838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6115] Loss: 0.5225459194816084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6116] Loss: 0.5225627086644635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6117] Loss: 0.5225572219815764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6118] Loss: 0.5226274221768861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6119] Loss: 0.5225890264231203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6120] Loss: 0.5226222344765477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6121] Loss: 0.5226550856321344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6122] Loss: 0.5227084401769249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6123] Loss: 0.5226669912959045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6124] Loss: 0.5226765709959361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6125] Loss: 0.5226937312768807\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6126] Loss: 0.5226539415824745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6127] Loss: 0.5226321574423223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6128] Loss: 0.5227258792786931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6129] Loss: 0.5226636161916004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6130] Loss: 0.5226979252149838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6131] Loss: 0.5226834837020535\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6132] Loss: 0.5226446196903372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6133] Loss: 0.5226054668516615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6134] Loss: 0.5226165892352294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6135] Loss: 0.5226529073847664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6136] Loss: 0.5226322081545219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6137] Loss: 0.5226884256682262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6138] Loss: 0.5226914873790174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6139] Loss: 0.5226407131376392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6140] Loss: 0.5226922194154182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6141] Loss: 0.5227188158391995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6142] Loss: 0.5226837063912005\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6143] Loss: 0.5226809189567425\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6144] Loss: 0.5226742434726991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6145] Loss: 0.5226597199555298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6146] Loss: 0.5226503395421908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6147] Loss: 0.5226166509045593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6148] Loss: 0.522567066765394\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6149] Loss: 0.522607969593486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6150] Loss: 0.5226448735261852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6151] Loss: 0.522624928498285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6152] Loss: 0.5225942250458706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6153] Loss: 0.5226031555053974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6154] Loss: 0.5225758331436413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6155] Loss: 0.5226324033087814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6156] Loss: 0.5226763019953853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6157] Loss: 0.5226772182073681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6158] Loss: 0.5226191637625986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6159] Loss: 0.5226076331011361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6160] Loss: 0.5225920826434034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6161] Loss: 0.5225453090586508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6162] Loss: 0.5225094820057047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6163] Loss: 0.5224727667471424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6164] Loss: 0.5224334694706049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6165] Loss: 0.5224936607899021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6166] Loss: 0.5224882282838331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6167] Loss: 0.5224785655052054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6168] Loss: 0.5225003611608502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6169] Loss: 0.5224913399634291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6170] Loss: 0.5224541134447808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6171] Loss: 0.5224576218902928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6172] Loss: 0.5224556195092723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6173] Loss: 0.5224617299928527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6174] Loss: 0.5224284858569124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6175] Loss: 0.5224026053933475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6176] Loss: 0.5224572802590389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6177] Loss: 0.522482179427391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6178] Loss: 0.5225593409906539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6179] Loss: 0.5225367556043689\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6180] Loss: 0.5226121199272927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6181] Loss: 0.5225769156654809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6182] Loss: 0.5225588434174846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6183] Loss: 0.522537559746288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6184] Loss: 0.5225137866479349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6185] Loss: 0.5224693947400291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6186] Loss: 0.5224753058606718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6187] Loss: 0.5225195437512264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6188] Loss: 0.5225204751052962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6189] Loss: 0.5224819440046317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6190] Loss: 0.5225064880804906\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6191] Loss: 0.5225601953675137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6192] Loss: 0.5225658893600941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6193] Loss: 0.5225668716903683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6194] Loss: 0.5225496631742237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6195] Loss: 0.5225380567022867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6196] Loss: 0.5224955541317915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6197] Loss: 0.522462481385546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6198] Loss: 0.5224305142775576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6199] Loss: 0.5224029657464179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6200] Loss: 0.5223528367799489\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6201] Loss: 0.522315979289309\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6202] Loss: 0.522402726875609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6203] Loss: 0.5224367145016389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6204] Loss: 0.5224971859197387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6205] Loss: 0.5225584481686641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6206] Loss: 0.5225448530884146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6207] Loss: 0.522573473938666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6208] Loss: 0.5226186603156044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6209] Loss: 0.5226895514889378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6210] Loss: 0.5227371868987878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6211] Loss: 0.5227538983359962\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6212] Loss: 0.5228507602629808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6213] Loss: 0.5228065284886486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6214] Loss: 0.5228263533022385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6215] Loss: 0.5228857439380273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6216] Loss: 0.5228452458142022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6217] Loss: 0.5227937974950907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6218] Loss: 0.5227860485000592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6219] Loss: 0.5228436545521559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6220] Loss: 0.5228635669720152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6221] Loss: 0.5229257931565888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6222] Loss: 0.5229049979028206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6223] Loss: 0.5228864614415373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6224] Loss: 0.5228455829289688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6225] Loss: 0.5228265497810676\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6226] Loss: 0.5228129137747975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6227] Loss: 0.5228077395382056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6228] Loss: 0.5227610825861655\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6229] Loss: 0.5227947626956632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6230] Loss: 0.522742670326383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6231] Loss: 0.5228135023807191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6232] Loss: 0.5228117008552564\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6233] Loss: 0.5227892778145982\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6234] Loss: 0.5228208665619523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6235] Loss: 0.5228583833119754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6236] Loss: 0.5228258098364709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6237] Loss: 0.5228727087053263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6238] Loss: 0.5228654659462458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6239] Loss: 0.522840843012995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6240] Loss: 0.522844022616458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6241] Loss: 0.522890540800925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6242] Loss: 0.5229261299466554\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6243] Loss: 0.5229681676103248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6244] Loss: 0.5229126059420649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6245] Loss: 0.5229016485584229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6246] Loss: 0.522896950916391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6247] Loss: 0.522943662162315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6248] Loss: 0.5229793428055456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6249] Loss: 0.5229596939988825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6250] Loss: 0.5230592619822416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6251] Loss: 0.5230558477063105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6252] Loss: 0.5230359752983301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6253] Loss: 0.5229868902606705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6254] Loss: 0.5229842795479347\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6255] Loss: 0.5230240005581559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6256] Loss: 0.523068670353282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6257] Loss: 0.5231403462084476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6258] Loss: 0.5231089960101917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6259] Loss: 0.5230626670987446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6260] Loss: 0.5232161820504976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6261] Loss: 0.5232226470670914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6262] Loss: 0.5232584853300382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6263] Loss: 0.5233221264035105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6264] Loss: 0.5233200350993202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6265] Loss: 0.5233228476900934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6266] Loss: 0.5232761873138125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6267] Loss: 0.5233493113975383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6268] Loss: 0.5233040156205854\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6269] Loss: 0.5232838899657883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6270] Loss: 0.5232649718049086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6271] Loss: 0.5232804681904675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6272] Loss: 0.5232395877164919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6273] Loss: 0.5232240071101009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6274] Loss: 0.5232075352084711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6275] Loss: 0.5231732304263714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6276] Loss: 0.5231272656647563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6277] Loss: 0.5230959586673827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6278] Loss: 0.523052462390863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6279] Loss: 0.5229974457369992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6280] Loss: 0.5229516285079397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6281] Loss: 0.5229292386709311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6282] Loss: 0.5228994721733937\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6283] Loss: 0.52290627244707\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6284] Loss: 0.5230492335838721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6285] Loss: 0.5230710510412852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6286] Loss: 0.5231040089940678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6287] Loss: 0.5231070888739918\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6288] Loss: 0.523100382091678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6289] Loss: 0.5232345644712737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6290] Loss: 0.5231931379509632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6291] Loss: 0.5232385525322356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6292] Loss: 0.5233879211719207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6293] Loss: 0.5234797901190339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6294] Loss: 0.5234845294971354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6295] Loss: 0.5235028250936609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6296] Loss: 0.5234810426970316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6297] Loss: 0.5235252518696717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6298] Loss: 0.5235926365658754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6299] Loss: 0.5235593950198627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6300] Loss: 0.5235275709824653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6301] Loss: 0.5234992475969519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6302] Loss: 0.5235296772093158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6303] Loss: 0.5234909081024847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6304] Loss: 0.5234478246581361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6305] Loss: 0.5234317572699013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6306] Loss: 0.5234840399154389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6307] Loss: 0.5235048056738694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6308] Loss: 0.5235311194365088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6309] Loss: 0.5234888927661914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6310] Loss: 0.5235462890565395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6311] Loss: 0.5235069256633587\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6312] Loss: 0.52347972555043\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6313] Loss: 0.5234528142697602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6314] Loss: 0.5234363510547466\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6315] Loss: 0.5234437222474613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6316] Loss: 0.5235177525878979\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6317] Loss: 0.5235192855531057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6318] Loss: 0.5235211383448735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6319] Loss: 0.5235158051480359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6320] Loss: 0.5234905835632203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6321] Loss: 0.5235106840773307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6322] Loss: 0.5235032978104216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6323] Loss: 0.5234672075322383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6324] Loss: 0.5235316787372556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6325] Loss: 0.5234836538239788\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6326] Loss: 0.5234827629483959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6327] Loss: 0.5234707023949071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6328] Loss: 0.5235288931961607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6329] Loss: 0.5235494078946986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6330] Loss: 0.5235321519659557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6331] Loss: 0.523565094012449\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6332] Loss: 0.5235328199537052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6333] Loss: 0.5235169646554891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6334] Loss: 0.5235674282955517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6335] Loss: 0.5235514574245322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6336] Loss: 0.5235358131195809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6337] Loss: 0.5235089933778115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6338] Loss: 0.5234814873022745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6339] Loss: 0.5235316569787967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6340] Loss: 0.5235689725362621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6341] Loss: 0.5236080431330983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6342] Loss: 0.5235817716118403\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6343] Loss: 0.5236551715950399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6344] Loss: 0.5236861508800675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6345] Loss: 0.5236801166818401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6346] Loss: 0.5236728222883671\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6347] Loss: 0.5236235955448945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6348] Loss: 0.523593845076249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6349] Loss: 0.5236405777000241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6350] Loss: 0.5236280554828987\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6351] Loss: 0.5236331070608277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6352] Loss: 0.523671745361224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6353] Loss: 0.52363441444088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6354] Loss: 0.5236792156369534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6355] Loss: 0.5236422297636396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6356] Loss: 0.5236500754335585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6357] Loss: 0.5236810668168852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6358] Loss: 0.5237488907630646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6359] Loss: 0.5237275040960858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6360] Loss: 0.5237057873632154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6361] Loss: 0.5237335177837855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6362] Loss: 0.5236954370020842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6363] Loss: 0.5237423357404686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6364] Loss: 0.5237819701993185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6365] Loss: 0.5238256604421048\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6366] Loss: 0.5237860971186412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6367] Loss: 0.5238437975442143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6368] Loss: 0.5238848273493235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6369] Loss: 0.5238653153287894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6370] Loss: 0.5238485823566597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6371] Loss: 0.52393412187815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6372] Loss: 0.5239046698684621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6373] Loss: 0.5238928054453833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6374] Loss: 0.5239026766641404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6375] Loss: 0.5238785290707916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6376] Loss: 0.5238333252349731\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6377] Loss: 0.5239353123801478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6378] Loss: 0.5239659062832411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6379] Loss: 0.5239826774887625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6380] Loss: 0.5240024598381995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6381] Loss: 0.5239707690155541\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6382] Loss: 0.5239516443925674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6383] Loss: 0.5238950484486672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6384] Loss: 0.5238674754263554\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6385] Loss: 0.5238449052901978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6386] Loss: 0.5238738681656963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6387] Loss: 0.5239033648883361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6388] Loss: 0.5239408909818193\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6389] Loss: 0.5239823743901217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6390] Loss: 0.5239495718205461\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6391] Loss: 0.523909702676633\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6392] Loss: 0.5239110480448049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6393] Loss: 0.5238973950521082\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6394] Loss: 0.5239127325330925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6395] Loss: 0.5238821718079009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6396] Loss: 0.5238396956164781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6397] Loss: 0.523788662399848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6398] Loss: 0.5237377582715176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6399] Loss: 0.523692279310173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6400] Loss: 0.5236309837050268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6401] Loss: 0.5235998616915084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6402] Loss: 0.5236333256735336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6403] Loss: 0.5236113964702164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6404] Loss: 0.5236102575654291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6405] Loss: 0.5235555911438257\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6406] Loss: 0.5235098965334601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6407] Loss: 0.5235104370121231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6408] Loss: 0.5234723943558819\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6409] Loss: 0.5235186136765164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6410] Loss: 0.5234819859973455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6411] Loss: 0.5234620364070686\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6412] Loss: 0.5235334808989648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6413] Loss: 0.5235634674344167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6414] Loss: 0.5235310659761513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6415] Loss: 0.5234800270409224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6416] Loss: 0.5234312785885877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6417] Loss: 0.5234648235052678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6418] Loss: 0.5234813467197791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6419] Loss: 0.5234725244644632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6420] Loss: 0.5235039931994606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6421] Loss: 0.5234952368871993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6422] Loss: 0.5235147924062888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6423] Loss: 0.5235290321529809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6424] Loss: 0.5235692993370834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6425] Loss: 0.5235314829789877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6426] Loss: 0.52350254467436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6427] Loss: 0.5234831613963049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6428] Loss: 0.5235054752071511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6429] Loss: 0.5234602789008591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6430] Loss: 0.5234150654636324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6431] Loss: 0.523468555587809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6432] Loss: 0.5234107523703647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6433] Loss: 0.5233901027900111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6434] Loss: 0.5233831718805511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6435] Loss: 0.5234376234092792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6436] Loss: 0.5233948667619505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6437] Loss: 0.5233729487278418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6438] Loss: 0.5234185360754148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6439] Loss: 0.523414178463797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6440] Loss: 0.5234015116100569\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6441] Loss: 0.5234290291961139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6442] Loss: 0.5234146523148082\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6443] Loss: 0.5234772572173008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6444] Loss: 0.5234539753957995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6445] Loss: 0.5234264272998739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6446] Loss: 0.5233984067232782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6447] Loss: 0.5234728492990937\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6448] Loss: 0.5234650773014754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6449] Loss: 0.5234461147579647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6450] Loss: 0.5233928218534097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6451] Loss: 0.5233414739275035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6452] Loss: 0.5233871048546447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6453] Loss: 0.5233880425470356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6454] Loss: 0.5234025735232583\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6455] Loss: 0.5233708972134843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6456] Loss: 0.5234085165297508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6457] Loss: 0.5233959155772662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6458] Loss: 0.5233451028347577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6459] Loss: 0.5233128943559923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6460] Loss: 0.5233737934988086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6461] Loss: 0.5233244113793114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6462] Loss: 0.5233121013308886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6463] Loss: 0.5232999190530436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6464] Loss: 0.5233466293754135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6465] Loss: 0.5233277801802737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6466] Loss: 0.523277509302061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6467] Loss: 0.523241979717728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6468] Loss: 0.5232212609739942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6469] Loss: 0.523243103439545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6470] Loss: 0.5232994671560974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6471] Loss: 0.5232547581110839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6472] Loss: 0.5232050820575269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6473] Loss: 0.5231742225081333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6474] Loss: 0.5232407729835378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6475] Loss: 0.5231961632439397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6476] Loss: 0.5231878219101799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6477] Loss: 0.5231722635586138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6478] Loss: 0.5231278792857604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6479] Loss: 0.5230843970989897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6480] Loss: 0.5230607422377596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6481] Loss: 0.523048522381642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6482] Loss: 0.5230045736320093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6483] Loss: 0.5229780191769856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6484] Loss: 0.5230201800725793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6485] Loss: 0.5230111297079709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6486] Loss: 0.5230127003871732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6487] Loss: 0.5229554513021518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6488] Loss: 0.5229550155385078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6489] Loss: 0.5229181952377011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6490] Loss: 0.5229693999682182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6491] Loss: 0.5229557464078325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6492] Loss: 0.522900215982494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6493] Loss: 0.5228655583784171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6494] Loss: 0.5229148257967025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6495] Loss: 0.5229898301307221\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6496] Loss: 0.5229445823256363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6497] Loss: 0.5229900240405326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6498] Loss: 0.5229507756849369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6499] Loss: 0.52294443585047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6500] Loss: 0.5229019057198439\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6501] Loss: 0.5229307818449592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6502] Loss: 0.5229170423160209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6503] Loss: 0.5228642750921183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6504] Loss: 0.5228097753243165\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6505] Loss: 0.5227762893699824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6506] Loss: 0.5227528039298907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6507] Loss: 0.5227527280976539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6508] Loss: 0.5228115253923098\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6509] Loss: 0.5228189339755396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6510] Loss: 0.5228720555901527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6511] Loss: 0.522833418978232\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8558999999999999\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6512] Loss: 0.522825389251396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6513] Loss: 0.5228016759313465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6514] Loss: 0.5228638238534778\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6515] Loss: 0.5228320597361565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6516] Loss: 0.5228293862167772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6517] Loss: 0.5229200514249562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6518] Loss: 0.5228698410679117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6519] Loss: 0.5228325362450709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6520] Loss: 0.5227919703513731\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6521] Loss: 0.5227610383537878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6522] Loss: 0.5228086305622331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6523] Loss: 0.5227574626355248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6524] Loss: 0.5227575699529617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6525] Loss: 0.5227365969100399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6526] Loss: 0.5227272902848199\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6527] Loss: 0.522686467832736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6528] Loss: 0.5226642498817702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6529] Loss: 0.5226283942861877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6530] Loss: 0.5226102343008566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6531] Loss: 0.5226016748146527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6532] Loss: 0.5225993907236847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6533] Loss: 0.5225807400675444\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6534] Loss: 0.5225449885460604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6535] Loss: 0.5224907128791096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6536] Loss: 0.5224588378754479\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6537] Loss: 0.5224069412547359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6538] Loss: 0.5224115425385856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6539] Loss: 0.5224515167777861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6540] Loss: 0.5225033798571646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6541] Loss: 0.5224661895230998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6542] Loss: 0.5224293739354579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6543] Loss: 0.5224148496138246\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6544] Loss: 0.522453283232334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6545] Loss: 0.5224391785305991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6546] Loss: 0.5224456239597856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6547] Loss: 0.5224011188926179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6548] Loss: 0.5223901394869406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6549] Loss: 0.522408392541277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6550] Loss: 0.5223778417518992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6551] Loss: 0.5223369344666864\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6552] Loss: 0.5223135842871721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6553] Loss: 0.5222908729416398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6554] Loss: 0.5223137089485841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6555] Loss: 0.5223019571517301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6556] Loss: 0.5223319731206638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6557] Loss: 0.5223372621498404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6558] Loss: 0.5223359511524597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6559] Loss: 0.5224065403048314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6560] Loss: 0.5223708350254485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6561] Loss: 0.5223956080561452\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6562] Loss: 0.5223696899932107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6563] Loss: 0.5223676138045905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6564] Loss: 0.5224337491327125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6565] Loss: 0.5224592215703796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6566] Loss: 0.5225335200276605\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6567] Loss: 0.5225380207074443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6568] Loss: 0.5225507946455585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6569] Loss: 0.5225437086620903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6570] Loss: 0.5225271443771844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6571] Loss: 0.5225517971496774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6572] Loss: 0.5225025242712583\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6573] Loss: 0.5224934521188206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6574] Loss: 0.522441497092236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6575] Loss: 0.5224471045493686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6576] Loss: 0.522393100653943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6577] Loss: 0.5223844055683821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6578] Loss: 0.5223342759845362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6579] Loss: 0.5223260869901682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6580] Loss: 0.522334552228549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6581] Loss: 0.5222932871445226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6582] Loss: 0.5223275577116939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6583] Loss: 0.5223234309094871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6584] Loss: 0.5222814158611012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6585] Loss: 0.522232276863522\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6586] Loss: 0.5221937828948029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6587] Loss: 0.5221643727067389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6588] Loss: 0.522178767370958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6589] Loss: 0.5221530431282522\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6590] Loss: 0.5221653215538122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6591] Loss: 0.5222104572726401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6592] Loss: 0.5222305176496035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6593] Loss: 0.5222525364931685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6594] Loss: 0.5222294446641799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6595] Loss: 0.5221722623536894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6596] Loss: 0.5221689072974097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6597] Loss: 0.5222283670330197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6598] Loss: 0.52217252093579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6599] Loss: 0.5221476321429684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6600] Loss: 0.5221462991363897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6601] Loss: 0.5221637487240249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6602] Loss: 0.5221299640736491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6603] Loss: 0.5220861271740317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6604] Loss: 0.5220779265595883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6605] Loss: 0.5220821381568713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6606] Loss: 0.5220494681939952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6607] Loss: 0.5220145643348868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6608] Loss: 0.5220375234702839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6609] Loss: 0.5220363997454369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6610] Loss: 0.5221293467454245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6611] Loss: 0.5220938369111533\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6612] Loss: 0.5221385893710165\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6613] Loss: 0.522157573124219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6614] Loss: 0.5222089925846806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6615] Loss: 0.5222521742111732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6616] Loss: 0.5222195622703696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6617] Loss: 0.52224826970613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6618] Loss: 0.5222362214778675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6619] Loss: 0.5222775576269398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6620] Loss: 0.5222326745987914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6621] Loss: 0.5222601150967957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6622] Loss: 0.5223058762921128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6623] Loss: 0.5222575706861695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6624] Loss: 0.5222309935554048\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6625] Loss: 0.5221963813849775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6626] Loss: 0.5221625068404653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6627] Loss: 0.5221894789004181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6628] Loss: 0.5221685930872205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6629] Loss: 0.5222596529969243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6630] Loss: 0.5223178543905223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6631] Loss: 0.5222839484479065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6632] Loss: 0.5222983629324397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6633] Loss: 0.5222455468463797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6634] Loss: 0.5222226710786896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6635] Loss: 0.5222459894613344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6636] Loss: 0.522276229207768\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6637] Loss: 0.5222372546030799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6638] Loss: 0.5221888764540108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6639] Loss: 0.5222499378914155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6640] Loss: 0.5222104144888834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6641] Loss: 0.5221931138210323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6642] Loss: 0.5221569754367977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6643] Loss: 0.5221337776126354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6644] Loss: 0.5220977224659795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6645] Loss: 0.5221422863351493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6646] Loss: 0.5221893884984696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6647] Loss: 0.5221743097892118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6648] Loss: 0.5221264695511381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6649] Loss: 0.5220980118279893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6650] Loss: 0.5221400823480532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6651] Loss: 0.522207508159371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6652] Loss: 0.5221895250036599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6653] Loss: 0.522202186857705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6654] Loss: 0.522153543822545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6655] Loss: 0.5222005847899093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6656] Loss: 0.5221497023673931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6657] Loss: 0.5221197894671806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6658] Loss: 0.5221694764682978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6659] Loss: 0.5221524104381464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6660] Loss: 0.5221409758949668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6661] Loss: 0.5221155525765522\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6662] Loss: 0.5221513561584704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6663] Loss: 0.5221591671656617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6664] Loss: 0.5221729983185498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6665] Loss: 0.5221352835383287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6666] Loss: 0.5221069194207028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6667] Loss: 0.5221543826026074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6668] Loss: 0.5221972968737852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6669] Loss: 0.5221498646211036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6670] Loss: 0.5221163985428291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6671] Loss: 0.5221014037508215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6672] Loss: 0.5220716808430176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6673] Loss: 0.5220196130609195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6674] Loss: 0.522078779162833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6675] Loss: 0.5221210128151104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6676] Loss: 0.5221023726320251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6677] Loss: 0.5221178282383913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6678] Loss: 0.5220778845830418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6679] Loss: 0.5220230848085051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6680] Loss: 0.5220395590681312\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6681] Loss: 0.5220972592769614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6682] Loss: 0.5220849653361914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6683] Loss: 0.5220482383094149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6684] Loss: 0.5220346534217412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6685] Loss: 0.5221464092239194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6686] Loss: 0.5222186565669398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6687] Loss: 0.5222771104551238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6688] Loss: 0.5222599616122886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6689] Loss: 0.5222372772682444\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6690] Loss: 0.5222030755289164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6691] Loss: 0.522242678068078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6692] Loss: 0.5221932909295652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6693] Loss: 0.5221616128044145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6694] Loss: 0.5221365049775367\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6695] Loss: 0.5221390866833342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6696] Loss: 0.522110557861113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6697] Loss: 0.5220828793397412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6698] Loss: 0.522082669430478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6699] Loss: 0.5220660321579397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6700] Loss: 0.5220659116911965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6701] Loss: 0.5220308113278181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6702] Loss: 0.5219924127086227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6703] Loss: 0.5219617453878229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6704] Loss: 0.521922564861137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6705] Loss: 0.5219208666184335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6706] Loss: 0.5219918622522873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6707] Loss: 0.5220726702013034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6708] Loss: 0.5220960950675454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6709] Loss: 0.5220810404817487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6710] Loss: 0.5220883608705574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6711] Loss: 0.5221068997298339\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6712] Loss: 0.5221612895233751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6713] Loss: 0.5221347998111224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6714] Loss: 0.5221990679878907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6715] Loss: 0.5221679845877369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6716] Loss: 0.5221687016004407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6717] Loss: 0.5221251237381862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6718] Loss: 0.5220902369612079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6719] Loss: 0.5220921966793994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6720] Loss: 0.5220528425201894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6721] Loss: 0.5220010456779195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6722] Loss: 0.5220136453408967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6723] Loss: 0.5220561222443542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6724] Loss: 0.5219987930378079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6725] Loss: 0.5219564675829244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6726] Loss: 0.5219146343525811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6727] Loss: 0.521881013895339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6728] Loss: 0.5218672486733263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6729] Loss: 0.5218330279201111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6730] Loss: 0.5217872249423691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6731] Loss: 0.5217494673498803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6732] Loss: 0.5217148222170509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6733] Loss: 0.5217412989559653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6734] Loss: 0.5217164018356594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6735] Loss: 0.5217378721778173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6736] Loss: 0.5217528278029046\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6737] Loss: 0.5217937923523939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6738] Loss: 0.5217464433608525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6739] Loss: 0.5217024801984647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6740] Loss: 0.5217691047951268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6741] Loss: 0.5217481146672243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6742] Loss: 0.5217364657770684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6743] Loss: 0.5217673759790938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6744] Loss: 0.5218102753946404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6745] Loss: 0.5218445954747265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6746] Loss: 0.5219013649179356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6747] Loss: 0.5218811005435159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6748] Loss: 0.5218418843425516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6749] Loss: 0.5218190950907123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6750] Loss: 0.5218039041289534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6751] Loss: 0.5217617664690909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6752] Loss: 0.5217255825932994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6753] Loss: 0.5216972124074374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6754] Loss: 0.5216761860306793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6755] Loss: 0.521643723808736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6756] Loss: 0.5216013836652622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6757] Loss: 0.521650546333094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6758] Loss: 0.5216946268213314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6759] Loss: 0.5217454308155995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6760] Loss: 0.5218037097406387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6761] Loss: 0.5217969845941669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6762] Loss: 0.5218548447093899\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6763] Loss: 0.5218336327913264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6764] Loss: 0.5218308444912532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6765] Loss: 0.5218036119838794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6766] Loss: 0.5217451427841697\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6767] Loss: 0.5217114892991107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6768] Loss: 0.52170337683031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6769] Loss: 0.5216963597547896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6770] Loss: 0.5216580551058149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6771] Loss: 0.5216213798959917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6772] Loss: 0.521602179123376\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6773] Loss: 0.5215724765773819\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6774] Loss: 0.521571372187248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6775] Loss: 0.5215620792516212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6776] Loss: 0.521532942290946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6777] Loss: 0.521491042605062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6778] Loss: 0.5214521531431668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6779] Loss: 0.5214631241115015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6780] Loss: 0.5214614622211723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6781] Loss: 0.5215174124331551\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6782] Loss: 0.5215553227522202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6783] Loss: 0.521597896865751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6784] Loss: 0.5215759185784974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6785] Loss: 0.5215712292949517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6786] Loss: 0.5215619946381824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6787] Loss: 0.5215802368192664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6788] Loss: 0.5216135617277925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6789] Loss: 0.5215885528577021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6790] Loss: 0.5216446305702257\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6791] Loss: 0.5216098962302156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6792] Loss: 0.5215875463016433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6793] Loss: 0.5216589912679683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6794] Loss: 0.5216394636085236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6795] Loss: 0.5216805360553373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6796] Loss: 0.521632450567265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6797] Loss: 0.521637560029714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6798] Loss: 0.5217098496669462\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6799] Loss: 0.521724164962181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6800] Loss: 0.5217149949438621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6801] Loss: 0.5216773439786749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6802] Loss: 0.5216554794046088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6803] Loss: 0.5216573443427746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6804] Loss: 0.5216385952922363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6805] Loss: 0.5216808579169728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6806] Loss: 0.521726866677555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6807] Loss: 0.5218016636978355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6808] Loss: 0.5217676443400062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6809] Loss: 0.5218263071342051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6810] Loss: 0.5217897288359347\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6811] Loss: 0.5217673076327652\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6812] Loss: 0.5217527331118166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6813] Loss: 0.5217518975186042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6814] Loss: 0.5217496083723144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6815] Loss: 0.5217105526686849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6816] Loss: 0.5216750230863477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6817] Loss: 0.5216655549396138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6818] Loss: 0.521706453574728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6819] Loss: 0.5216790970942169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6820] Loss: 0.5217439582178786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6821] Loss: 0.5217101049227715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6822] Loss: 0.5217104496597224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6823] Loss: 0.5217652391319548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6824] Loss: 0.5218482787284551\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6825] Loss: 0.5219421831488326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6826] Loss: 0.5219062383890039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6827] Loss: 0.5218888763146667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6828] Loss: 0.5218486861715251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6829] Loss: 0.5218805331423758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6830] Loss: 0.5218679551907544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6831] Loss: 0.5218399253978399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6832] Loss: 0.521834958378875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6833] Loss: 0.521815909860765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6834] Loss: 0.5217897300405226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6835] Loss: 0.5217501177359004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6836] Loss: 0.5216931740515148\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6837] Loss: 0.5216853285459464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6838] Loss: 0.5216766227173293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6839] Loss: 0.5216802182892974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6840] Loss: 0.5216742710809196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6841] Loss: 0.5216413809119234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6842] Loss: 0.5216798993393506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6843] Loss: 0.5216744461463146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6844] Loss: 0.5216431435554005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6845] Loss: 0.521667036468247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6846] Loss: 0.5217122607158892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6847] Loss: 0.5216965492316858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6848] Loss: 0.5217732331210335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6849] Loss: 0.5217957631667944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6850] Loss: 0.521754061212111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6851] Loss: 0.5217835077059192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6852] Loss: 0.5217469718526945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6853] Loss: 0.5217546634581607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6854] Loss: 0.5217408367995205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6855] Loss: 0.5216922532088931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6856] Loss: 0.5217540442563178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6857] Loss: 0.521752911201796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6858] Loss: 0.5217287774211254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6859] Loss: 0.5217529158622078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6860] Loss: 0.5217529579762399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6861] Loss: 0.5217533619563198\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6862] Loss: 0.5217849050510103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6863] Loss: 0.5218204170062037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6864] Loss: 0.521789845772179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6865] Loss: 0.5217877301876231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6866] Loss: 0.5217723918432818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6867] Loss: 0.521746991149787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6868] Loss: 0.521732001144412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6869] Loss: 0.5217162414150317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6870] Loss: 0.5218015517242862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6871] Loss: 0.521812723970342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6872] Loss: 0.5218374194827368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6873] Loss: 0.5218290282943499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6874] Loss: 0.5218214202136044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6875] Loss: 0.5217728532329845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6876] Loss: 0.5217901189564872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6877] Loss: 0.5217520516264545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6878] Loss: 0.5217239067693189\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6879] Loss: 0.5217044820032533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6880] Loss: 0.5216687375957401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6881] Loss: 0.5216290347361149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6882] Loss: 0.5216615967854754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6883] Loss: 0.5217061840592833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6884] Loss: 0.5216955889424336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6885] Loss: 0.5217376113241794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6886] Loss: 0.5217049289214304\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6887] Loss: 0.5217330448606348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6888] Loss: 0.5217910290217653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6889] Loss: 0.5217480135736728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6890] Loss: 0.5218151525527145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6891] Loss: 0.5218697673188839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6892] Loss: 0.5218592441635833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6893] Loss: 0.5218089601144921\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6894] Loss: 0.5218494706238646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6895] Loss: 0.5218959400638903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6896] Loss: 0.5219694323303407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6897] Loss: 0.5219303633902803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6898] Loss: 0.5218910862765763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6899] Loss: 0.5218845921272515\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6900] Loss: 0.5218646911061798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6901] Loss: 0.5218310490952395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6902] Loss: 0.5218284622869817\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6903] Loss: 0.5218092358095553\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6904] Loss: 0.5217598683393333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6905] Loss: 0.5218138597643497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6906] Loss: 0.5217689234830947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6907] Loss: 0.52183041028237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6908] Loss: 0.5217967883902962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6909] Loss: 0.5217960773035593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6910] Loss: 0.5217874155007303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6911] Loss: 0.5218278610637422\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6912] Loss: 0.521787587673133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6913] Loss: 0.5217386837138174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6914] Loss: 0.5217868880526414\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6915] Loss: 0.5217476180900734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6916] Loss: 0.5217487644375096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6917] Loss: 0.5218134836602066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6918] Loss: 0.5218199259891045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6919] Loss: 0.5218830528172228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6920] Loss: 0.5218781756025023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6921] Loss: 0.5218577488194649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6922] Loss: 0.5218459369564755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6923] Loss: 0.521823465577827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6924] Loss: 0.5218915755564006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6925] Loss: 0.5219335881742015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6926] Loss: 0.5219622362089201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6927] Loss: 0.5219396118092652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6928] Loss: 0.5220022992938803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6929] Loss: 0.5220680051799652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6930] Loss: 0.5221093098183287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6931] Loss: 0.5220943275381333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6932] Loss: 0.5221540535333482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6933] Loss: 0.522132125600335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6934] Loss: 0.5221045644278841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6935] Loss: 0.5220565398611448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6936] Loss: 0.5220510285293206\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6937] Loss: 0.5220111964537972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6938] Loss: 0.5220091964843189\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6939] Loss: 0.5220589079443747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6940] Loss: 0.5220069274445344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6941] Loss: 0.522072638391225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6942] Loss: 0.5220424982960062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6943] Loss: 0.5220147497186427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6944] Loss: 0.5219930774854472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6945] Loss: 0.5219791406263375\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6946] Loss: 0.5219628527237252\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6947] Loss: 0.5220205269954173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6948] Loss: 0.5219885633738192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6949] Loss: 0.5219591257850434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6950] Loss: 0.5219141353060555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6951] Loss: 0.5219281100441147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6952] Loss: 0.5218740172432061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6953] Loss: 0.5218281873552284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6954] Loss: 0.5217965143755272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6955] Loss: 0.521752055457528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6956] Loss: 0.5217433321759858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6957] Loss: 0.5217472796493192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6958] Loss: 0.5217491143519433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6959] Loss: 0.5217088468431742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6960] Loss: 0.5217782628928969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6961] Loss: 0.521773925542813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6962] Loss: 0.5217445364818947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6963] Loss: 0.521723874270057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6964] Loss: 0.5217015857242341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6965] Loss: 0.5216988457186476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6966] Loss: 0.5217545436535959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6967] Loss: 0.5217169143682998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6968] Loss: 0.5216994218184469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6969] Loss: 0.5216800424429373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6970] Loss: 0.521656627609357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6971] Loss: 0.5216177862723939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6972] Loss: 0.5215770877581822\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6973] Loss: 0.5215994677993879\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6974] Loss: 0.5216615838897737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6975] Loss: 0.5217778826004207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6976] Loss: 0.5217369361154037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6977] Loss: 0.5217782046380323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6978] Loss: 0.5218277331245815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6979] Loss: 0.5217968316627518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6980] Loss: 0.5218168133492632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6981] Loss: 0.5218485154175828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6982] Loss: 0.5218302517120895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6983] Loss: 0.5217935180467927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6984] Loss: 0.5218450120538888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6985] Loss: 0.5218327495006981\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6986] Loss: 0.5218319084571238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6987] Loss: 0.5218106585993192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6988] Loss: 0.5217890858203955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6989] Loss: 0.5217971297685039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6990] Loss: 0.5217655145712656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6991] Loss: 0.5217759122411294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6992] Loss: 0.5217466096967615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6993] Loss: 0.5218458748329905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6994] Loss: 0.5218522582940853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6995] Loss: 0.521830783746476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6996] Loss: 0.5218683029701597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6997] Loss: 0.5219290774055452\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6998] Loss: 0.5219925005613618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 6999] Loss: 0.5219943406360876\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7000] Loss: 0.5219871669617383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7001] Loss: 0.5220100176116752\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7002] Loss: 0.5220124482745861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7003] Loss: 0.5219941089305858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7004] Loss: 0.521962890002795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7005] Loss: 0.5220059831042396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7006] Loss: 0.5220147441233063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7007] Loss: 0.5220846907279116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7008] Loss: 0.5221460923580031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7009] Loss: 0.5221442055185862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7010] Loss: 0.5221282046597737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7011] Loss: 0.5220777579173366\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8214\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7012] Loss: 0.5221395319501314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7013] Loss: 0.5220953113236612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7014] Loss: 0.522173304039742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7015] Loss: 0.5221590499090837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7016] Loss: 0.5221638574719869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7017] Loss: 0.5221813849031934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7018] Loss: 0.5221797033626937\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7019] Loss: 0.5221783028717213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7020] Loss: 0.5221429614313005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7021] Loss: 0.52215016093045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7022] Loss: 0.5222019732744417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7023] Loss: 0.5221727836242784\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7024] Loss: 0.5221746521339959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7025] Loss: 0.5221243448080873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7026] Loss: 0.5220965308493416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7027] Loss: 0.5221324700788429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7028] Loss: 0.5221008574484353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7029] Loss: 0.5220472021992724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7030] Loss: 0.5220127797611286\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7031] Loss: 0.5220323137857855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7032] Loss: 0.5220153531215557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7033] Loss: 0.5220057709248505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7034] Loss: 0.5220729306267128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7035] Loss: 0.5220507760111857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7036] Loss: 0.5220435990572634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7037] Loss: 0.5219897064316852\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7038] Loss: 0.5219624638762873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7039] Loss: 0.5219551977286534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7040] Loss: 0.5219225537973196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7041] Loss: 0.5219408749616853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7042] Loss: 0.5219339685399366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7043] Loss: 0.5219757616880312\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7044] Loss: 0.5220028135297238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7045] Loss: 0.522055008154656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7046] Loss: 0.5220292373474874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7047] Loss: 0.5220067429514276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7048] Loss: 0.5219983811401635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7049] Loss: 0.5220578112862383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7050] Loss: 0.5220645804721463\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7051] Loss: 0.5221253595841877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7052] Loss: 0.5220874855889891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7053] Loss: 0.5220506166908039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7054] Loss: 0.5220067150057419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7055] Loss: 0.5219835459981099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7056] Loss: 0.5219970200378123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7057] Loss: 0.5220025528568848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7058] Loss: 0.5219969318186319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7059] Loss: 0.5220482141726034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7060] Loss: 0.522099779528061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7061] Loss: 0.5220576281734794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7062] Loss: 0.5220117041447697\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7063] Loss: 0.5220753596485908\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7064] Loss: 0.5221191667972959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7065] Loss: 0.5221083975242895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7066] Loss: 0.5221297273740781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7067] Loss: 0.5221057354163083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7068] Loss: 0.522134003226315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7069] Loss: 0.5221675737367787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7070] Loss: 0.5221590496985833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7071] Loss: 0.522161715662692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7072] Loss: 0.5222158637130562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7073] Loss: 0.5222038742760488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7074] Loss: 0.5221729647438567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7075] Loss: 0.5221832018200896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7076] Loss: 0.5222212736890006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7077] Loss: 0.5221999241519922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7078] Loss: 0.5221941062928734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7079] Loss: 0.5222088387906597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7080] Loss: 0.5222041155490883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7081] Loss: 0.5222108045922006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7082] Loss: 0.5222632015964024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7083] Loss: 0.5222606564196535\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7084] Loss: 0.5222660100950562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7085] Loss: 0.5223082461397911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7086] Loss: 0.5223309171392861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7087] Loss: 0.5223042911035557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7088] Loss: 0.5223844367891831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7089] Loss: 0.5224013427055286\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7090] Loss: 0.5223758084863696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7091] Loss: 0.5223753068711037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7092] Loss: 0.5224155003119478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7093] Loss: 0.5223770439737966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7094] Loss: 0.5223264322160117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7095] Loss: 0.5222882936624478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7096] Loss: 0.5222769864774605\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7097] Loss: 0.5222534731480644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7098] Loss: 0.5222329497088536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7099] Loss: 0.5222595787103521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7100] Loss: 0.5222936844359942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7101] Loss: 0.5223670253517992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7102] Loss: 0.5223347215972008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7103] Loss: 0.5223256640738572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7104] Loss: 0.5223391404861857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7105] Loss: 0.5222934295816617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7106] Loss: 0.5222549656933549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7107] Loss: 0.5223292693443798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7108] Loss: 0.5222928288055934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7109] Loss: 0.5222739227830795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7110] Loss: 0.5222494580845038\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7111] Loss: 0.5222268886835968\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7112] Loss: 0.5222119338465112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7113] Loss: 0.5222475807693499\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7114] Loss: 0.5222356929869669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7115] Loss: 0.5222328449495808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7116] Loss: 0.5222307753326529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7117] Loss: 0.5222297619136826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7118] Loss: 0.5222196523731543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7119] Loss: 0.5221862114085464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7120] Loss: 0.522167407533044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7121] Loss: 0.5222118329873804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7122] Loss: 0.5222182470608753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7123] Loss: 0.5221911772835477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7124] Loss: 0.5221903306058356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7125] Loss: 0.5222188582541091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7126] Loss: 0.5222434957062391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7127] Loss: 0.5222593256267731\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7128] Loss: 0.5222468065212197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7129] Loss: 0.5222228056877685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7130] Loss: 0.5222265183655517\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7131] Loss: 0.5222077761742846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7132] Loss: 0.5222495697625488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7133] Loss: 0.522278440803195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7134] Loss: 0.5222534506271282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7135] Loss: 0.5222151806129599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7136] Loss: 0.5222571905631512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7137] Loss: 0.5223047911032519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7138] Loss: 0.5222806074602027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7139] Loss: 0.5222874604787091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7140] Loss: 0.522285002308194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7141] Loss: 0.5223456007325799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7142] Loss: 0.5223534843009258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7143] Loss: 0.5224282924772728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7144] Loss: 0.5224228689100572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7145] Loss: 0.522427412883267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7146] Loss: 0.5224052074984515\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7147] Loss: 0.522421350756808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7148] Loss: 0.5223791006639371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7149] Loss: 0.5223741829350405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7150] Loss: 0.5224573878189885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7151] Loss: 0.522494627434596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7152] Loss: 0.5224585101048798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7153] Loss: 0.5224402704395732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7154] Loss: 0.5224080679154338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7155] Loss: 0.5224531588516709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7156] Loss: 0.5224040782473154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7157] Loss: 0.5224032546678701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7158] Loss: 0.522463822900962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7159] Loss: 0.5224631113438772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7160] Loss: 0.5224422227260762\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7161] Loss: 0.5224324462647152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7162] Loss: 0.5224579739251627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7163] Loss: 0.5224308683946404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7164] Loss: 0.5223812258634117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7165] Loss: 0.5223947467793207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7166] Loss: 0.522437760901924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7167] Loss: 0.5224176813563338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7168] Loss: 0.5224542364043923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7169] Loss: 0.5224580164651359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7170] Loss: 0.5224367719199564\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7171] Loss: 0.5224941190997145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7172] Loss: 0.5224795760433082\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7173] Loss: 0.5224911708266782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7174] Loss: 0.5225448230815892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7175] Loss: 0.52259441766628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7176] Loss: 0.5225563678402032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7177] Loss: 0.5225342047282454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7178] Loss: 0.5225353804609699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7179] Loss: 0.5225286080233428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7180] Loss: 0.5225780257042023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7181] Loss: 0.5225320072246267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7182] Loss: 0.5225803857620099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7183] Loss: 0.5225818271349703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7184] Loss: 0.5225553827207889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7185] Loss: 0.5225321201349465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7186] Loss: 0.5225239484930838\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7187] Loss: 0.5225151605916377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7188] Loss: 0.5225712793490672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7189] Loss: 0.522544860228537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7190] Loss: 0.5225879677800004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7191] Loss: 0.5225800808104832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7192] Loss: 0.5225584544408681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7193] Loss: 0.5225483991242482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7194] Loss: 0.5225172590069753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7195] Loss: 0.522499344811657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7196] Loss: 0.5224638719143997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7197] Loss: 0.5224338058525446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7198] Loss: 0.5224018551534015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7199] Loss: 0.5224473379638132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7200] Loss: 0.5224648415151376\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7201] Loss: 0.5224598933809133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7202] Loss: 0.5224137410468066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7203] Loss: 0.5223786888877034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7204] Loss: 0.5223462732738618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7205] Loss: 0.5223566486107226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7206] Loss: 0.5223682856225056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7207] Loss: 0.5223687665843778\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7208] Loss: 0.5223166988203155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7209] Loss: 0.5222736941218145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7210] Loss: 0.522238255490118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7211] Loss: 0.5222195754378897\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7212] Loss: 0.5221801080683458\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7213] Loss: 0.5221987036996754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7214] Loss: 0.5221740737121757\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7215] Loss: 0.5221349295620417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7216] Loss: 0.5221174141026027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7217] Loss: 0.5220703196024351\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7218] Loss: 0.5220997160346151\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7219] Loss: 0.5221408816399562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7220] Loss: 0.5222141532979673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7221] Loss: 0.5221937933678316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7222] Loss: 0.5221485782756566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7223] Loss: 0.522139725458883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7224] Loss: 0.5221242547248282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7225] Loss: 0.5221089921863951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7226] Loss: 0.5220853011476063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7227] Loss: 0.5220825575071377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7228] Loss: 0.5220497989626149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7229] Loss: 0.5220698582698483\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7230] Loss: 0.5220310102172551\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7231] Loss: 0.5220798519799582\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7232] Loss: 0.5220544413729673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7233] Loss: 0.5220299900225297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7234] Loss: 0.5221183170508515\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7235] Loss: 0.5220809198046262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7236] Loss: 0.5220751568493374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7237] Loss: 0.5220651079140359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7238] Loss: 0.5220823013284615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7239] Loss: 0.522102285842517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7240] Loss: 0.5220745708710371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7241] Loss: 0.5220391341356267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7242] Loss: 0.5220707241430286\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7243] Loss: 0.5220473109303657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7244] Loss: 0.5220155210090668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7245] Loss: 0.521988956438849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7246] Loss: 0.5219438084447646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7247] Loss: 0.5219381633707478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7248] Loss: 0.5219212893090321\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7249] Loss: 0.5218954742604891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7250] Loss: 0.5219027091226698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7251] Loss: 0.5219567273905786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7252] Loss: 0.5220051010619176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7253] Loss: 0.5220125047195812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7254] Loss: 0.5220670475605721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7255] Loss: 0.5220522280199781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7256] Loss: 0.522012872507107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7257] Loss: 0.5219610522915138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7258] Loss: 0.522010207748141\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7259] Loss: 0.5220147686824037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7260] Loss: 0.5220465176348333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7261] Loss: 0.5220791300439496\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7262] Loss: 0.5221282813093355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7263] Loss: 0.5221185290039371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7264] Loss: 0.522137334495827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7265] Loss: 0.5221189499673448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7266] Loss: 0.5221201640511314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7267] Loss: 0.5220912961440625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7268] Loss: 0.5221671781801162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7269] Loss: 0.5221643658044299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7270] Loss: 0.5221776918568731\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7271] Loss: 0.5222296381032849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7272] Loss: 0.5222861639024837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7273] Loss: 0.5222710345657059\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7274] Loss: 0.5222701337287617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7275] Loss: 0.5222528650795013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7276] Loss: 0.5222199842897598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7277] Loss: 0.5222752536106835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7278] Loss: 0.5222987403353372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7279] Loss: 0.5222814473206437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7280] Loss: 0.5222753043122799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7281] Loss: 0.5222731402121553\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7282] Loss: 0.5222863023974138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7283] Loss: 0.522361563482067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7284] Loss: 0.5223160845351001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7285] Loss: 0.5223479038806859\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7286] Loss: 0.5223606775746787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7287] Loss: 0.522313192018422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7288] Loss: 0.5222958500633622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7289] Loss: 0.5222506334114645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7290] Loss: 0.5223024777980153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7291] Loss: 0.5222542669270844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7292] Loss: 0.522242984277191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7293] Loss: 0.5221986337413166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7294] Loss: 0.5221847614211926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7295] Loss: 0.5221608003097681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7296] Loss: 0.5221307617248493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7297] Loss: 0.5221586096211247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7298] Loss: 0.5221582174511888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7299] Loss: 0.5221248685189841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7300] Loss: 0.522086321521929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7301] Loss: 0.5220495020407603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7302] Loss: 0.5220513130824473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7303] Loss: 0.5220427434108894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7304] Loss: 0.5220912159989433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7305] Loss: 0.5221044202557902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7306] Loss: 0.5220951626429212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7307] Loss: 0.5220650354518388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7308] Loss: 0.5220642286684626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7309] Loss: 0.5220968246477494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7310] Loss: 0.5221490282083259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7311] Loss: 0.5222187109895181\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7312] Loss: 0.5221846355993374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7313] Loss: 0.5221856616576525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7314] Loss: 0.5221813690226125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7315] Loss: 0.5222208125682372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7316] Loss: 0.522227840982498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7317] Loss: 0.5221856831935108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7318] Loss: 0.5221430652684036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7319] Loss: 0.5221221845770464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7320] Loss: 0.5221145758063432\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7321] Loss: 0.5220884624433805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7322] Loss: 0.5221276641443275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7323] Loss: 0.5220861440470053\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7324] Loss: 0.5220801317760907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7325] Loss: 0.5220384240281644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7326] Loss: 0.5220198445240727\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7327] Loss: 0.521998213387417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7328] Loss: 0.5219872915681837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7329] Loss: 0.5219691864275341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7330] Loss: 0.5219541701316134\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7331] Loss: 0.5219121790014392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7332] Loss: 0.5219545372985948\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7333] Loss: 0.522019166676823\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7334] Loss: 0.5220070508508512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7335] Loss: 0.5220242093465267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7336] Loss: 0.5220801041399039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7337] Loss: 0.5220789237577325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7338] Loss: 0.5220795776902337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7339] Loss: 0.5220634979356137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7340] Loss: 0.5220930170349635\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7341] Loss: 0.5220782934938757\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7342] Loss: 0.5220325908970428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7343] Loss: 0.5220160584323807\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7344] Loss: 0.5220077419345547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7345] Loss: 0.5220554086834127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7346] Loss: 0.5220244497086933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7347] Loss: 0.5220196661683085\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7348] Loss: 0.5219749916855896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7349] Loss: 0.522001445696514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7350] Loss: 0.522053537010188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7351] Loss: 0.5220906830416875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7352] Loss: 0.5220698296035009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7353] Loss: 0.5221008518717537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7354] Loss: 0.5220848089753536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7355] Loss: 0.5221370093421052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7356] Loss: 0.5221370707369151\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7357] Loss: 0.5221959415802002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7358] Loss: 0.5222083126590805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7359] Loss: 0.5222017086432056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7360] Loss: 0.5221774832284364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7361] Loss: 0.5222197137637097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7362] Loss: 0.5222507419572437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7363] Loss: 0.5222655917025475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7364] Loss: 0.5222362650855659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7365] Loss: 0.522247871726121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7366] Loss: 0.5223439258339629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7367] Loss: 0.5223262103385022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7368] Loss: 0.5222893461376454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7369] Loss: 0.5222995850479034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7370] Loss: 0.5222583149073771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7371] Loss: 0.5223063447787835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7372] Loss: 0.5222681894685602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7373] Loss: 0.5222718081885275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7374] Loss: 0.5222371526948222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7375] Loss: 0.522288879980072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7376] Loss: 0.5222935796721558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7377] Loss: 0.5223057478453649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7378] Loss: 0.52233194271249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7379] Loss: 0.522335366202088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7380] Loss: 0.5222992248800644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7381] Loss: 0.5223470541253185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7382] Loss: 0.5223051789406431\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7383] Loss: 0.5223106971287266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7384] Loss: 0.5223091320829194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7385] Loss: 0.522267637612603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7386] Loss: 0.5222320505583127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7387] Loss: 0.5222432127747058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7388] Loss: 0.5222755802270457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7389] Loss: 0.5222277355490426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7390] Loss: 0.5222007212987126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7391] Loss: 0.5221966231368236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7392] Loss: 0.5221766859067799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7393] Loss: 0.5222142564813949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7394] Loss: 0.522180898873734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7395] Loss: 0.5221453623184711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7396] Loss: 0.5221459601348201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7397] Loss: 0.5221801998611111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7398] Loss: 0.5222527604165047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7399] Loss: 0.5222339569908006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7400] Loss: 0.5222184057765152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7401] Loss: 0.5222070460663083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7402] Loss: 0.5222253045861761\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7403] Loss: 0.5222886778153755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7404] Loss: 0.5222800830878968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7405] Loss: 0.5223282849295923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7406] Loss: 0.522370870205684\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7407] Loss: 0.5223313511104987\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7408] Loss: 0.522332668583111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7409] Loss: 0.5222947701978103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7410] Loss: 0.5222785777307075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7411] Loss: 0.5222278661251828\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7412] Loss: 0.5222037526592936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7413] Loss: 0.522173340991841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7414] Loss: 0.5221315171280368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7415] Loss: 0.5220882582025714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7416] Loss: 0.5220522153750385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7417] Loss: 0.5220355564784741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7418] Loss: 0.5220715656453775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7419] Loss: 0.5220507212055456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7420] Loss: 0.5220342190583087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7421] Loss: 0.5220984274926018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7422] Loss: 0.5220753107375155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7423] Loss: 0.5220413368379054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7424] Loss: 0.5220392037324213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7425] Loss: 0.5220232853058534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7426] Loss: 0.5220003566468224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7427] Loss: 0.5219649962862224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7428] Loss: 0.5219332542583824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7429] Loss: 0.5219187176560094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7430] Loss: 0.5219298511485144\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7431] Loss: 0.5219105256506343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7432] Loss: 0.5219178387442616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7433] Loss: 0.5219247018423071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7434] Loss: 0.5219253489412992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7435] Loss: 0.5219741953968572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7436] Loss: 0.5219624703317028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7437] Loss: 0.5220125924332827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7438] Loss: 0.5219909632384915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7439] Loss: 0.5219791747478029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7440] Loss: 0.5220575102049895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7441] Loss: 0.5220360295835502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7442] Loss: 0.522002186924993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7443] Loss: 0.5219761301140996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7444] Loss: 0.5219870960253407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7445] Loss: 0.5219509637862895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7446] Loss: 0.521931505786747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7447] Loss: 0.5219182477219744\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7448] Loss: 0.5218911586722919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7449] Loss: 0.5220207555533865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7450] Loss: 0.5220206662926619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7451] Loss: 0.5219792589993938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7452] Loss: 0.5219963021089143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7453] Loss: 0.5219752760771696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7454] Loss: 0.5220491850130637\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7455] Loss: 0.5220161559386078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7456] Loss: 0.5220492136408059\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7457] Loss: 0.5220247717174469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7458] Loss: 0.5219850631373911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7459] Loss: 0.5219362975346269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7460] Loss: 0.5219880517588245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7461] Loss: 0.5219962165207679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7462] Loss: 0.5219657728930783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7463] Loss: 0.521999284466462\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7464] Loss: 0.5220374226042996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7465] Loss: 0.5221139363317846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7466] Loss: 0.5220983170761228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7467] Loss: 0.5221229063504144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7468] Loss: 0.5221079727760606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7469] Loss: 0.5221556822451765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7470] Loss: 0.5221508718363341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7471] Loss: 0.5221362222377262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7472] Loss: 0.5221330516834397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7473] Loss: 0.5220937228688183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7474] Loss: 0.5220652525000261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7475] Loss: 0.5220890429814942\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7476] Loss: 0.5220672592281131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7477] Loss: 0.5221129752619632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7478] Loss: 0.5221716957241639\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7479] Loss: 0.5221705862231021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7480] Loss: 0.5221402974197991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7481] Loss: 0.5221789390096669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7482] Loss: 0.5221559588855631\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7483] Loss: 0.5221924745705725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7484] Loss: 0.5221860093687714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7485] Loss: 0.5221887930939274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7486] Loss: 0.5221718869090867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7487] Loss: 0.522222120762811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7488] Loss: 0.5222224759962436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7489] Loss: 0.5222052592490051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7490] Loss: 0.5222017954469239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7491] Loss: 0.522186581143044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7492] Loss: 0.5221470425268772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7493] Loss: 0.5221336754557739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7494] Loss: 0.5221027369117034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7495] Loss: 0.522189382862089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7496] Loss: 0.5221711883815728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7497] Loss: 0.5222015992922385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7498] Loss: 0.522199830932742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7499] Loss: 0.522191564497185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7500] Loss: 0.5221776927255083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7501] Loss: 0.522157455896158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7502] Loss: 0.5221194536862064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7503] Loss: 0.5221377506087168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7504] Loss: 0.5221390263405159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7505] Loss: 0.5222203009229971\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7506] Loss: 0.5222282231028691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7507] Loss: 0.5222579403014392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7508] Loss: 0.5222868577506107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7509] Loss: 0.5223322012715211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7510] Loss: 0.5223529072425195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7511] Loss: 0.5223425010807462\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8506999999999999\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7512] Loss: 0.5223954563348063\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7513] Loss: 0.5223796467078544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7514] Loss: 0.5224089870502749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7515] Loss: 0.5224519986773626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7516] Loss: 0.5224721265683983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7517] Loss: 0.522486456983675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7518] Loss: 0.5224561247247437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7519] Loss: 0.5224965426330476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7520] Loss: 0.5225930148732986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7521] Loss: 0.5226377948381101\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7522] Loss: 0.5226668973654696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7523] Loss: 0.5227248391024178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7524] Loss: 0.5226912685195401\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7525] Loss: 0.5226941895947996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7526] Loss: 0.5227568893116753\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7527] Loss: 0.5228132769769118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7528] Loss: 0.5227813137927161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7529] Loss: 0.522767059951872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7530] Loss: 0.5228238959873673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7531] Loss: 0.5227923312323536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7532] Loss: 0.5227976935888858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7533] Loss: 0.5227488761344188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7534] Loss: 0.5227180635896013\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7535] Loss: 0.5227612757194933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7536] Loss: 0.5227982596054974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7537] Loss: 0.5228252039218468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7538] Loss: 0.5228182118095555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7539] Loss: 0.522857795907366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7540] Loss: 0.5228176668116751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7541] Loss: 0.5228267900530228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7542] Loss: 0.5228358228352934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7543] Loss: 0.5229065860362746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7544] Loss: 0.5229111724184391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7545] Loss: 0.5229298810947848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7546] Loss: 0.5229075078605216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7547] Loss: 0.5229937664098288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7548] Loss: 0.5229717354229582\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7549] Loss: 0.5229426634478491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7550] Loss: 0.5228987593500113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7551] Loss: 0.5228665749460539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7552] Loss: 0.5228769144773889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7553] Loss: 0.5229048550416677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7554] Loss: 0.5229065151147921\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7555] Loss: 0.5229336736090561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7556] Loss: 0.5229061549778455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7557] Loss: 0.5229332557853882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7558] Loss: 0.5229478044166873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7559] Loss: 0.5229653233939966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7560] Loss: 0.5229661911852816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7561] Loss: 0.5230005272076225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7562] Loss: 0.5229654106239129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7563] Loss: 0.522942770419857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7564] Loss: 0.5229056768852353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7565] Loss: 0.5228839695876072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7566] Loss: 0.5228864424175619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7567] Loss: 0.5229703453954732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7568] Loss: 0.5229595152877079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7569] Loss: 0.5229573471803878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7570] Loss: 0.5229435598926233\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7571] Loss: 0.5229329489147236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7572] Loss: 0.522968179420485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7573] Loss: 0.5229265831857097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7574] Loss: 0.5229591158105826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7575] Loss: 0.5229309681810898\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7576] Loss: 0.5229404259282234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7577] Loss: 0.5229831968323587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7578] Loss: 0.5229416512054074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7579] Loss: 0.5229115787182624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7580] Loss: 0.5229470850997295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7581] Loss: 0.5229103326515099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7582] Loss: 0.522915277278211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7583] Loss: 0.5229588813619434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7584] Loss: 0.5229226625362526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7585] Loss: 0.5229029255178708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7586] Loss: 0.5228599278101516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7587] Loss: 0.5228897763545928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7588] Loss: 0.5228891458512869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7589] Loss: 0.522846201892828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7590] Loss: 0.5228480985524964\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7591] Loss: 0.5228709352250457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7592] Loss: 0.5228315413427433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7593] Loss: 0.5229096439461006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7594] Loss: 0.5229362062289512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7595] Loss: 0.5229325340260228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7596] Loss: 0.5229055113688789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7597] Loss: 0.5229179003148855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7598] Loss: 0.5228752906205086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7599] Loss: 0.5228730896486519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7600] Loss: 0.5228235003684236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7601] Loss: 0.522857199714981\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7602] Loss: 0.5228207633235464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7603] Loss: 0.5227884629432227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7604] Loss: 0.5227631171606009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7605] Loss: 0.5227418898191983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7606] Loss: 0.5227806730428973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7607] Loss: 0.522791567975171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7608] Loss: 0.5227716302839794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7609] Loss: 0.5227825656451378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7610] Loss: 0.5227486407127179\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7611] Loss: 0.5227218084000581\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7612] Loss: 0.5227075716712984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7613] Loss: 0.5227597592524135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7614] Loss: 0.52275360024509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7615] Loss: 0.522721298453172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7616] Loss: 0.5227015428297492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7617] Loss: 0.5226715583077997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7618] Loss: 0.5226270885814189\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7619] Loss: 0.5225836844099006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7620] Loss: 0.5225458506646706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7621] Loss: 0.5225787088201613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7622] Loss: 0.5225564225883766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7623] Loss: 0.5226044235925174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7624] Loss: 0.5226641029291251\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7625] Loss: 0.522668076598217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7626] Loss: 0.5226631406265373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7627] Loss: 0.5226313196571754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7628] Loss: 0.5225961606663813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7629] Loss: 0.522556613393422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7630] Loss: 0.5225601251184773\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7631] Loss: 0.5225589281030961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7632] Loss: 0.5226178739499222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7633] Loss: 0.5226043181696016\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7634] Loss: 0.5226148494207109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7635] Loss: 0.5225738847067481\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7636] Loss: 0.522540847034542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7637] Loss: 0.522598969770361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7638] Loss: 0.5225896905547407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7639] Loss: 0.5226261003967387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7640] Loss: 0.5226364649591907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7641] Loss: 0.522658376533827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7642] Loss: 0.5226569127393819\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7643] Loss: 0.5226678946514409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7644] Loss: 0.522629882042686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7645] Loss: 0.5226721167961199\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7646] Loss: 0.5226544549692873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7647] Loss: 0.5226658034140078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7648] Loss: 0.5226440122666797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7649] Loss: 0.5226056346653356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7650] Loss: 0.5225754366931962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7651] Loss: 0.5225372018871346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7652] Loss: 0.5225212675300475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7653] Loss: 0.5224841380405539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7654] Loss: 0.5224914028552284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7655] Loss: 0.5224755277973192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7656] Loss: 0.5224670560072858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7657] Loss: 0.5225194661233147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7658] Loss: 0.522549506381079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7659] Loss: 0.5225238670546919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7660] Loss: 0.5225889027097842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7661] Loss: 0.5226256404180925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7662] Loss: 0.5225989386601836\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7663] Loss: 0.5225590469994512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7664] Loss: 0.5225118767716946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7665] Loss: 0.5225241444270816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7666] Loss: 0.5225265762196565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7667] Loss: 0.5225191969219926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7668] Loss: 0.5225008715045089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7669] Loss: 0.5225003228988173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7670] Loss: 0.5225654973331111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7671] Loss: 0.5226013234184035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7672] Loss: 0.5226064214440772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7673] Loss: 0.5226228259536086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7674] Loss: 0.5225996722361683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7675] Loss: 0.5225854998440154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7676] Loss: 0.5226493487802591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7677] Loss: 0.522617183678716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7678] Loss: 0.522659557663636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7679] Loss: 0.5227148566872593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7680] Loss: 0.5227494192838337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7681] Loss: 0.522742265505266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7682] Loss: 0.5227002876634645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7683] Loss: 0.5227088318631586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7684] Loss: 0.5226796102002035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7685] Loss: 0.5227014428813282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7686] Loss: 0.5227229795029331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7687] Loss: 0.5226919413011553\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7688] Loss: 0.5226444755051587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7689] Loss: 0.5226868902230465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7690] Loss: 0.5226390972090132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7691] Loss: 0.5226908179793479\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7692] Loss: 0.5227422648649201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7693] Loss: 0.5227716389110489\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7694] Loss: 0.5227531728983954\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7695] Loss: 0.5228161146409327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7696] Loss: 0.5227666359126253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7697] Loss: 0.5228110102414919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7698] Loss: 0.5227816794471868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7699] Loss: 0.5228118933628457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7700] Loss: 0.5228144423023219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7701] Loss: 0.5228072854527975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7702] Loss: 0.5227693973421984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7703] Loss: 0.5228269100479236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7704] Loss: 0.5228097102469393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7705] Loss: 0.5227662227442067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7706] Loss: 0.5227535715851603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7707] Loss: 0.5227690734506498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7708] Loss: 0.5228054409980708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7709] Loss: 0.5227815662014565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7710] Loss: 0.5227566094199816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7711] Loss: 0.5227298193500367\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7712] Loss: 0.5227836331984164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7713] Loss: 0.5227853219144695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7714] Loss: 0.5227485882704752\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7715] Loss: 0.5227545834390096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7716] Loss: 0.5227672746330044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7717] Loss: 0.5228416636985235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7718] Loss: 0.522802881683563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7719] Loss: 0.5228021067746355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7720] Loss: 0.5228485245583127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7721] Loss: 0.5228524334225537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7722] Loss: 0.5228367447088609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7723] Loss: 0.5228782886339427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7724] Loss: 0.5229226984734313\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7725] Loss: 0.5229023230567378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7726] Loss: 0.5229214950921035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7727] Loss: 0.5228912166562523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7728] Loss: 0.5228802713682428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7729] Loss: 0.5228525989210693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7730] Loss: 0.5228242696219039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7731] Loss: 0.5228165748471485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7732] Loss: 0.5228136544456544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7733] Loss: 0.522816902988673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7734] Loss: 0.5228578785077513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7735] Loss: 0.5228203583485528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7736] Loss: 0.5228920741846547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7737] Loss: 0.5229028584993577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7738] Loss: 0.5228813564354337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7739] Loss: 0.5228496038566603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7740] Loss: 0.5228729846257392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7741] Loss: 0.5229147449125476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7742] Loss: 0.5229663566863412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7743] Loss: 0.5229645525887516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7744] Loss: 0.5229553668549913\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7745] Loss: 0.5229546235957966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7746] Loss: 0.5229238607689549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7747] Loss: 0.5229688720473668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7748] Loss: 0.5229442567765532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7749] Loss: 0.5229600363569639\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7750] Loss: 0.5229144730311895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7751] Loss: 0.5229152090299065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7752] Loss: 0.5228992564713254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7753] Loss: 0.5228728553516568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7754] Loss: 0.5228543016465081\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7755] Loss: 0.5228209955742475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7756] Loss: 0.5227967675301274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7757] Loss: 0.5228371054707223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7758] Loss: 0.522810592912302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7759] Loss: 0.5228387387103254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7760] Loss: 0.5227996426574115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7761] Loss: 0.5227690771922886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7762] Loss: 0.522728511861228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7763] Loss: 0.5227142361982862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7764] Loss: 0.5227313179359444\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7765] Loss: 0.5227043128399748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7766] Loss: 0.5226544786795004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7767] Loss: 0.5226664683539397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7768] Loss: 0.5226991227788188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7769] Loss: 0.5227341239245277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7770] Loss: 0.522700126746223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7771] Loss: 0.5226725204874048\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7772] Loss: 0.5226740219977335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7773] Loss: 0.5226346594964311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7774] Loss: 0.5226111496482783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7775] Loss: 0.522588231501215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7776] Loss: 0.5226692315749876\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7777] Loss: 0.522674466460611\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7778] Loss: 0.5226450982489754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7779] Loss: 0.5226377173161504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7780] Loss: 0.5226999572661425\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7781] Loss: 0.5227142576501747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7782] Loss: 0.5227635564124755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7783] Loss: 0.5227399968091287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7784] Loss: 0.5227907574665163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7785] Loss: 0.5228537118721663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7786] Loss: 0.5228143045704334\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7787] Loss: 0.522878519679813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7788] Loss: 0.5228474952995072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7789] Loss: 0.5228628790842044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7790] Loss: 0.5228995224849864\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7791] Loss: 0.5228749678416855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7792] Loss: 0.5228733961250192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7793] Loss: 0.5228322453139068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7794] Loss: 0.5227942407082097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7795] Loss: 0.5227786138684437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7796] Loss: 0.5227523935412169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7797] Loss: 0.5227926317083503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7798] Loss: 0.5227814248336723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7799] Loss: 0.5228319175818409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7800] Loss: 0.5227943770740748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7801] Loss: 0.5228395344703028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7802] Loss: 0.5228724397342903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7803] Loss: 0.5229035393598883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7804] Loss: 0.5229155507611842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7805] Loss: 0.5229561333034527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7806] Loss: 0.5229735025674465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7807] Loss: 0.5229798721743558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7808] Loss: 0.5230618194276485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7809] Loss: 0.523117036119375\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7810] Loss: 0.5230735163798887\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7811] Loss: 0.5230999943504202\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7812] Loss: 0.5230589903823744\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7813] Loss: 0.5230816184727604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7814] Loss: 0.523056510523745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7815] Loss: 0.5230582152079589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7816] Loss: 0.5231750102511276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7817] Loss: 0.523174904573383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7818] Loss: 0.5231309683843591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7819] Loss: 0.5231534207889851\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7820] Loss: 0.5231750107741062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7821] Loss: 0.5231711030772765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7822] Loss: 0.5231439555980901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7823] Loss: 0.5231247435194123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7824] Loss: 0.5230996480381225\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7825] Loss: 0.5230670676032566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7826] Loss: 0.5230241844185133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7827] Loss: 0.5230528474204345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7828] Loss: 0.5230169005164621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7829] Loss: 0.5229805394679344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7830] Loss: 0.5229827953695925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7831] Loss: 0.5229446035788825\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7832] Loss: 0.522970480439898\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7833] Loss: 0.5229410441597677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7834] Loss: 0.5229424561660254\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7835] Loss: 0.522937872674278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7836] Loss: 0.5229300108318474\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7837] Loss: 0.522928197891175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7838] Loss: 0.5229911282658577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7839] Loss: 0.5230401737137923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7840] Loss: 0.5229987932381819\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7841] Loss: 0.5230252689715866\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7842] Loss: 0.5229878799956286\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7843] Loss: 0.5229557173388336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7844] Loss: 0.522930616265524\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7845] Loss: 0.5228999055236395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7846] Loss: 0.5228647324119491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7847] Loss: 0.5229853842642549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7848] Loss: 0.5229483527606305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7849] Loss: 0.5229471161411315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7850] Loss: 0.5229762535653907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7851] Loss: 0.5230650692800624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7852] Loss: 0.5230360298626301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7853] Loss: 0.5230001039893332\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7854] Loss: 0.5229718694808306\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7855] Loss: 0.522924284627845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7856] Loss: 0.5229024934125245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7857] Loss: 0.5229086705770106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7858] Loss: 0.5229234947525391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7859] Loss: 0.5229576436118668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7860] Loss: 0.52299185194734\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7861] Loss: 0.5229514273996662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7862] Loss: 0.5229136090891859\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7863] Loss: 0.5228866401309467\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7864] Loss: 0.5228635029670237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7865] Loss: 0.5228172505817634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7866] Loss: 0.5227827288295472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7867] Loss: 0.5227413213725354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7868] Loss: 0.5227122259252613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7869] Loss: 0.5227091721496856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7870] Loss: 0.5227596132793342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7871] Loss: 0.5227377111032239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7872] Loss: 0.5227378447036871\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7873] Loss: 0.5227448971747024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7874] Loss: 0.5226995171832365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7875] Loss: 0.5226966339313328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7876] Loss: 0.522670812364703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7877] Loss: 0.5226519980002764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7878] Loss: 0.5226555353073034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7879] Loss: 0.522622004060348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7880] Loss: 0.5226454659831411\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7881] Loss: 0.5226816930096148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7882] Loss: 0.5226491953100255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7883] Loss: 0.5227073091802904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7884] Loss: 0.522693218857834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7885] Loss: 0.5226924019304372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7886] Loss: 0.522718119521882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7887] Loss: 0.5227164503025851\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7888] Loss: 0.5227675029104659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7889] Loss: 0.5227375239121719\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7890] Loss: 0.5227548867504448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7891] Loss: 0.5227185009924833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7892] Loss: 0.5226940081326886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7893] Loss: 0.5226898429985697\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7894] Loss: 0.5226568948086008\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7895] Loss: 0.522620974352978\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7896] Loss: 0.5225810791934986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7897] Loss: 0.5225448915680675\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7898] Loss: 0.5225115466376834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7899] Loss: 0.5224739967554602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7900] Loss: 0.5224685344285507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7901] Loss: 0.5224652732661348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7902] Loss: 0.5225216495538396\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7903] Loss: 0.5225563239244232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7904] Loss: 0.5225665900037035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7905] Loss: 0.5226163268230347\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7906] Loss: 0.5226326030253429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7907] Loss: 0.5227506179813277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7908] Loss: 0.5227926129580479\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7909] Loss: 0.5228379166078723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7910] Loss: 0.5229588113986963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7911] Loss: 0.5229625841130793\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7912] Loss: 0.5230103848782368\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7913] Loss: 0.5230797444741081\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7914] Loss: 0.5232226667369236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7915] Loss: 0.5232137602841347\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7916] Loss: 0.5231822975726343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7917] Loss: 0.523151698756012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7918] Loss: 0.5231993993036208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7919] Loss: 0.5231970631673214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7920] Loss: 0.5232563349741352\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7921] Loss: 0.5232815572409121\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7922] Loss: 0.523275039766408\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7923] Loss: 0.5232898865383067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7924] Loss: 0.5232876785901444\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7925] Loss: 0.5232694850065222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7926] Loss: 0.5232939171145681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7927] Loss: 0.5233244945860076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7928] Loss: 0.5233659442884802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7929] Loss: 0.5233527875413373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7930] Loss: 0.5233333118099247\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7931] Loss: 0.5233611562790663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7932] Loss: 0.5233346861055584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7933] Loss: 0.5233852472645597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7934] Loss: 0.5233672319104542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7935] Loss: 0.523363825831349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7936] Loss: 0.5233970645071682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7937] Loss: 0.5233977415491213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7938] Loss: 0.5233766906300442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7939] Loss: 0.5233492562240186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7940] Loss: 0.5233239514329552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7941] Loss: 0.52336835767961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7942] Loss: 0.5233506593468276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7943] Loss: 0.5233532479425049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7944] Loss: 0.5233319321061483\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7945] Loss: 0.5233904081438722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7946] Loss: 0.5234157970458702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7947] Loss: 0.5234186488930154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7948] Loss: 0.5234015681216171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7949] Loss: 0.5233555822995645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7950] Loss: 0.523317425698042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7951] Loss: 0.5233086686335557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7952] Loss: 0.5232740536880762\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7953] Loss: 0.5233246653523318\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7954] Loss: 0.5233020735669367\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7955] Loss: 0.5232755346174925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7956] Loss: 0.5233257196097165\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7957] Loss: 0.5233434312604843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7958] Loss: 0.5233108035611863\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7959] Loss: 0.5232819262079336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7960] Loss: 0.5232738437988614\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7961] Loss: 0.5233162914469904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7962] Loss: 0.5232770203768216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7963] Loss: 0.5233022514748186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7964] Loss: 0.5232649361092506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7965] Loss: 0.5232443954463296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7966] Loss: 0.5232178891325457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7967] Loss: 0.5231729106535713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7968] Loss: 0.523143747870901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7969] Loss: 0.5231361112832416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7970] Loss: 0.5231390674216498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7971] Loss: 0.5231753091297014\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7972] Loss: 0.5232098028443193\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7973] Loss: 0.5232114818705386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7974] Loss: 0.5232103656566156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7975] Loss: 0.5231915414612802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7976] Loss: 0.52320813961844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7977] Loss: 0.5232005008368251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7978] Loss: 0.5232279953323843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7979] Loss: 0.5231903306685995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7980] Loss: 0.5231814552243136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7981] Loss: 0.5231951149695371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7982] Loss: 0.5231788883193115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7983] Loss: 0.5232318238702051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7984] Loss: 0.5232556642341939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7985] Loss: 0.5232583799629307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7986] Loss: 0.5232922479195382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7987] Loss: 0.5232780969177192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7988] Loss: 0.523257641252763\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7989] Loss: 0.5232950481485915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7990] Loss: 0.5232994748369099\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7991] Loss: 0.5232739242568339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7992] Loss: 0.5232410505891993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7993] Loss: 0.5231986709509593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7994] Loss: 0.5232552232699621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7995] Loss: 0.5232366579408716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7996] Loss: 0.5232387377780738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7997] Loss: 0.523249788311051\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7998] Loss: 0.5232397197252219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 7999] Loss: 0.5232428950013235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8000] Loss: 0.5232448359078495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8001] Loss: 0.5232466772495069\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8002] Loss: 0.523229980474319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8003] Loss: 0.5232414108410579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8004] Loss: 0.5232052695068767\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8005] Loss: 0.5231908781477577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8006] Loss: 0.5232213189747889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8007] Loss: 0.5232416312623314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8008] Loss: 0.5232529807810975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8009] Loss: 0.5232202888155066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8010] Loss: 0.5231968038956324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8011] Loss: 0.5231619528922505\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8489\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8012] Loss: 0.5231447417325765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8013] Loss: 0.5231032933804983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8014] Loss: 0.5231148443305924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8015] Loss: 0.5231308163125701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8016] Loss: 0.5231044708912194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8017] Loss: 0.5231124269822569\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8018] Loss: 0.5230766270958686\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8019] Loss: 0.5230498668682021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8020] Loss: 0.5230268699827588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8021] Loss: 0.5230442514560493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8022] Loss: 0.52310809897729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8023] Loss: 0.5230647193600902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8024] Loss: 0.52308821934579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8025] Loss: 0.5231232076526402\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8026] Loss: 0.5230787643779142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8027] Loss: 0.523042650628442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8028] Loss: 0.5230057527427345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8029] Loss: 0.522985234115412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8030] Loss: 0.5230281214546808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8031] Loss: 0.5230097588631657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8032] Loss: 0.5229677841606148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8033] Loss: 0.5229271537510778\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8034] Loss: 0.5229700024099599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8035] Loss: 0.5229594140333986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8036] Loss: 0.5229241780297191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8037] Loss: 0.5228972550156393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8038] Loss: 0.52289343869371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8039] Loss: 0.5229020959325704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8040] Loss: 0.5229159055497858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8041] Loss: 0.5229465123006285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8042] Loss: 0.5229824058933107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8043] Loss: 0.5230274868904793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8044] Loss: 0.5230006471608563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8045] Loss: 0.5229635808866547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8046] Loss: 0.5229506455243204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8047] Loss: 0.5229615787738859\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8048] Loss: 0.5229379190559507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8049] Loss: 0.5229088398684145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8050] Loss: 0.5229327096665569\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8051] Loss: 0.5228963741777494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8052] Loss: 0.5228803068699744\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8053] Loss: 0.522837038897052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8054] Loss: 0.5229084264735907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8055] Loss: 0.522889629553453\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8056] Loss: 0.5228932506876032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8057] Loss: 0.5228567484144787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8058] Loss: 0.522890985631949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8059] Loss: 0.5228437086986002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8060] Loss: 0.52280132858169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8061] Loss: 0.5227704347166663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8062] Loss: 0.5227388510543663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8063] Loss: 0.5227885312013211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8064] Loss: 0.5227784564255532\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8065] Loss: 0.5227331097732705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8066] Loss: 0.5227006918964183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8067] Loss: 0.5227027467982044\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8068] Loss: 0.5226642473049503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8069] Loss: 0.5226446034862087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8070] Loss: 0.5226192187241934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8071] Loss: 0.5226291576783472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8072] Loss: 0.52266507049152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8073] Loss: 0.5227137375398869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8074] Loss: 0.5227114024681903\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8075] Loss: 0.5227027523009413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8076] Loss: 0.5226995368408615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8077] Loss: 0.5226763471364345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8078] Loss: 0.5226714728660015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8079] Loss: 0.5227009686824116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8080] Loss: 0.5227496702514363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8081] Loss: 0.5227501653287357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8082] Loss: 0.5227428286248298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8083] Loss: 0.5228051251174594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8084] Loss: 0.5227754230817359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8085] Loss: 0.5227550856842853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8086] Loss: 0.5227689304602468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8087] Loss: 0.5227394185599417\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8088] Loss: 0.5227183054736404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8089] Loss: 0.5227121637264364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8090] Loss: 0.522739626347311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8091] Loss: 0.5227107919213042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8092] Loss: 0.5226867003413028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8093] Loss: 0.5226718877132314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8094] Loss: 0.522649897763482\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8095] Loss: 0.5226541715836792\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8096] Loss: 0.5227058527555593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8097] Loss: 0.5226999138955372\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8098] Loss: 0.5226871653012716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8099] Loss: 0.5226788468485153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8100] Loss: 0.522695565755738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8101] Loss: 0.5226925995125787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8102] Loss: 0.5226450422925931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8103] Loss: 0.5226467268851283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8104] Loss: 0.52264087151737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8105] Loss: 0.522618959540988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8106] Loss: 0.5226211273094238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8107] Loss: 0.5226505235204324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8108] Loss: 0.5226355554532552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8109] Loss: 0.522691384952058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8110] Loss: 0.5226778466764249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8111] Loss: 0.5226298149691304\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8112] Loss: 0.5226542927409059\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8113] Loss: 0.5226118723122867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8114] Loss: 0.5225968305207691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8115] Loss: 0.5225542603042234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8116] Loss: 0.5225390879897234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8117] Loss: 0.5225455597037161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8118] Loss: 0.5225200172039738\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8119] Loss: 0.5224875552744508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8120] Loss: 0.5224817496874956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8121] Loss: 0.5224443053098127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8122] Loss: 0.522435099955333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8123] Loss: 0.5224081385539644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8124] Loss: 0.5223799841451557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8125] Loss: 0.5223853915205772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8126] Loss: 0.5224382293355816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8127] Loss: 0.5224214141898164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8128] Loss: 0.5224633787319272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8129] Loss: 0.5224457654462961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8130] Loss: 0.5224198987086613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8131] Loss: 0.5224791920540167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8132] Loss: 0.5224880178560894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8133] Loss: 0.5224663142369486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8134] Loss: 0.5224824669767965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8135] Loss: 0.5224656629288783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8136] Loss: 0.5224596434906653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8137] Loss: 0.5224411620863365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8138] Loss: 0.5224074089931892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8139] Loss: 0.5224528222561258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8140] Loss: 0.5224509627720632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8141] Loss: 0.5224835587046184\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8142] Loss: 0.5224582914700361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8143] Loss: 0.522443682634704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8144] Loss: 0.5224269082565285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8145] Loss: 0.5223964483346046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8146] Loss: 0.5223754142406662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8147] Loss: 0.5223512113586027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8148] Loss: 0.5223186121393667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8149] Loss: 0.5223329051507831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8150] Loss: 0.5223000842824345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8151] Loss: 0.5222760559282851\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8152] Loss: 0.5222756854762771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8153] Loss: 0.5222586662563746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8154] Loss: 0.5222475729582511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8155] Loss: 0.5222425120417943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8156] Loss: 0.5222135140981282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8157] Loss: 0.52220812588404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8158] Loss: 0.5222021844004238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8159] Loss: 0.5222099895632522\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8160] Loss: 0.5221773246417638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8161] Loss: 0.5221607808840226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8162] Loss: 0.5221868833002157\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8163] Loss: 0.5221575593794154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8164] Loss: 0.5221324217539093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8165] Loss: 0.5221802787987879\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8166] Loss: 0.5221363216436146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8167] Loss: 0.5221126300906861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8168] Loss: 0.5220773878881159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8169] Loss: 0.5221035884128319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8170] Loss: 0.522123647269163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8171] Loss: 0.5221270557958329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8172] Loss: 0.522107749827317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8173] Loss: 0.5222121464681893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8174] Loss: 0.5222175059711759\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8175] Loss: 0.5222216927919681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8176] Loss: 0.5222148310102497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8177] Loss: 0.5222059037355147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8178] Loss: 0.522198080013802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8179] Loss: 0.522246010651068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8180] Loss: 0.5222259555832814\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8181] Loss: 0.5222325213122377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8182] Loss: 0.5221912456895842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8183] Loss: 0.5221812704268046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8184] Loss: 0.5221582713333275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8185] Loss: 0.5222133569589267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8186] Loss: 0.5221826871444528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8187] Loss: 0.5221691128738507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8188] Loss: 0.5221432369550743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8189] Loss: 0.5221332174378116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8190] Loss: 0.5221075040501698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8191] Loss: 0.5220698331551943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8192] Loss: 0.5220430663063544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8193] Loss: 0.5221220554763818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8194] Loss: 0.5221104338815564\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8195] Loss: 0.5221623670826039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8196] Loss: 0.5221523058571885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8197] Loss: 0.522131788289809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8198] Loss: 0.5221113396496448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8199] Loss: 0.5220709964662058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8200] Loss: 0.5220461313161026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8201] Loss: 0.5220396077678628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8202] Loss: 0.5220557439334214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8203] Loss: 0.522020149960035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8204] Loss: 0.5220081651748419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8205] Loss: 0.522050786130687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8206] Loss: 0.5220914328830963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8207] Loss: 0.5220569725491652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8208] Loss: 0.5220330453872929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8209] Loss: 0.5221162091789191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8210] Loss: 0.5221407785276314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8211] Loss: 0.5221116675632121\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8212] Loss: 0.5220859288101349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8213] Loss: 0.5221123953430773\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8214] Loss: 0.5221620399121926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8215] Loss: 0.5221831640046112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8216] Loss: 0.5221405080192094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8217] Loss: 0.5221300696342916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8218] Loss: 0.5221019846381891\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8219] Loss: 0.5220844195941834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8220] Loss: 0.5221653368261229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8221] Loss: 0.5221323004733294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8222] Loss: 0.5221243767160536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8223] Loss: 0.5221003590159637\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8224] Loss: 0.5220898924395925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8225] Loss: 0.5220766448194303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8226] Loss: 0.5220616604030102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8227] Loss: 0.5220435054033118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8228] Loss: 0.5220184482481542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8229] Loss: 0.5220523992803588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8230] Loss: 0.5220615981001928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8231] Loss: 0.5220340845336601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8232] Loss: 0.5220369856435071\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8233] Loss: 0.5220074996450598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8234] Loss: 0.5220066551591743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8235] Loss: 0.5219987585668039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8236] Loss: 0.5220094985590678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8237] Loss: 0.5220752739460452\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8238] Loss: 0.5220757681089714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8239] Loss: 0.5220647084518645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8240] Loss: 0.5221175377034277\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8241] Loss: 0.5220837126210257\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8242] Loss: 0.5221199878427382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8243] Loss: 0.5221325364370912\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8244] Loss: 0.5220920675703189\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8245] Loss: 0.5220875996925327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8246] Loss: 0.5220612008875812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8247] Loss: 0.522038512537611\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8248] Loss: 0.5220575629008712\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8249] Loss: 0.5220309243343616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8250] Loss: 0.5219951977332433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8251] Loss: 0.5219833891592874\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8252] Loss: 0.521979270433154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8253] Loss: 0.5219994662775791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8254] Loss: 0.5219818997650969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8255] Loss: 0.5219694166586736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8256] Loss: 0.5219629221174288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8257] Loss: 0.5219471250385781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8258] Loss: 0.5219869979868307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8259] Loss: 0.5219675041326539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8260] Loss: 0.5220149641844534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8261] Loss: 0.5220031370356566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8262] Loss: 0.522041525407257\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8263] Loss: 0.5220169620630465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8264] Loss: 0.5219761080922679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8265] Loss: 0.5219776807716783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8266] Loss: 0.5219502046074125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8267] Loss: 0.5219458048877235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8268] Loss: 0.5219221251754301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8269] Loss: 0.5219127979340328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8270] Loss: 0.5218995588073103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8271] Loss: 0.5219583423070695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8272] Loss: 0.5219354866453128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8273] Loss: 0.5219245510825818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8274] Loss: 0.5219499679269832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8275] Loss: 0.5219336661076591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8276] Loss: 0.5219078481565761\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8277] Loss: 0.5219738819937733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8278] Loss: 0.5219442123196152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8279] Loss: 0.5219456246114327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8280] Loss: 0.5219185105123422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8281] Loss: 0.5219594954240124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8282] Loss: 0.5219390272933125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8283] Loss: 0.5219871783546387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8284] Loss: 0.5219941270751637\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8285] Loss: 0.5219584580134732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8286] Loss: 0.521923995469486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8287] Loss: 0.5218964453120304\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8288] Loss: 0.5218646505645934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8289] Loss: 0.5219426837490341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8290] Loss: 0.521922328462944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8291] Loss: 0.5218901504029837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8292] Loss: 0.5218574462505883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8293] Loss: 0.5219023148925909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8294] Loss: 0.5219171252695103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8295] Loss: 0.5218826444966715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8296] Loss: 0.5218664943146087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8297] Loss: 0.5218410917105809\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8298] Loss: 0.5218404770762295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8299] Loss: 0.5218284532498573\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8300] Loss: 0.5218193739882176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8301] Loss: 0.521858322572194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8302] Loss: 0.5218219327715511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8303] Loss: 0.5218469319884468\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8304] Loss: 0.5218237981655245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8305] Loss: 0.5218658243829888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8306] Loss: 0.5218753478713926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8307] Loss: 0.5218529917421105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8308] Loss: 0.5218479763310394\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8309] Loss: 0.521827460974665\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8310] Loss: 0.5218096038278861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8311] Loss: 0.5218606607461034\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8312] Loss: 0.5219055111860196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8313] Loss: 0.5219544331209791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8314] Loss: 0.5219371577964448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8315] Loss: 0.521931368880055\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8316] Loss: 0.521912360852601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8317] Loss: 0.5218827907576732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8318] Loss: 0.5218568800902758\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8319] Loss: 0.5218212564183283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8320] Loss: 0.5217799452809609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8321] Loss: 0.5217490050841074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8322] Loss: 0.5217696095006593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8323] Loss: 0.5217511823013926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8324] Loss: 0.5217333517154373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8325] Loss: 0.521709941526826\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8326] Loss: 0.5217466614778205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8327] Loss: 0.5217158369213872\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8328] Loss: 0.5217027385087994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8329] Loss: 0.5217126688184694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8330] Loss: 0.5217274986283706\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8331] Loss: 0.5217354840431931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8332] Loss: 0.5217186152131105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8333] Loss: 0.5217305838996974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8334] Loss: 0.5217112420233451\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8335] Loss: 0.5216678872028478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8336] Loss: 0.521619992288305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8337] Loss: 0.5216047699056672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8338] Loss: 0.5216117340483164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8339] Loss: 0.5215982502867152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8340] Loss: 0.5215578744161784\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8341] Loss: 0.5216163334574314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8342] Loss: 0.5215934966752673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8343] Loss: 0.5215832687658505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8344] Loss: 0.5215580039884717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8345] Loss: 0.5215306090677172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8346] Loss: 0.5215024414028727\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8347] Loss: 0.5215127661099799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8348] Loss: 0.5214729790945812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8349] Loss: 0.5214266673040293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8350] Loss: 0.5213973939209721\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8351] Loss: 0.5213619243705657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8352] Loss: 0.5213926370310041\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8353] Loss: 0.5213522419236561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8354] Loss: 0.5213572516076149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8355] Loss: 0.5213842202421964\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8356] Loss: 0.5214368955569549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8357] Loss: 0.5215119314817345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8358] Loss: 0.5215078142664331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8359] Loss: 0.5214934550545628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8360] Loss: 0.521482993715508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8361] Loss: 0.5214434724684021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8362] Loss: 0.5214240073330294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8363] Loss: 0.5213878548564265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8364] Loss: 0.5213610087569992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8365] Loss: 0.5213333615066897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8366] Loss: 0.5213304128869609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8367] Loss: 0.5213039853963686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8368] Loss: 0.5213388511737996\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8369] Loss: 0.5213224201548842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8370] Loss: 0.5212885552273149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8371] Loss: 0.5212697093722748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8372] Loss: 0.5213231218775487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8373] Loss: 0.5212761127098241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8374] Loss: 0.5212401696201319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8375] Loss: 0.5212798113172704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8376] Loss: 0.521323382055035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8377] Loss: 0.5213583524722345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8378] Loss: 0.5213760856745445\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8379] Loss: 0.5213749068432348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8380] Loss: 0.5214484363491381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8381] Loss: 0.5214633115204843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8382] Loss: 0.5214404988337339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8383] Loss: 0.5214334308881553\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8384] Loss: 0.5214767403444542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8385] Loss: 0.5215733098908076\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8386] Loss: 0.5215494882158364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8387] Loss: 0.5215844820035175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8388] Loss: 0.5215718919445821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8389] Loss: 0.5216174730720186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8390] Loss: 0.5216157793923077\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8391] Loss: 0.5216007701425803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8392] Loss: 0.5215871910158374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8393] Loss: 0.5215479023570293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8394] Loss: 0.5215850159949242\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8395] Loss: 0.521571281520188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8396] Loss: 0.5216358240987636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8397] Loss: 0.5216296830262857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8398] Loss: 0.5216583302679035\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8399] Loss: 0.5217071599773859\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8400] Loss: 0.5216664601528146\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8401] Loss: 0.5217883623933448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8402] Loss: 0.5217609983400168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8403] Loss: 0.5217698952906428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8404] Loss: 0.5217521747199982\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8405] Loss: 0.5217347408413057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8406] Loss: 0.521748799268296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8407] Loss: 0.5217333169500934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8408] Loss: 0.5217842572578812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8409] Loss: 0.5217965211516952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8410] Loss: 0.5218109115618693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8411] Loss: 0.5218003137762555\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8412] Loss: 0.5217793872275855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8413] Loss: 0.5217678593628379\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8414] Loss: 0.521746187595281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8415] Loss: 0.5218014050480084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8416] Loss: 0.5218033684738491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8417] Loss: 0.5217805958863108\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8418] Loss: 0.5218162000684287\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8419] Loss: 0.5217971706423541\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8420] Loss: 0.5217568721165398\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8421] Loss: 0.521756073155546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8422] Loss: 0.5218084504859875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8423] Loss: 0.5217859781407593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8424] Loss: 0.5217736846559542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8425] Loss: 0.5217745872366225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8426] Loss: 0.5217605677667202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8427] Loss: 0.5217549803432557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8428] Loss: 0.521796528860667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8429] Loss: 0.5218345141121798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8430] Loss: 0.5218211678392959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8431] Loss: 0.5218076959483747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8432] Loss: 0.521765654626834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8433] Loss: 0.5217401762509999\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8434] Loss: 0.5217360096115287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8435] Loss: 0.521751653211726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8436] Loss: 0.5218010845845797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8437] Loss: 0.5217775296289683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8438] Loss: 0.5217590084380266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8439] Loss: 0.5217471413134867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8440] Loss: 0.521726952585266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8441] Loss: 0.5217024470402214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8442] Loss: 0.5217146007470933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8443] Loss: 0.5217806039046748\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8444] Loss: 0.5217826501237436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8445] Loss: 0.521797095538688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8446] Loss: 0.5218423454188591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8447] Loss: 0.5218521203196618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8448] Loss: 0.5218263296065507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8449] Loss: 0.5218238534269983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8450] Loss: 0.5218129517261868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8451] Loss: 0.5217808103692004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8452] Loss: 0.5218462178934374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8453] Loss: 0.5218228868231221\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8454] Loss: 0.5218692265435948\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8455] Loss: 0.5218655515609859\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8456] Loss: 0.5218409931002592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8457] Loss: 0.5218206941931446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8458] Loss: 0.521835623038103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8459] Loss: 0.5218834512705233\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8460] Loss: 0.5218890692940298\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8461] Loss: 0.5218600850407544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8462] Loss: 0.5218309401431434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8463] Loss: 0.5218524308785033\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8464] Loss: 0.5218215365190436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8465] Loss: 0.5218130059020464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8466] Loss: 0.5217881528890091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8467] Loss: 0.5217657367817883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8468] Loss: 0.5217555330967478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8469] Loss: 0.52171479423736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8470] Loss: 0.5217167970373403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8471] Loss: 0.521700293263693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8472] Loss: 0.5216696681583446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8473] Loss: 0.5216586604783632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8474] Loss: 0.5216474887722704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8475] Loss: 0.5216405978347997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8476] Loss: 0.5216241129006449\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8477] Loss: 0.521652442864766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8478] Loss: 0.5216625928841382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8479] Loss: 0.5216953631797598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8480] Loss: 0.52169086954824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8481] Loss: 0.5217257903594621\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8482] Loss: 0.5216915756763074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8483] Loss: 0.5217283199904899\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8484] Loss: 0.5217201159447096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8485] Loss: 0.5216987001297989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8486] Loss: 0.5216872581433031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8487] Loss: 0.5217048648501512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8488] Loss: 0.5216838150327443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8489] Loss: 0.5216816388017358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8490] Loss: 0.5217366086734566\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8491] Loss: 0.5217855800633562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8492] Loss: 0.5217654355387435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8493] Loss: 0.521774139964131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8494] Loss: 0.5218037789527006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8495] Loss: 0.5217897591773017\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8496] Loss: 0.5217613289970041\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8497] Loss: 0.5218013434785022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8498] Loss: 0.5217772432205137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8499] Loss: 0.5218082939389203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8500] Loss: 0.5218009043172542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8501] Loss: 0.5218127880074357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8502] Loss: 0.5218359412507968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8503] Loss: 0.5218227444237349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8504] Loss: 0.521799620379798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8505] Loss: 0.521777782580344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8506] Loss: 0.5217563997378285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8507] Loss: 0.521713742121497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8508] Loss: 0.5217016854973905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8509] Loss: 0.521765004930563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8510] Loss: 0.521758924394846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8511] Loss: 0.5217835632745452\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8147\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8512] Loss: 0.5217562942870168\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8513] Loss: 0.5217353801913192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8514] Loss: 0.5217786992209843\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8515] Loss: 0.5217386511844966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8516] Loss: 0.5217247853018837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8517] Loss: 0.521726728520382\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8518] Loss: 0.521726411874299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8519] Loss: 0.5218011497826087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8520] Loss: 0.5218451033147533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8521] Loss: 0.5218348919251575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8522] Loss: 0.5218136461002495\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8523] Loss: 0.5218054840854234\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8524] Loss: 0.5218141012829023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8525] Loss: 0.5218788959276505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8526] Loss: 0.5218869367997327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8527] Loss: 0.5218641331208619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8528] Loss: 0.5218992773568907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8529] Loss: 0.5218901608052404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8530] Loss: 0.5219058106813644\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8531] Loss: 0.5219167295112795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8532] Loss: 0.5219329840983574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8533] Loss: 0.5218995437722513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8534] Loss: 0.5219528572643148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8535] Loss: 0.5219785799118589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8536] Loss: 0.5220008100799096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8537] Loss: 0.5219838342037536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8538] Loss: 0.5220539310068174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8539] Loss: 0.5220352713821804\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8540] Loss: 0.5220285658187617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8541] Loss: 0.5220475872483857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8542] Loss: 0.5221122758819228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8543] Loss: 0.5221385341325465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8544] Loss: 0.5221283779286436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8545] Loss: 0.5221093546884283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8546] Loss: 0.5221578141590029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8547] Loss: 0.5221283806116409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8548] Loss: 0.5221745296256048\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8549] Loss: 0.5221422494518059\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8550] Loss: 0.5221394322516016\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8551] Loss: 0.5221700630271305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8552] Loss: 0.5221388630607893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8553] Loss: 0.522177607899324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8554] Loss: 0.522161778471975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8555] Loss: 0.5221659118192695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8556] Loss: 0.5221553188323856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8557] Loss: 0.5222431991105349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8558] Loss: 0.5222290495473514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8559] Loss: 0.5222186415754412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8560] Loss: 0.5221847917148785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8561] Loss: 0.5221809249900169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8562] Loss: 0.5221918996162546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8563] Loss: 0.522154951976735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8564] Loss: 0.5221456417636745\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8565] Loss: 0.5221200828020207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8566] Loss: 0.5221183087691561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8567] Loss: 0.5221079099806394\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8568] Loss: 0.5221663726973131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8569] Loss: 0.5221583501589862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8570] Loss: 0.5221930785432791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8571] Loss: 0.5221858123254279\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8572] Loss: 0.5221751627847977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8573] Loss: 0.5221508444889477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8574] Loss: 0.5221306183763469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8575] Loss: 0.5221662661653088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8576] Loss: 0.5221265817627758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8577] Loss: 0.5221637855867508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8578] Loss: 0.5221993188305869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8579] Loss: 0.5221771016386395\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8580] Loss: 0.5221926991768929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8581] Loss: 0.5221821500505158\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8582] Loss: 0.522159153732173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8583] Loss: 0.5221229973711164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8584] Loss: 0.5220970851058915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8585] Loss: 0.5220754529042141\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8586] Loss: 0.5220454149593984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8587] Loss: 0.5220351491076854\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8588] Loss: 0.5220084632754945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8589] Loss: 0.5220353061137097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8590] Loss: 0.5220413466897046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8591] Loss: 0.5220913109640986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8592] Loss: 0.522068173257439\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8593] Loss: 0.522116865226719\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8594] Loss: 0.5221693703682632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8595] Loss: 0.5222207275414511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8596] Loss: 0.522183584235931\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8597] Loss: 0.5222419939591562\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8598] Loss: 0.5222301815298379\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8599] Loss: 0.5222146222486471\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8600] Loss: 0.5221884739118984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8601] Loss: 0.5221516868580023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8602] Loss: 0.5221328828990224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8603] Loss: 0.5221107021189585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8604] Loss: 0.5221429073905574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8605] Loss: 0.5221099511994632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8606] Loss: 0.5221118624322116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8607] Loss: 0.5220980529959914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8608] Loss: 0.5221086160474632\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8609] Loss: 0.5221333203584062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8610] Loss: 0.5220957499245803\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8611] Loss: 0.5221193461300576\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8612] Loss: 0.5220810269053645\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8613] Loss: 0.5220732743833796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8614] Loss: 0.5220489401649776\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8615] Loss: 0.5220624775098915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8616] Loss: 0.5220795057248898\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8617] Loss: 0.5221349739040975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8618] Loss: 0.5221107635880463\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8619] Loss: 0.5221152461083018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8620] Loss: 0.5220953926260316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8621] Loss: 0.5220645989737225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8622] Loss: 0.5220980636125221\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8623] Loss: 0.5220953941135831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8624] Loss: 0.5221332517223667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8625] Loss: 0.5221121034446647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8626] Loss: 0.5221648553359708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8627] Loss: 0.5221522422891776\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8628] Loss: 0.5221969369109642\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8629] Loss: 0.5221787796226507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8630] Loss: 0.5222169573946392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8631] Loss: 0.522200067821519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8632] Loss: 0.5222170118520454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8633] Loss: 0.5221932307683701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8634] Loss: 0.5221870215907729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8635] Loss: 0.5221650540223488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8636] Loss: 0.522210804578224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8637] Loss: 0.5221866305788695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8638] Loss: 0.5221461127218827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8639] Loss: 0.5221268729114169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8640] Loss: 0.5221354635219293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8641] Loss: 0.5221882170745916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8642] Loss: 0.52222541976579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8643] Loss: 0.5222249507728163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8644] Loss: 0.5222313825727125\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8645] Loss: 0.5222715594465189\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8646] Loss: 0.5222693828677241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8647] Loss: 0.5222573571581944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8648] Loss: 0.5222546833684738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8649] Loss: 0.5222317088600869\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8650] Loss: 0.5221949062857054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8651] Loss: 0.5221599677778729\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8652] Loss: 0.5221301414103626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8653] Loss: 0.5220925361485325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8654] Loss: 0.5220828608322472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8655] Loss: 0.5221420859031841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8656] Loss: 0.5222405113138966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8657] Loss: 0.5222346825483782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8658] Loss: 0.5222165717299393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8659] Loss: 0.5222088581492585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8660] Loss: 0.5221712587508687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8661] Loss: 0.5222236515407752\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8662] Loss: 0.5222118433722102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8663] Loss: 0.5221814332557546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8664] Loss: 0.5221528641026132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8665] Loss: 0.5221181684699546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8666] Loss: 0.5220865839694925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8667] Loss: 0.5220513786625977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8668] Loss: 0.5220941983037324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8669] Loss: 0.5220668698945369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8670] Loss: 0.5220579743969674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8671] Loss: 0.5220403164249032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8672] Loss: 0.5220519079644433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8673] Loss: 0.522061888353506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8674] Loss: 0.5220639409678991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8675] Loss: 0.5220517613080405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8676] Loss: 0.5220212733485726\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8677] Loss: 0.5220211292037156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8678] Loss: 0.5219790997908584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8679] Loss: 0.5219790079716531\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8680] Loss: 0.5219739402084631\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8681] Loss: 0.5219577474804786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8682] Loss: 0.5219335796104537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8683] Loss: 0.5219415846287249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8684] Loss: 0.5219767202855848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8685] Loss: 0.5219692615522157\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8686] Loss: 0.5219774115320346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8687] Loss: 0.521959062367295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8688] Loss: 0.5219409337295067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8689] Loss: 0.5219321288118328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8690] Loss: 0.5219229222692021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8691] Loss: 0.5219209396836227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8692] Loss: 0.5219199537495126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8693] Loss: 0.5219226559398956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8694] Loss: 0.5219067884017353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8695] Loss: 0.5218833361920163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8696] Loss: 0.5219434109936709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8697] Loss: 0.5219199907922389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8698] Loss: 0.5219475810862367\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8699] Loss: 0.5219423023062408\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8700] Loss: 0.5219104149802991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8701] Loss: 0.5219420693926636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8702] Loss: 0.5219820171369065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8703] Loss: 0.5219793818662793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8704] Loss: 0.5219569337973223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8705] Loss: 0.5219748587270857\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8706] Loss: 0.5219694214798928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8707] Loss: 0.5219296657346117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8708] Loss: 0.5219320108571934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8709] Loss: 0.5219032883422376\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8710] Loss: 0.5219530956392608\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8711] Loss: 0.5219075024680326\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8712] Loss: 0.5219694269638008\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8713] Loss: 0.5219875938112265\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8714] Loss: 0.521959030137069\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8715] Loss: 0.5219314906162724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8716] Loss: 0.5219056194885574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8717] Loss: 0.5218803969097149\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8718] Loss: 0.521891149301917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8719] Loss: 0.5218978758516984\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8720] Loss: 0.5218739379519521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8721] Loss: 0.5218606781631422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8722] Loss: 0.5218190639644963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8723] Loss: 0.5217909585699506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8724] Loss: 0.5217673354465827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8725] Loss: 0.5217837084740019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8726] Loss: 0.5217927482846069\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8727] Loss: 0.5218469749637891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8728] Loss: 0.5219113738193335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8729] Loss: 0.521946815036628\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8730] Loss: 0.5219263021745821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8731] Loss: 0.5219120599502572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8732] Loss: 0.5219393235993078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8733] Loss: 0.5219388992793946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8734] Loss: 0.5219062072648902\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8735] Loss: 0.5218984306438353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8736] Loss: 0.5219170073255147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8737] Loss: 0.5219027826856173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8738] Loss: 0.5218753158415934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8739] Loss: 0.5218358472132686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8740] Loss: 0.5218927575562733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8741] Loss: 0.5219406223705538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8742] Loss: 0.5219226808470934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8743] Loss: 0.5219308872436038\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8744] Loss: 0.521979764896543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8745] Loss: 0.5220118846253474\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8746] Loss: 0.5219894368427653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8747] Loss: 0.5219626806235884\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8748] Loss: 0.5219564875386589\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8749] Loss: 0.5219519131065558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8750] Loss: 0.5219590032816801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8751] Loss: 0.5219441351598653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8752] Loss: 0.5219175681685163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8753] Loss: 0.5219521817291273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8754] Loss: 0.5219352253030828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8755] Loss: 0.5219422073021595\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8756] Loss: 0.5219314696536953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8757] Loss: 0.5219220528895167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8758] Loss: 0.5219037883836418\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8759] Loss: 0.5219298188672498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8760] Loss: 0.5220146890950925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8761] Loss: 0.5220084290347841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8762] Loss: 0.521996549958606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8763] Loss: 0.5219818681864887\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8764] Loss: 0.5219788307033846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8765] Loss: 0.5219523279432958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8766] Loss: 0.521936453319364\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8767] Loss: 0.521965976952929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8768] Loss: 0.5219452155938475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8769] Loss: 0.5219758834528828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8770] Loss: 0.5220128338432196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8771] Loss: 0.5219950608971361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8772] Loss: 0.521988551420728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8773] Loss: 0.5219730249685308\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8774] Loss: 0.521995151705773\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8775] Loss: 0.5219653657870227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8776] Loss: 0.5219693250779289\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8777] Loss: 0.5219830757540314\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8778] Loss: 0.5220342029226709\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8779] Loss: 0.522025552726827\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8780] Loss: 0.5220150515238925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8781] Loss: 0.5220247245320615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8782] Loss: 0.5220790708787033\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8783] Loss: 0.5220852795216966\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8784] Loss: 0.5220621382344424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8785] Loss: 0.5220789068307041\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8786] Loss: 0.5220710968534805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8787] Loss: 0.5220481979324144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8788] Loss: 0.522086458740156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8789] Loss: 0.5220730790468154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8790] Loss: 0.5220638470301305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8791] Loss: 0.5220375748961001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8792] Loss: 0.5220684149585695\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8793] Loss: 0.522126016255725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8794] Loss: 0.5220842667891747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8795] Loss: 0.5221184847868561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8796] Loss: 0.5220859442252397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8797] Loss: 0.5220569861018645\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8798] Loss: 0.5220918248094705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8799] Loss: 0.5220786007699841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8800] Loss: 0.5220846336123739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8801] Loss: 0.5220646674965737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8802] Loss: 0.5220403072846092\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8803] Loss: 0.5220551053865042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8804] Loss: 0.5221130508513574\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8805] Loss: 0.5220944282770014\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8806] Loss: 0.5220639357018206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8807] Loss: 0.522079638135538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8808] Loss: 0.5221663292135552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8809] Loss: 0.5221457342804735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8810] Loss: 0.5221343095295401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8811] Loss: 0.5222063350032689\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8812] Loss: 0.5221829164914125\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8813] Loss: 0.5221778499201437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8814] Loss: 0.5221726888672292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8815] Loss: 0.5221354887795548\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8816] Loss: 0.5221358494366217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8817] Loss: 0.522163805916979\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8818] Loss: 0.5221549237239343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8819] Loss: 0.5221200288561567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8820] Loss: 0.5221399270921551\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8821] Loss: 0.5221337335496413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8822] Loss: 0.522159484058217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8823] Loss: 0.522120633574724\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8824] Loss: 0.5221043004205601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8825] Loss: 0.5221427395912945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8826] Loss: 0.5221331834420383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8827] Loss: 0.5221298827035102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8828] Loss: 0.5221131808534497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8829] Loss: 0.5221717480835626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8830] Loss: 0.5221375048697855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8831] Loss: 0.5221302307772387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8832] Loss: 0.5221106239205741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8833] Loss: 0.5220874703361569\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8834] Loss: 0.5220644688667844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8835] Loss: 0.5220662419108657\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8836] Loss: 0.5220719480418597\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8837] Loss: 0.5220982183517636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8838] Loss: 0.5221290990216416\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8839] Loss: 0.5220962909106976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8840] Loss: 0.5220796568124664\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8841] Loss: 0.5220589190417443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8842] Loss: 0.522021600054653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8843] Loss: 0.5220914534284733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8844] Loss: 0.5220891306418608\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8845] Loss: 0.522130592261236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8846] Loss: 0.5221033916592369\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8847] Loss: 0.52209364740886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8848] Loss: 0.5220866538503811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8849] Loss: 0.5220563452954858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8850] Loss: 0.5220729769634114\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8851] Loss: 0.5220572517267192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8852] Loss: 0.5220311636462883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8853] Loss: 0.5220309719398669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8854] Loss: 0.522025346273801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8855] Loss: 0.5220647805782761\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8856] Loss: 0.5220523949631278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8857] Loss: 0.5220208671138117\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8858] Loss: 0.5220648221632311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8859] Loss: 0.5220922258053301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8860] Loss: 0.5220863099405152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8861] Loss: 0.5220448540193765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8862] Loss: 0.5220499911998537\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8863] Loss: 0.5220120489654406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8864] Loss: 0.5220596407349953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8865] Loss: 0.5220312471021655\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8866] Loss: 0.5220455234061593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8867] Loss: 0.5220134355947135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8868] Loss: 0.5220837503044079\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8869] Loss: 0.5221361779492072\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8870] Loss: 0.5221401205010107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8871] Loss: 0.5221165861685357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8872] Loss: 0.5220853883565484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8873] Loss: 0.5220575944617307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8874] Loss: 0.5220293216923958\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8875] Loss: 0.522063247197699\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8876] Loss: 0.5220338249289189\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8877] Loss: 0.5220108621505422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8878] Loss: 0.5220172536732134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8879] Loss: 0.5219846757573742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8880] Loss: 0.5219823468418674\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8881] Loss: 0.5219791218640201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8882] Loss: 0.5220015645899302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8883] Loss: 0.5219968716655882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8884] Loss: 0.5220170537722062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8885] Loss: 0.522037150737065\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8886] Loss: 0.5220082907242756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8887] Loss: 0.5219840283690282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8888] Loss: 0.5220316207015477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8889] Loss: 0.5220742303949649\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8890] Loss: 0.5220464928533552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8891] Loss: 0.5220204672310891\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8892] Loss: 0.5219950556090185\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8893] Loss: 0.5219982701515818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8894] Loss: 0.5219887769478907\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8895] Loss: 0.522035059265317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8896] Loss: 0.5220085045221936\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8897] Loss: 0.521991965790123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8898] Loss: 0.5219956370826472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8899] Loss: 0.5220372074070839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8900] Loss: 0.5220347491737912\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8901] Loss: 0.5219959485265896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8902] Loss: 0.522079374326892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8903] Loss: 0.5220887689404815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8904] Loss: 0.5221501581596283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8905] Loss: 0.5221436797205787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8906] Loss: 0.5221442919723217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8907] Loss: 0.5221951999993811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8908] Loss: 0.5222118218052555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8909] Loss: 0.5222074193821193\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8910] Loss: 0.5221733443811536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8911] Loss: 0.5221982835861292\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8912] Loss: 0.5222212754259136\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8913] Loss: 0.5221858603158736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8914] Loss: 0.5222125740057795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8915] Loss: 0.5221795332655178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8916] Loss: 0.5222148449629255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8917] Loss: 0.5221948015575703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8918] Loss: 0.5221975438534278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8919] Loss: 0.5221597061955783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8920] Loss: 0.5221614889260138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8921] Loss: 0.5221220171791012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8922] Loss: 0.5221351315694739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8923] Loss: 0.5221331088240133\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8924] Loss: 0.5221461256032237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8925] Loss: 0.5221429659030442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8926] Loss: 0.5221680117852476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8927] Loss: 0.5222287019220118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8928] Loss: 0.5222075986615069\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8929] Loss: 0.5222397670831231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8930] Loss: 0.5222777017345762\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8931] Loss: 0.5222621391655756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8932] Loss: 0.5222359704284112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8933] Loss: 0.5221956028594946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8934] Loss: 0.5221627666725506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8935] Loss: 0.5221366716209435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8936] Loss: 0.5221476152861115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8937] Loss: 0.5221275294592361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8938] Loss: 0.5221010247076582\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8939] Loss: 0.5220861753784956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8940] Loss: 0.5221145316587225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8941] Loss: 0.5220880062254054\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8942] Loss: 0.5220680054510454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8943] Loss: 0.5220532793457591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8944] Loss: 0.5220658539920049\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8945] Loss: 0.522113198328103\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8946] Loss: 0.5220957235130669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8947] Loss: 0.5220875761951755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8948] Loss: 0.5220543264770542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8949] Loss: 0.522024071072498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8950] Loss: 0.5220314283365322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8951] Loss: 0.5220133735026077\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8952] Loss: 0.5220375654398618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8953] Loss: 0.5220036806811601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8954] Loss: 0.521966080099683\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8955] Loss: 0.5219912033433248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8956] Loss: 0.5220172350532405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8957] Loss: 0.5220238526936211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8958] Loss: 0.5219901888684201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8959] Loss: 0.5220433548560212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8960] Loss: 0.5220108700539233\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8961] Loss: 0.5220031511704607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8962] Loss: 0.5220120661366172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8963] Loss: 0.5219998312330691\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8964] Loss: 0.5220302003782805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8965] Loss: 0.5220476680257348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8966] Loss: 0.522079300593546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8967] Loss: 0.5220534520516007\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8968] Loss: 0.5220648036108256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8969] Loss: 0.5220902451902067\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8970] Loss: 0.522067946597194\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8971] Loss: 0.5220804168616975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8972] Loss: 0.5220751797186514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8973] Loss: 0.5220593251362768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8974] Loss: 0.5220414610836648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8975] Loss: 0.522081880747603\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8976] Loss: 0.5220654854291781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8977] Loss: 0.5220476140291136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8978] Loss: 0.522023054891225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8979] Loss: 0.5219977850375737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8980] Loss: 0.5219850453041239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8981] Loss: 0.5219528066367527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8982] Loss: 0.5219525331675274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8983] Loss: 0.5219254639436289\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8984] Loss: 0.5219801348498443\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8985] Loss: 0.522020460043685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8986] Loss: 0.5220369142447231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8987] Loss: 0.5220053687127424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8988] Loss: 0.5220100133205028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8989] Loss: 0.5220481434518232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8990] Loss: 0.5220247898075097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8991] Loss: 0.5220610254209512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8992] Loss: 0.5220747088428883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8993] Loss: 0.5220715174813547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8994] Loss: 0.5220432573791552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8995] Loss: 0.5220933896447183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8996] Loss: 0.5220951576566404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8997] Loss: 0.5220864596749779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8998] Loss: 0.5221231106096399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 8999] Loss: 0.5221351031834953\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9000] Loss: 0.5221669333597516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9001] Loss: 0.522206830772715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9002] Loss: 0.5221814293795131\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9003] Loss: 0.5222193523934406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9004] Loss: 0.5222233883319025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9005] Loss: 0.5221888197282261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9006] Loss: 0.5221934493126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9007] Loss: 0.5221875940363057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9008] Loss: 0.5222123390395491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9009] Loss: 0.5222530767123466\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9010] Loss: 0.5222341109619422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9011] Loss: 0.522265927807161\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under the ROC Curve: 0.8015000000000001\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9012] Loss: 0.5222912548998052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9013] Loss: 0.5223142982727917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9014] Loss: 0.522312345093612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9015] Loss: 0.5223118083922741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9016] Loss: 0.5223306413996565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9017] Loss: 0.5223079967564641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9018] Loss: 0.5222931905887359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9019] Loss: 0.5223458745824505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9020] Loss: 0.5223937415698441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9021] Loss: 0.5224087098737032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9022] Loss: 0.5223807992791771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9023] Loss: 0.5224046522311006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9024] Loss: 0.5224353523357446\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9025] Loss: 0.5224482261217558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9026] Loss: 0.5224470670876116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9027] Loss: 0.5224405829966929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9028] Loss: 0.5224259137358568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9029] Loss: 0.5224030248568027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9030] Loss: 0.5224473741797494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9031] Loss: 0.5224668565434115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9032] Loss: 0.5225116533835542\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9033] Loss: 0.522500946752401\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9034] Loss: 0.5225079644703909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9035] Loss: 0.5225298373475452\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9036] Loss: 0.5225441605971225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9037] Loss: 0.5225200766700365\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9038] Loss: 0.5225290993287963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9039] Loss: 0.522513906066646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9040] Loss: 0.5224867670889616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9041] Loss: 0.5224665007981392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9042] Loss: 0.5224759740850147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9043] Loss: 0.522525336745754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9044] Loss: 0.5224923413951419\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9045] Loss: 0.5224998823717873\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9046] Loss: 0.5225803411191272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9047] Loss: 0.5226027420816313\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9048] Loss: 0.5225718101396246\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9049] Loss: 0.5226339100953747\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9050] Loss: 0.5226245840588275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9051] Loss: 0.5226676386792268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9052] Loss: 0.5226564577189075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9053] Loss: 0.5226457090172962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9054] Loss: 0.5226686732697018\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9055] Loss: 0.5226503871490273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9056] Loss: 0.5226117981584268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9057] Loss: 0.5225722779565294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9058] Loss: 0.5225706793346944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9059] Loss: 0.5225386196691427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9060] Loss: 0.5225434402049633\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9061] Loss: 0.5225758340561296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9062] Loss: 0.5226612182977017\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9063] Loss: 0.5226811284292348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9064] Loss: 0.5227073484372808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9065] Loss: 0.5228310585701278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9066] Loss: 0.5228233490402228\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9067] Loss: 0.5228628786340612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9068] Loss: 0.5228848490715167\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9069] Loss: 0.5228583876346926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9070] Loss: 0.5228297033094775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9071] Loss: 0.5228737371764497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9072] Loss: 0.5228701252016028\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9073] Loss: 0.522886336662426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9074] Loss: 0.5228637778667828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9075] Loss: 0.522838380278653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9076] Loss: 0.5228767380354952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9077] Loss: 0.5229091900130672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9078] Loss: 0.5228959391629245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9079] Loss: 0.522892830250199\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9080] Loss: 0.522862838448624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9081] Loss: 0.5228464734041764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9082] Loss: 0.5228556833980493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9083] Loss: 0.5228792026154041\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9084] Loss: 0.5228488320167147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9085] Loss: 0.5228125030740705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9086] Loss: 0.5227942145863937\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9087] Loss: 0.5228333875243308\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9088] Loss: 0.5228482722475218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9089] Loss: 0.5229342403178004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9090] Loss: 0.5229101356179703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9091] Loss: 0.5228709620329733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9092] Loss: 0.5228698572907462\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9093] Loss: 0.5228408133387774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9094] Loss: 0.5228234559005926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9095] Loss: 0.522799536535472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9096] Loss: 0.5227741371556481\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9097] Loss: 0.5228066360789433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9098] Loss: 0.5227991412851385\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9099] Loss: 0.5228188575209333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9100] Loss: 0.5228136402070175\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9101] Loss: 0.5227876201787159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9102] Loss: 0.5227696585713485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9103] Loss: 0.5227470254157262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9104] Loss: 0.5227298610621561\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9105] Loss: 0.5227183011085783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9106] Loss: 0.5227395479882413\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9107] Loss: 0.5227248343122661\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9108] Loss: 0.5227557307628732\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9109] Loss: 0.5227352169685994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9110] Loss: 0.5227748605123786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9111] Loss: 0.5227439911396555\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9112] Loss: 0.5227149890740509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9113] Loss: 0.5226883304911714\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9114] Loss: 0.5226654160381583\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9115] Loss: 0.5226330458113512\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9116] Loss: 0.5226057436392525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9117] Loss: 0.5225932738209513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9118] Loss: 0.5225633551404711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9119] Loss: 0.5225584214884917\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9120] Loss: 0.5225653707634973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9121] Loss: 0.5225322777462703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9122] Loss: 0.5225186811952994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9123] Loss: 0.5225379039404437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9124] Loss: 0.5225610948574911\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9125] Loss: 0.5226230453836039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9126] Loss: 0.5226055503292779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9127] Loss: 0.5226661064750124\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9128] Loss: 0.5226904988690024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9129] Loss: 0.5226569841163647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9130] Loss: 0.5226594342605974\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9131] Loss: 0.5226742026597355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9132] Loss: 0.5226583271610352\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9133] Loss: 0.5226866110915667\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9134] Loss: 0.5226593664285756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9135] Loss: 0.5226977305412293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9136] Loss: 0.5227241928978749\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9137] Loss: 0.5227114416376469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9138] Loss: 0.5226862403906547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9139] Loss: 0.522686333821538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9140] Loss: 0.5227081070769565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9141] Loss: 0.5227148963310893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9142] Loss: 0.522686448649083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9143] Loss: 0.5226722976865352\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9144] Loss: 0.5226364175385421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9145] Loss: 0.522620350534887\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9146] Loss: 0.5226067129044607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9147] Loss: 0.5226160746313182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9148] Loss: 0.5226135756678122\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9149] Loss: 0.5226527507406016\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9150] Loss: 0.5226812800064821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9151] Loss: 0.5227451021800837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9152] Loss: 0.5227309368452494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9153] Loss: 0.5227752559306816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9154] Loss: 0.5227916324144847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9155] Loss: 0.5227534401496068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9156] Loss: 0.5227536840255327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9157] Loss: 0.5227164924265374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9158] Loss: 0.522697596365343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9159] Loss: 0.5226571217932195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9160] Loss: 0.5226667936947305\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9161] Loss: 0.522681404654862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9162] Loss: 0.5226812778343026\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9163] Loss: 0.5226822346380016\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9164] Loss: 0.522714661061075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9165] Loss: 0.5226821303092145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9166] Loss: 0.5227115188076293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9167] Loss: 0.5227384695186618\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9168] Loss: 0.5227402508754492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9169] Loss: 0.5227194627825885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9170] Loss: 0.5227652419556371\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9171] Loss: 0.5227566384860377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9172] Loss: 0.5227318119986232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9173] Loss: 0.5227115155711235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9174] Loss: 0.5227444396661259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9175] Loss: 0.5227776623914094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9176] Loss: 0.5228026052242665\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9177] Loss: 0.5227976411330022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9178] Loss: 0.5227816224744587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9179] Loss: 0.5227437302313788\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9180] Loss: 0.5227502209608118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9181] Loss: 0.5227443007893002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9182] Loss: 0.5227356512491279\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9183] Loss: 0.5227013116378788\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9184] Loss: 0.5226734923909605\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9185] Loss: 0.5227192340597983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9186] Loss: 0.5226963708935491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9187] Loss: 0.5226813547551061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9188] Loss: 0.5227241753213113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9189] Loss: 0.5226833916868341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9190] Loss: 0.5226726599312324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9191] Loss: 0.5226966057411744\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9192] Loss: 0.5227521821921719\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9193] Loss: 0.5227209202851704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9194] Loss: 0.522705983001617\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9195] Loss: 0.5226812462149232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9196] Loss: 0.5226647856081518\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9197] Loss: 0.52263605821403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9198] Loss: 0.5226242643254457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9199] Loss: 0.5226030261651846\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9200] Loss: 0.5225810136633172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9201] Loss: 0.522573538643213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9202] Loss: 0.522555918492346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9203] Loss: 0.5225414257699295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9204] Loss: 0.5225329720202001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9205] Loss: 0.5224976612960833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9206] Loss: 0.5225064135135152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9207] Loss: 0.5225379804201313\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9208] Loss: 0.5225737362270877\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9209] Loss: 0.5225765380465802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9210] Loss: 0.5225530569854824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9211] Loss: 0.5225865827553738\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9212] Loss: 0.5225804763994006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9213] Loss: 0.5225723177780163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9214] Loss: 0.5225450268857564\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9215] Loss: 0.5225405851326613\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9216] Loss: 0.5225343391412213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9217] Loss: 0.5225365427847337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9218] Loss: 0.5225356327909694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9219] Loss: 0.522575323385135\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9220] Loss: 0.5225581499314883\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9221] Loss: 0.5225370537945959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9222] Loss: 0.5225034087530318\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9223] Loss: 0.522489538831209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9224] Loss: 0.5224643120555421\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9225] Loss: 0.5224458181628946\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9226] Loss: 0.5224431369294673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9227] Loss: 0.5224797673625138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9228] Loss: 0.5224403020085941\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9229] Loss: 0.5224790924311723\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9230] Loss: 0.522469043295933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9231] Loss: 0.5224471873437274\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9232] Loss: 0.5224867197361405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9233] Loss: 0.5224786996175209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9234] Loss: 0.5224943421883547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9235] Loss: 0.522514595652378\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9236] Loss: 0.5225103769349774\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9237] Loss: 0.5225686026788104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9238] Loss: 0.5225616351476812\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9239] Loss: 0.5225566139570393\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9240] Loss: 0.5225247244125516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9241] Loss: 0.5225117515570764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9242] Loss: 0.5224833773262202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9243] Loss: 0.522496425191261\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9244] Loss: 0.5225460481300408\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9245] Loss: 0.5225406833713779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9246] Loss: 0.5225261935278528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9247] Loss: 0.5225073328989791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9248] Loss: 0.5224921179683374\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9249] Loss: 0.522533687763662\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9250] Loss: 0.5225123081637602\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9251] Loss: 0.5224767146179679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9252] Loss: 0.5225136899543801\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9253] Loss: 0.5224864716359002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9254] Loss: 0.522470192602015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9255] Loss: 0.5224560313408139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9256] Loss: 0.5224703325322011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9257] Loss: 0.5224586124367423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9258] Loss: 0.522461450246095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9259] Loss: 0.5225156729872505\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9260] Loss: 0.5225332742163113\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9261] Loss: 0.5225702456090399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9262] Loss: 0.5225953626066746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9263] Loss: 0.522574672469286\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9264] Loss: 0.5225534324272735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9265] Loss: 0.5225466757127722\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9266] Loss: 0.5225438223347619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9267] Loss: 0.5225584841817975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9268] Loss: 0.5226223737419972\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9269] Loss: 0.5225923087552933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9270] Loss: 0.5225853347495947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9271] Loss: 0.5226012080560848\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9272] Loss: 0.5225961055879559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9273] Loss: 0.522589938906615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9274] Loss: 0.5225731638561558\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9275] Loss: 0.5226302200276989\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9276] Loss: 0.5226618045324363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9277] Loss: 0.5226739181380485\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9278] Loss: 0.5227109778138815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9279] Loss: 0.5226946238397342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9280] Loss: 0.5227458204806462\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9281] Loss: 0.5228048793427037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9282] Loss: 0.5228107008901269\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9283] Loss: 0.5227802927340095\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9284] Loss: 0.5227793519609552\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9285] Loss: 0.5227870999662965\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9286] Loss: 0.5227869350069229\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9287] Loss: 0.522756974594501\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9288] Loss: 0.5227967121469238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9289] Loss: 0.5227710373978881\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9290] Loss: 0.5227654651333503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9291] Loss: 0.5227511300050022\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9292] Loss: 0.5227838223747039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9293] Loss: 0.5227782908281188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9294] Loss: 0.5227935772053287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9295] Loss: 0.5227642812526015\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9296] Loss: 0.5227538139771118\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9297] Loss: 0.5227859728932855\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9298] Loss: 0.5227679511380429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9299] Loss: 0.5227593476674787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9300] Loss: 0.5227821633524862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9301] Loss: 0.5227913147756202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9302] Loss: 0.5227661894192009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9303] Loss: 0.5227364533731559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9304] Loss: 0.522702717209353\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9305] Loss: 0.5227321156407027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9306] Loss: 0.5227105519283273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9307] Loss: 0.5227284254225679\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9308] Loss: 0.5227031914716717\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9309] Loss: 0.522758192839047\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9310] Loss: 0.5227376807599583\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9311] Loss: 0.5227208071598992\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9312] Loss: 0.5226911812279204\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9313] Loss: 0.5226926515468153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9314] Loss: 0.5226795546074094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9315] Loss: 0.522681288051375\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9316] Loss: 0.5226531651343997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9317] Loss: 0.5226659854252005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9318] Loss: 0.522710303656696\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9319] Loss: 0.5227637918210758\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9320] Loss: 0.5227312330017025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9321] Loss: 0.5227412935851611\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9322] Loss: 0.522701280591464\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9323] Loss: 0.5227513087501892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9324] Loss: 0.5227993946883422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9325] Loss: 0.522772550701216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9326] Loss: 0.5227619127121952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9327] Loss: 0.5228230986916128\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9328] Loss: 0.5228292971957592\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9329] Loss: 0.5228313961115834\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9330] Loss: 0.5228076752722939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9331] Loss: 0.5227878424041789\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9332] Loss: 0.5228279945133387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9333] Loss: 0.5227951596825757\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9334] Loss: 0.5227710762291687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9335] Loss: 0.5228490882926217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9336] Loss: 0.5228238935720994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9337] Loss: 0.5228030065609912\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9338] Loss: 0.5227727216120267\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9339] Loss: 0.5227707803877203\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9340] Loss: 0.5227416461618408\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9341] Loss: 0.5227265436197648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9342] Loss: 0.5227661047373777\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9343] Loss: 0.5227716407297585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9344] Loss: 0.5227566561211496\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9345] Loss: 0.5227480994049412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9346] Loss: 0.5227493708417824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9347] Loss: 0.5227357194011427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9348] Loss: 0.5228151137990956\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9349] Loss: 0.5227897408011543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9350] Loss: 0.5227750607340584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9351] Loss: 0.5227553741908454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9352] Loss: 0.5228073205230086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9353] Loss: 0.5228416014281142\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9354] Loss: 0.5228825702029943\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9355] Loss: 0.5228738932234881\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9356] Loss: 0.5228848437863509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9357] Loss: 0.5228900287371511\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9358] Loss: 0.5228611881091895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9359] Loss: 0.5228237743559012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9360] Loss: 0.522802075571933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9361] Loss: 0.5227813796025456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9362] Loss: 0.5227707354457088\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9363] Loss: 0.5227454816347109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9364] Loss: 0.5227729572765386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9365] Loss: 0.5227549625816216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9366] Loss: 0.5227327755069345\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9367] Loss: 0.5227737785045188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9368] Loss: 0.5227588135862652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9369] Loss: 0.5227546043267624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9370] Loss: 0.5227802657616865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9371] Loss: 0.5227579751014441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9372] Loss: 0.5227379903678188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9373] Loss: 0.522798750573513\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9374] Loss: 0.5227968351518743\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9375] Loss: 0.5228695648979212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9376] Loss: 0.5228631267785865\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9377] Loss: 0.5228823576380062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9378] Loss: 0.5228717370672058\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9379] Loss: 0.5229163260857219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9380] Loss: 0.5228896648296509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9381] Loss: 0.5228597126298955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9382] Loss: 0.5228347560446256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9383] Loss: 0.522840234448162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9384] Loss: 0.5228792607905173\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9385] Loss: 0.5228453570671484\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9386] Loss: 0.5229417285812685\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9387] Loss: 0.5229264869790793\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9388] Loss: 0.5229308115342335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9389] Loss: 0.52291627072183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9390] Loss: 0.5228971129719604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9391] Loss: 0.5229280910863356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9392] Loss: 0.5229542711244272\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9393] Loss: 0.5229558717150362\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9394] Loss: 0.5229744716744094\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9395] Loss: 0.5229458321345006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9396] Loss: 0.5229283391627781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9397] Loss: 0.5229715032563161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9398] Loss: 0.5229669215390669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9399] Loss: 0.5229819355460653\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9400] Loss: 0.5230477423129521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9401] Loss: 0.5230372600544828\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9402] Loss: 0.5230077831506112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9403] Loss: 0.5230429596863004\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9404] Loss: 0.5230180543290516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9405] Loss: 0.5230171074168343\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9406] Loss: 0.5229860419698944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9407] Loss: 0.5230065081777285\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9408] Loss: 0.5230562835211673\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9409] Loss: 0.5230491592741666\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9410] Loss: 0.5230280172339317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9411] Loss: 0.5230092714893163\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9412] Loss: 0.5230333548222491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9413] Loss: 0.523058259510919\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9414] Loss: 0.5230383552185797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9415] Loss: 0.5230147275828967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9416] Loss: 0.5229893890028585\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9417] Loss: 0.5229726331987083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9418] Loss: 0.5229734385115955\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9419] Loss: 0.5229518124201252\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9420] Loss: 0.5229531956926473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9421] Loss: 0.5229220828642389\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9422] Loss: 0.5228873282851554\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9423] Loss: 0.5228575809019578\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9424] Loss: 0.5228448109005448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9425] Loss: 0.5228541903494451\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9426] Loss: 0.5228736894134183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9427] Loss: 0.5229594675170021\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9428] Loss: 0.5229297349153011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9429] Loss: 0.5229541796153805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9430] Loss: 0.522936808628619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9431] Loss: 0.5229073809028647\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9432] Loss: 0.5228825975222001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9433] Loss: 0.5229132749748967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9434] Loss: 0.5228890198821821\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9435] Loss: 0.5228682794824702\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9436] Loss: 0.5228940266051582\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9437] Loss: 0.5229011462719481\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9438] Loss: 0.5228734483848924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9439] Loss: 0.5228706824010504\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9440] Loss: 0.5228627210001129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9441] Loss: 0.5228549576582506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9442] Loss: 0.5228473263514506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9443] Loss: 0.5228164411132925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9444] Loss: 0.5228077433494639\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9445] Loss: 0.522788894756853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9446] Loss: 0.5227834114483029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9447] Loss: 0.522800903045799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9448] Loss: 0.522860931793991\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9449] Loss: 0.52286086374914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9450] Loss: 0.5228903378616243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9451] Loss: 0.5228510356805373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9452] Loss: 0.5228435937502698\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9453] Loss: 0.5228628060681221\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9454] Loss: 0.5228446344936629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9455] Loss: 0.5228129293213616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9456] Loss: 0.522857263900811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9457] Loss: 0.5228439120889523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9458] Loss: 0.5228093601513453\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9459] Loss: 0.5227777231457593\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9460] Loss: 0.5227395944605326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9461] Loss: 0.5227398574435502\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9462] Loss: 0.5227236705278716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9463] Loss: 0.5227093581702341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9464] Loss: 0.522698125158979\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9465] Loss: 0.5226794016970262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9466] Loss: 0.5226688723314042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9467] Loss: 0.5226436428001208\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9468] Loss: 0.5227022728158922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9469] Loss: 0.5226759351012377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9470] Loss: 0.5227324492115129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9471] Loss: 0.5227640918799509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9472] Loss: 0.5227436464453643\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9473] Loss: 0.52271139311176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9474] Loss: 0.52270776543738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9475] Loss: 0.5226652566740384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9476] Loss: 0.5226523373180433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9477] Loss: 0.5226170511379733\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9478] Loss: 0.5225825023494268\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9479] Loss: 0.5226221693666607\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9480] Loss: 0.5226264947433535\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9481] Loss: 0.5226591944481906\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9482] Loss: 0.5226524947970986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9483] Loss: 0.5226806602947797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9484] Loss: 0.5226766944183973\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9485] Loss: 0.5227248197345681\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9486] Loss: 0.5226945275948807\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9487] Loss: 0.5226924454427527\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9488] Loss: 0.522697735756244\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9489] Loss: 0.5226978812806922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9490] Loss: 0.5227195077424326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9491] Loss: 0.5227090530234367\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9492] Loss: 0.5227074781210856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9493] Loss: 0.5227347569749361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9494] Loss: 0.5227332907237066\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9495] Loss: 0.5227183022264248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9496] Loss: 0.522767455914677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9497] Loss: 0.5228279506263497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9498] Loss: 0.5228259511156034\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9499] Loss: 0.5228858369882599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9500] Loss: 0.5229074926576572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9501] Loss: 0.5228908647917864\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9502] Loss: 0.5229366258005496\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9503] Loss: 0.522924016301576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9504] Loss: 0.5229446160275061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9505] Loss: 0.5229685558982529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9506] Loss: 0.5229371555146243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9507] Loss: 0.5229292051439962\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9508] Loss: 0.5229761682170898\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9509] Loss: 0.5230274092410588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9510] Loss: 0.5230079744921791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9511] Loss: 0.5230323955716325\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8371999999999999\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9512] Loss: 0.5230135566802641\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9513] Loss: 0.5230497672572926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9514] Loss: 0.5230505500751725\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9515] Loss: 0.5230436304421772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9516] Loss: 0.5230840965824183\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9517] Loss: 0.5230929884619939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9518] Loss: 0.5230744661065232\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9519] Loss: 0.5230390309215663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9520] Loss: 0.523008827754316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9521] Loss: 0.5230294966814111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9522] Loss: 0.5230211079239686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9523] Loss: 0.5230627079974477\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9524] Loss: 0.5230442519254845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9525] Loss: 0.5230384396997612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9526] Loss: 0.5230663620305728\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9527] Loss: 0.5230585481747166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9528] Loss: 0.5230854703049407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9529] Loss: 0.5230772450759599\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9530] Loss: 0.5230917205666491\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9531] Loss: 0.5230798598714631\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9532] Loss: 0.5230443393462673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9533] Loss: 0.5230602154535688\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9534] Loss: 0.5230872007879488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9535] Loss: 0.5230653365662223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9536] Loss: 0.5230462320219934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9537] Loss: 0.5230239484910224\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9538] Loss: 0.5230092365537325\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9539] Loss: 0.5230144371436091\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9540] Loss: 0.5230125657809798\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9541] Loss: 0.5229845425441355\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9542] Loss: 0.5230336854740832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9543] Loss: 0.5230289754625519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9544] Loss: 0.5230413087014211\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9545] Loss: 0.5230653756187315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9546] Loss: 0.5231035209304994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9547] Loss: 0.5230906552790646\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9548] Loss: 0.5230715843752227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9549] Loss: 0.5230607419088359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9550] Loss: 0.5230432492837441\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9551] Loss: 0.5230345329330132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9552] Loss: 0.5230034063579472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9553] Loss: 0.5229754622061273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9554] Loss: 0.5229730735500399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9555] Loss: 0.5229699208253388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9556] Loss: 0.523007255654577\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9557] Loss: 0.52304847292397\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9558] Loss: 0.5230432555102517\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9559] Loss: 0.5230385701782678\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9560] Loss: 0.5230155649517781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9561] Loss: 0.5230322713539648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9562] Loss: 0.5230088566853391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9563] Loss: 0.52300524845428\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9564] Loss: 0.522990357371877\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9565] Loss: 0.523039026231742\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9566] Loss: 0.5230366974335177\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9567] Loss: 0.5230403694228462\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9568] Loss: 0.5230559174618755\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9569] Loss: 0.523081811346654\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9570] Loss: 0.5230673817954711\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9571] Loss: 0.5230985975297736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9572] Loss: 0.5230956475690905\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9573] Loss: 0.5230952246433376\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9574] Loss: 0.5230839957334331\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9575] Loss: 0.5230527818120781\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9576] Loss: 0.5230812253411995\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9577] Loss: 0.5230984801153923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9578] Loss: 0.523075476139258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9579] Loss: 0.5231009128223457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9580] Loss: 0.5230617763813433\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9581] Loss: 0.5230379758451782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9582] Loss: 0.5230076875770875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9583] Loss: 0.5229858464595174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9584] Loss: 0.5229848289863507\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9585] Loss: 0.5229567580824056\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9586] Loss: 0.5229751668647161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9587] Loss: 0.5230287772053281\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9588] Loss: 0.5229978193058057\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9589] Loss: 0.5229684185096773\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9590] Loss: 0.5229493901921264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9591] Loss: 0.522924860157692\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9592] Loss: 0.5229106762780148\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9593] Loss: 0.522943412264735\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9594] Loss: 0.5229329983964202\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9595] Loss: 0.5229843595983172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9596] Loss: 0.5229851885295087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9597] Loss: 0.5229753871765338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9598] Loss: 0.5229514619224036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9599] Loss: 0.5229365585011373\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9600] Loss: 0.5229360293207651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9601] Loss: 0.5229202646644526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9602] Loss: 0.5228946937327599\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9603] Loss: 0.5228715100743833\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9604] Loss: 0.5228432665028256\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9605] Loss: 0.5228320278296961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9606] Loss: 0.5228449543454768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9607] Loss: 0.5228325661930708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9608] Loss: 0.5228133856608176\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9609] Loss: 0.5227834155781215\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9610] Loss: 0.522789067196977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9611] Loss: 0.5227820690831383\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9612] Loss: 0.5227656718159319\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9613] Loss: 0.5227351444498676\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9614] Loss: 0.5227762583482795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9615] Loss: 0.5227531016413031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9616] Loss: 0.5227216610228023\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9617] Loss: 0.5227337270579144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9618] Loss: 0.5227731620245823\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9619] Loss: 0.5227755346136201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9620] Loss: 0.522741356478957\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9621] Loss: 0.5227970920185032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9622] Loss: 0.5227756341813006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9623] Loss: 0.5227533084173097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9624] Loss: 0.5227509773278514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9625] Loss: 0.5227864951827143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9626] Loss: 0.5227860821874427\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9627] Loss: 0.5227827782875171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9628] Loss: 0.5227679645029383\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9629] Loss: 0.5227702468721892\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9630] Loss: 0.5227465153720818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9631] Loss: 0.522757276495668\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9632] Loss: 0.5227535429609316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9633] Loss: 0.5227174803207909\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9634] Loss: 0.5227640287910622\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9635] Loss: 0.5227534007960802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9636] Loss: 0.5227429329003521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9637] Loss: 0.5227264705892328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9638] Loss: 0.5227236748459985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9639] Loss: 0.5227630761604944\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9640] Loss: 0.5227429513471672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9641] Loss: 0.5227134118413315\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9642] Loss: 0.5227817852103914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9643] Loss: 0.5228052390609815\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9644] Loss: 0.5228522459409262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9645] Loss: 0.5228543549661893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9646] Loss: 0.5228506235500403\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9647] Loss: 0.5228342627316528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9648] Loss: 0.5228125316931294\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9649] Loss: 0.5228141176255791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9650] Loss: 0.5228541018404638\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9651] Loss: 0.5228238241776976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9652] Loss: 0.5228128496942961\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9653] Loss: 0.5227826358331811\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9654] Loss: 0.5228103886970359\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9655] Loss: 0.5228671538872168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9656] Loss: 0.5228348287951105\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9657] Loss: 0.5228121414808311\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9658] Loss: 0.5228073058831519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9659] Loss: 0.5228213764550399\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9660] Loss: 0.5228462093710248\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9661] Loss: 0.5228384037172061\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9662] Loss: 0.5228831983086738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9663] Loss: 0.5228790274431415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9664] Loss: 0.5228699328488129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9665] Loss: 0.5228506890038594\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9666] Loss: 0.5228192809726693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9667] Loss: 0.5228610872551904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9668] Loss: 0.5228448180548096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9669] Loss: 0.522856009336444\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9670] Loss: 0.5228474512989084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9671] Loss: 0.5228126721505241\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9672] Loss: 0.5228082708977009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9673] Loss: 0.5227920415749342\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9674] Loss: 0.5227767524469216\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9675] Loss: 0.522746744755182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9676] Loss: 0.5227999880831886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9677] Loss: 0.5227929230873283\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9678] Loss: 0.5227904388921605\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9679] Loss: 0.5227696819915313\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9680] Loss: 0.5227567143453896\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9681] Loss: 0.5227521739883052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9682] Loss: 0.5227383156345626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9683] Loss: 0.5227562134179219\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9684] Loss: 0.5228256096107938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9685] Loss: 0.5228070551092033\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9686] Loss: 0.5228041158786081\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9687] Loss: 0.5228390504254019\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9688] Loss: 0.522816912013543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9689] Loss: 0.5227996522885598\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9690] Loss: 0.5227888800236264\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9691] Loss: 0.522764139873129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9692] Loss: 0.5227380509169188\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9693] Loss: 0.5227355571366545\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9694] Loss: 0.5227761301406715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9695] Loss: 0.5227735548595014\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9696] Loss: 0.5227733066894767\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9697] Loss: 0.5227910571389895\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9698] Loss: 0.5227833863268301\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9699] Loss: 0.5227867076346736\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9700] Loss: 0.5227632006587738\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9701] Loss: 0.5227426854732856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9702] Loss: 0.5227732522878391\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9703] Loss: 0.5227409778881109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9704] Loss: 0.5227131834766231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9705] Loss: 0.5227007210270875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9706] Loss: 0.5227439751578783\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9707] Loss: 0.5227270644148808\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9708] Loss: 0.5227682160314437\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9709] Loss: 0.5227504411684144\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9710] Loss: 0.5227897831619434\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9711] Loss: 0.522786565728258\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9712] Loss: 0.5227724454505136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9713] Loss: 0.5227485912465718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9714] Loss: 0.5227187253706407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9715] Loss: 0.5227551768066959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9716] Loss: 0.5227771571394466\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9717] Loss: 0.5227495300254488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9718] Loss: 0.5227302933754245\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9719] Loss: 0.5227280889418009\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9720] Loss: 0.5227207711478139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9721] Loss: 0.522697264176732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9722] Loss: 0.5226655075479424\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9723] Loss: 0.5227027710732997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9724] Loss: 0.5226802493813293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9725] Loss: 0.5227126908121197\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9726] Loss: 0.5227506213963756\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9727] Loss: 0.522766074687291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9728] Loss: 0.5227494850499327\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9729] Loss: 0.5227162332871768\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9730] Loss: 0.5226936651058415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9731] Loss: 0.5226753043509166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9732] Loss: 0.5226476506171116\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9733] Loss: 0.5226659706244303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9734] Loss: 0.5226871470879547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9735] Loss: 0.522696790355977\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9736] Loss: 0.5227194445338137\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9737] Loss: 0.5226999906679914\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9738] Loss: 0.5226821597353897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9739] Loss: 0.5227415524184968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9740] Loss: 0.5227295981229708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9741] Loss: 0.5227250827926557\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9742] Loss: 0.5226864819051746\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9743] Loss: 0.5226529473236112\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9744] Loss: 0.5226371026843405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9745] Loss: 0.5226160326679139\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9746] Loss: 0.5225889142255806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9747] Loss: 0.5226307155102042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9748] Loss: 0.5226096995795901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9749] Loss: 0.52260430506045\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9750] Loss: 0.5225862550496797\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9751] Loss: 0.5225950420328503\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9752] Loss: 0.522578127277514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9753] Loss: 0.5225713116987487\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9754] Loss: 0.5225608222531529\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9755] Loss: 0.5225424573703867\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9756] Loss: 0.5225542368193661\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9757] Loss: 0.5226359290922992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9758] Loss: 0.5226101545571868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9759] Loss: 0.5225830300619363\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9760] Loss: 0.5225545478608157\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9761] Loss: 0.522537395443456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9762] Loss: 0.5225369889841572\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9763] Loss: 0.5225124238349805\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9764] Loss: 0.5224972465543528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9765] Loss: 0.5224633802583835\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9766] Loss: 0.522441458647673\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9767] Loss: 0.5224431015955935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9768] Loss: 0.5224642875202147\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9769] Loss: 0.5224310718150875\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9770] Loss: 0.5224558030056001\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9771] Loss: 0.5224404583857366\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9772] Loss: 0.5224498739863055\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9773] Loss: 0.5224287291797901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9774] Loss: 0.5224364819165982\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9775] Loss: 0.5224311179475918\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9776] Loss: 0.5224535645723677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9777] Loss: 0.5224739985502923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9778] Loss: 0.5224463961581166\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9779] Loss: 0.5224584234093292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9780] Loss: 0.5224477529316787\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9781] Loss: 0.5224436209828912\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9782] Loss: 0.5224542154225813\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9783] Loss: 0.5224394010058322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9784] Loss: 0.5224252118824858\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9785] Loss: 0.522433608922997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9786] Loss: 0.5224179506619951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9787] Loss: 0.522403175733888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9788] Loss: 0.5223754327788412\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9789] Loss: 0.5223526813957837\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9790] Loss: 0.5223310388351694\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9791] Loss: 0.5223499264388669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9792] Loss: 0.5223311354514341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9793] Loss: 0.5223592326155796\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9794] Loss: 0.5223335714056568\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9795] Loss: 0.5223227302538302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9796] Loss: 0.5223137424256225\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9797] Loss: 0.5222943877893358\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9798] Loss: 0.5223352841304536\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9799] Loss: 0.5223020825794534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9800] Loss: 0.5222826428141994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9801] Loss: 0.5222842809975231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9802] Loss: 0.5222560168290852\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9803] Loss: 0.5222496261759465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9804] Loss: 0.5222249774184949\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9805] Loss: 0.5222292763321806\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9806] Loss: 0.522225216830954\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9807] Loss: 0.5222112401875935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9808] Loss: 0.52221128663938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9809] Loss: 0.5221872361167963\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9810] Loss: 0.5221859146302106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9811] Loss: 0.5222318002359545\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9812] Loss: 0.5222325475962155\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9813] Loss: 0.5222234459531624\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9814] Loss: 0.522203284358207\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9815] Loss: 0.5221766811410693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9816] Loss: 0.5221411102248626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9817] Loss: 0.5221564692653339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9818] Loss: 0.522147016346032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9819] Loss: 0.5221316809796508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9820] Loss: 0.5221730239137159\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9821] Loss: 0.5221430903144406\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9822] Loss: 0.5221156033341648\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9823] Loss: 0.5221559219418337\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9824] Loss: 0.5222069079963106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9825] Loss: 0.5222018315676266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9826] Loss: 0.5221856768859231\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9827] Loss: 0.522186806892663\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9828] Loss: 0.5221641671138788\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9829] Loss: 0.5221380541455478\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9830] Loss: 0.5221377448798377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9831] Loss: 0.522150664688717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9832] Loss: 0.5221914164682003\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9833] Loss: 0.5222296054052153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9834] Loss: 0.5221989269163387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9835] Loss: 0.52217019137844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9836] Loss: 0.5221976054240751\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9837] Loss: 0.5221780996553587\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9838] Loss: 0.5221559142655539\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9839] Loss: 0.5221599052175089\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9840] Loss: 0.5222072633149029\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9841] Loss: 0.5221741905841346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9842] Loss: 0.5221465587714967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9843] Loss: 0.5221326922672324\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9844] Loss: 0.5221578716794326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9845] Loss: 0.5221597089429686\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9846] Loss: 0.522153390833235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9847] Loss: 0.522135024008754\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9848] Loss: 0.5221180734123886\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9849] Loss: 0.5221026456465186\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9850] Loss: 0.5221256815599042\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9851] Loss: 0.5221073187232822\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9852] Loss: 0.5220864812006687\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9853] Loss: 0.522088067395764\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9854] Loss: 0.522061021907625\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9855] Loss: 0.5220387496735847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9856] Loss: 0.5220334253648785\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9857] Loss: 0.5220009211478316\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9858] Loss: 0.5219931757945534\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9859] Loss: 0.5219713338496456\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9860] Loss: 0.522009694137994\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9861] Loss: 0.5219784191876423\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9862] Loss: 0.5219502857240145\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9863] Loss: 0.5219140102837693\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9864] Loss: 0.5219614972795129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9865] Loss: 0.5219475949801333\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9866] Loss: 0.5219590827462739\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9867] Loss: 0.5219529293150273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9868] Loss: 0.5219365866167293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9869] Loss: 0.5219094046483196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9870] Loss: 0.521947582386052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9871] Loss: 0.5219455183542459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9872] Loss: 0.5219216870668861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9873] Loss: 0.5218895487801636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9874] Loss: 0.5218914462518407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9875] Loss: 0.5219264085001317\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9876] Loss: 0.5219266177840195\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9877] Loss: 0.521937372661062\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9878] Loss: 0.521909800117134\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9879] Loss: 0.5218779952056075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9880] Loss: 0.52186245986339\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9881] Loss: 0.5219035516005547\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9882] Loss: 0.5219121487843802\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9883] Loss: 0.5219330146462027\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9884] Loss: 0.5219353808188251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9885] Loss: 0.5219275978533426\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9886] Loss: 0.5219650398431276\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9887] Loss: 0.5219375240029818\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9888] Loss: 0.5219271757895567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9889] Loss: 0.5219035844855476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9890] Loss: 0.5219275042732388\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9891] Loss: 0.5219684721354596\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9892] Loss: 0.5219542605711488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9893] Loss: 0.5220259809518157\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9894] Loss: 0.5220198700552249\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9895] Loss: 0.5220494575080048\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9896] Loss: 0.5220727886398947\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9897] Loss: 0.522110718961111\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9898] Loss: 0.5220830588485263\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9899] Loss: 0.5220768540472329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9900] Loss: 0.5220516294907457\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9901] Loss: 0.5220609821395845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9902] Loss: 0.522085391162709\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9903] Loss: 0.5220592892691565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9904] Loss: 0.5220513711628925\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9905] Loss: 0.5220181951026703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9906] Loss: 0.5219916674673634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9907] Loss: 0.5219558993527087\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9908] Loss: 0.5220063949714546\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9909] Loss: 0.5219909694806377\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9910] Loss: 0.5220438180071242\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9911] Loss: 0.5220395884746162\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9912] Loss: 0.5220117636042386\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9913] Loss: 0.5219739889681473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9914] Loss: 0.5220147268581218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9915] Loss: 0.5219823067664213\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9916] Loss: 0.5219577374548677\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9917] Loss: 0.5219413727661293\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9918] Loss: 0.5219913118845169\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9919] Loss: 0.5219668140176608\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9920] Loss: 0.5219537919104162\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9921] Loss: 0.5219718020608455\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9922] Loss: 0.52198409809839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9923] Loss: 0.5219976490718278\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9924] Loss: 0.521999199018563\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9925] Loss: 0.522013744518786\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9926] Loss: 0.5220597510794993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9927] Loss: 0.5221086794215174\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9928] Loss: 0.5220895649746209\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9929] Loss: 0.5220793096002772\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9930] Loss: 0.5220467003224508\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9931] Loss: 0.5220235339263584\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9932] Loss: 0.5220089820926292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9933] Loss: 0.521981121880682\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9934] Loss: 0.5220223682693795\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9935] Loss: 0.522000981177196\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9936] Loss: 0.522032249686497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9937] Loss: 0.5220014335141273\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9938] Loss: 0.5220030657583543\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9939] Loss: 0.5219769892592435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9940] Loss: 0.521964592190335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9941] Loss: 0.5219360573928084\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9942] Loss: 0.5219812026915659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9943] Loss: 0.521956627004715\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9944] Loss: 0.5219811620678415\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9945] Loss: 0.5219565408976544\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9946] Loss: 0.5219733636603612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9947] Loss: 0.521954340267525\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9948] Loss: 0.5219353095887449\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9949] Loss: 0.5219043106557404\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9950] Loss: 0.521932928011579\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9951] Loss: 0.521964318721227\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9952] Loss: 0.5219741957189168\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9953] Loss: 0.5219775701394769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9954] Loss: 0.522002057825191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9955] Loss: 0.5220157423683296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9956] Loss: 0.5219934628489612\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9957] Loss: 0.5219849312167906\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9958] Loss: 0.5219968414844143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9959] Loss: 0.5220615265482708\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9960] Loss: 0.522065611446976\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9961] Loss: 0.5220334868105161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9962] Loss: 0.5220058029466295\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9963] Loss: 0.5219984542506341\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9964] Loss: 0.5219944628495002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9965] Loss: 0.521982139752442\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9966] Loss: 0.5219945009268329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9967] Loss: 0.5219796817638259\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9968] Loss: 0.5219585932153152\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9969] Loss: 0.5219760898972102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9970] Loss: 0.5219946757494521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9971] Loss: 0.5219984031570297\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9972] Loss: 0.5220271488169338\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9973] Loss: 0.5220209201710252\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9974] Loss: 0.5220026728883701\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9975] Loss: 0.5219949765175156\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9976] Loss: 0.5219655769548329\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9977] Loss: 0.5219629209170287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9978] Loss: 0.5219909259297951\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9979] Loss: 0.522016540149258\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9980] Loss: 0.5220254459816652\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9981] Loss: 0.5220775977005212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9982] Loss: 0.5220599043673861\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9983] Loss: 0.5220674377402346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9984] Loss: 0.5220629057414626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9985] Loss: 0.5220350206841894\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9986] Loss: 0.522013742251832\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9987] Loss: 0.5220048690698841\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9988] Loss: 0.5219909849329623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9989] Loss: 0.521995539041571\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9990] Loss: 0.5219637961515897\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9991] Loss: 0.5219417794817498\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9992] Loss: 0.5219115270617968\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9993] Loss: 0.5218952221057969\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9994] Loss: 0.5219288789213509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9995] Loss: 0.5219729087842284\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9996] Loss: 0.521951831440533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9997] Loss: 0.5219374099775601\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9998] Loss: 0.521943067263303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 9999] Loss: 0.5219284705695997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10000] Loss: 0.5219211235391201\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10001] Loss: 0.5219103518636771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10002] Loss: 0.5219507518271906\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10003] Loss: 0.5219562751207967\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10004] Loss: 0.5219309567064769\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10005] Loss: 0.5219452308268218\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10006] Loss: 0.521942849717438\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10007] Loss: 0.5219204160063671\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10008] Loss: 0.5218866344695845\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10009] Loss: 0.5219352413715971\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10010] Loss: 0.521976424647005\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10011] Loss: 0.5219538206458656\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "******************************************************************\n",
      "*********************** Performance Update ***********************\n",
      "******************************************************************\n",
      "\n",
      "Area Under the ROC Curve: 0.8243\n",
      "\n",
      "******************************************************************\n",
      "****************** Performance Update Complete! ******************\n",
      "******************************************************************\n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10012] Loss: 0.5219293965449762\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10013] Loss: 0.5219494886214164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10014] Loss: 0.5219241138516302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10015] Loss: 0.5219606004941469\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10016] Loss: 0.5219823077913588\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10017] Loss: 0.5219694413765881\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10018] Loss: 0.5219712567288032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10019] Loss: 0.5219476253173494\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10020] Loss: 0.5219437853300986\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10021] Loss: 0.5219419112486012\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10022] Loss: 0.521938033082387\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10023] Loss: 0.5219189076472266\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10024] Loss: 0.5219108198391732\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10025] Loss: 0.5219611174109354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10026] Loss: 0.521976711576904\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10027] Loss: 0.5220129028881775\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10028] Loss: 0.5220022125952032\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10029] Loss: 0.5220008363667651\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10030] Loss: 0.5220234308923993\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10031] Loss: 0.5220250436869279\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10032] Loss: 0.5220047726372765\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10033] Loss: 0.5219780918518611\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10034] Loss: 0.5219869477277741\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10035] Loss: 0.5220204616406459\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10036] Loss: 0.5219968475108885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10037] Loss: 0.5219842053584616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10038] Loss: 0.5219665951025927\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10039] Loss: 0.5219414512856407\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10040] Loss: 0.5219700385021888\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10041] Loss: 0.5219817143705556\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10042] Loss: 0.5219858077316039\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10043] Loss: 0.5220190959181576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10044] Loss: 0.5219969685338492\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10045] Loss: 0.5219744042007475\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10046] Loss: 0.5219659356075346\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10047] Loss: 0.5219600032532604\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10048] Loss: 0.5219423977314839\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10049] Loss: 0.5219664225689282\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10050] Loss: 0.5220677218784576\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10051] Loss: 0.5220528691991447\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10052] Loss: 0.5220427749117473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10053] Loss: 0.5220862967306171\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10054] Loss: 0.522102841531291\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10055] Loss: 0.5220982080982143\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10056] Loss: 0.5221121907234192\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10057] Loss: 0.5221661698122705\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10058] Loss: 0.5221449048026217\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10059] Loss: 0.5221286055392302\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10060] Loss: 0.5221070287770626\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10061] Loss: 0.5220785187359394\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10062] Loss: 0.5220799873359938\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10063] Loss: 0.5220754737100538\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10064] Loss: 0.5221011307708933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10065] Loss: 0.5220859605438856\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10066] Loss: 0.5220727565000126\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10067] Loss: 0.52206535375591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10068] Loss: 0.5220596252948975\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10069] Loss: 0.5221062656513429\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10070] Loss: 0.5221242907842831\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10071] Loss: 0.5221150428128409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10072] Loss: 0.5221705879035092\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10073] Loss: 0.5221733791638926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10074] Loss: 0.5221553617961384\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10075] Loss: 0.522159322751983\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10076] Loss: 0.522145883794138\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10077] Loss: 0.5221102514090153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10078] Loss: 0.5220824638428322\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10079] Loss: 0.5220928343081253\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10080] Loss: 0.5220812148948822\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10081] Loss: 0.5220898771211901\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10082] Loss: 0.522070761776237\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10083] Loss: 0.5220864423434214\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10084] Loss: 0.5221014824678959\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10085] Loss: 0.5220655711886154\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10086] Loss: 0.5220423724219115\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10087] Loss: 0.5220598807175074\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10088] Loss: 0.5220667422004472\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10089] Loss: 0.5221206488272307\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10090] Loss: 0.522138098274243\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10091] Loss: 0.5221864413324222\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10092] Loss: 0.522228976930514\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10093] Loss: 0.5222240851646448\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10094] Loss: 0.5222003624489988\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10095] Loss: 0.522168186516829\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10096] Loss: 0.5221455955173336\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10097] Loss: 0.5221205523311998\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10098] Loss: 0.5221581367443987\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10099] Loss: 0.5221603757889555\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10100] Loss: 0.5222080757552068\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10101] Loss: 0.5222096347999006\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10102] Loss: 0.522204637275799\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10103] Loss: 0.5222059489073178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10104] Loss: 0.5221897676441106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10105] Loss: 0.5221730587599239\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10106] Loss: 0.5221476672048766\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10107] Loss: 0.5221867119413844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10108] Loss: 0.5221597972009847\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10109] Loss: 0.5221284072788348\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10110] Loss: 0.5221446910624703\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10111] Loss: 0.5221250831286195\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10112] Loss: 0.5221307153622078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10113] Loss: 0.5221121055433356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10114] Loss: 0.5221141642894361\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10115] Loss: 0.5221058661307226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10116] Loss: 0.5221562346809238\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10117] Loss: 0.5221627417750571\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10118] Loss: 0.5221665751756627\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10119] Loss: 0.522156273215181\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10120] Loss: 0.5221400982631481\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10121] Loss: 0.522144648637078\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10122] Loss: 0.5221318870696037\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10123] Loss: 0.5221260485057093\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10124] Loss: 0.5221256746995565\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10125] Loss: 0.5221090460189357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10126] Loss: 0.5221452640955292\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10127] Loss: 0.5221143625525559\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10128] Loss: 0.5220879285901717\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10129] Loss: 0.5221063935009379\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10130] Loss: 0.522084741570102\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10131] Loss: 0.5220674695895275\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10132] Loss: 0.5220829879350509\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10133] Loss: 0.522077055482474\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10134] Loss: 0.5221012638462205\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10135] Loss: 0.5221045989340002\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10136] Loss: 0.5220906205510935\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10137] Loss: 0.5220942767167669\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10138] Loss: 0.5221277956595844\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10139] Loss: 0.5221576990244582\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10140] Loss: 0.5221471807264713\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10141] Loss: 0.5221438234455922\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10142] Loss: 0.5221203216893036\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10143] Loss: 0.5220960380131862\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10144] Loss: 0.5220757109237356\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10145] Loss: 0.5221031137742586\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10146] Loss: 0.5221167057814465\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10147] Loss: 0.5221329577250934\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10148] Loss: 0.5221489317364206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10149] Loss: 0.5221710837422868\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10150] Loss: 0.5222207201343354\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10151] Loss: 0.5222263602396349\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10152] Loss: 0.5222064327016656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10153] Loss: 0.5221853592479104\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10154] Loss: 0.5221653040274452\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10155] Loss: 0.5221296866224985\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10156] Loss: 0.522164757563357\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10157] Loss: 0.52215581636046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10158] Loss: 0.5221927983185916\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10159] Loss: 0.522171816446643\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10160] Loss: 0.5221426658756992\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10161] Loss: 0.5221773058009288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10162] Loss: 0.5221455116079672\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10163] Loss: 0.5221209384923178\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10164] Loss: 0.5221127805726262\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10165] Loss: 0.5222106976823323\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10166] Loss: 0.5221960753767251\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10167] Loss: 0.5221986689944191\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10168] Loss: 0.5221839875467299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10169] Loss: 0.5222045263696474\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10170] Loss: 0.5221920269169299\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10171] Loss: 0.5222076696507889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10172] Loss: 0.5222344329328893\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10173] Loss: 0.5222254193672344\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10174] Loss: 0.5222076839684132\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10175] Loss: 0.5221817815343889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10176] Loss: 0.5221486018557073\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10177] Loss: 0.5221159026694172\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10178] Loss: 0.5220972964933923\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10179] Loss: 0.5221118855940567\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10180] Loss: 0.5220866281789609\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10181] Loss: 0.522090134590405\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10182] Loss: 0.5221119391122591\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10183] Loss: 0.5220848382995107\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10184] Loss: 0.522106024379853\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10185] Loss: 0.5220946211269659\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10186] Loss: 0.5220810263692939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10187] Loss: 0.5220617480815486\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10188] Loss: 0.5220376441755997\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10189] Loss: 0.5220763129132606\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10190] Loss: 0.5220596345726493\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10191] Loss: 0.5220372389862328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10192] Loss: 0.5220260049080435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10193] Loss: 0.5219917652433499\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10194] Loss: 0.5219846332829656\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10195] Loss: 0.5220113660188206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10196] Loss: 0.5220094091979501\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10197] Loss: 0.5220424494264516\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10198] Loss: 0.5220127258431335\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10199] Loss: 0.5219896987322031\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10200] Loss: 0.5219821569182064\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10201] Loss: 0.5219522781532476\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10202] Loss: 0.5220118041157575\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10203] Loss: 0.5219868733475086\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10204] Loss: 0.5220026497173634\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10205] Loss: 0.5219854288982816\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10206] Loss: 0.5219880097972924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10207] Loss: 0.5219836757117182\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10208] Loss: 0.5220023619284161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10209] Loss: 0.5220361566104794\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10210] Loss: 0.5220317834124123\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10211] Loss: 0.5220185519708652\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10212] Loss: 0.5219960512973136\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10213] Loss: 0.521958709515629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10214] Loss: 0.5219785100507864\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10215] Loss: 0.5219433837364409\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10216] Loss: 0.5219221553273381\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10217] Loss: 0.5218880562559024\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10218] Loss: 0.5219410139350523\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10219] Loss: 0.5219272197256718\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10220] Loss: 0.5219510659069038\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10221] Loss: 0.5219317980178929\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10222] Loss: 0.5219127039718127\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10223] Loss: 0.5219121871537779\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10224] Loss: 0.5218932773777554\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10225] Loss: 0.521879716117052\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10226] Loss: 0.5218841334327153\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10227] Loss: 0.5218749697609326\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10228] Loss: 0.5219246372166522\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10229] Loss: 0.5219569178364235\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10230] Loss: 0.5219465146247497\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10231] Loss: 0.5219937476763791\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10232] Loss: 0.5219838862046615\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10233] Loss: 0.5220315647562945\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10234] Loss: 0.5220186554695392\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10235] Loss: 0.522074099779129\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10236] Loss: 0.5220871872153519\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10237] Loss: 0.5220939644386506\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10238] Loss: 0.522143994847092\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10239] Loss: 0.5221246482595926\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10240] Loss: 0.5220973732568255\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10241] Loss: 0.5220966723684097\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10242] Loss: 0.5220689645677842\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10243] Loss: 0.522072117136106\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10244] Loss: 0.5220378400615924\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10245] Loss: 0.5220046586006436\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10246] Loss: 0.5220108664714737\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10247] Loss: 0.5219930614857161\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10248] Loss: 0.5219678759657521\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10249] Loss: 0.5219416178350564\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10250] Loss: 0.5219237495909849\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10251] Loss: 0.5219817038248016\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10252] Loss: 0.5219924241760212\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10253] Loss: 0.5219621304263939\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10254] Loss: 0.5220298680869866\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10255] Loss: 0.5220273505393636\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10256] Loss: 0.522086611808435\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10257] Loss: 0.5220792274408075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10258] Loss: 0.5220808302849885\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10259] Loss: 0.5220682232890304\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10260] Loss: 0.5220832972098619\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10261] Loss: 0.5220587859825163\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10262] Loss: 0.5220600973029328\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10263] Loss: 0.5220470151039046\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10264] Loss: 0.5220681276109\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10265] Loss: 0.5220561225282296\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10266] Loss: 0.5220876258414952\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10267] Loss: 0.5221026401344083\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10268] Loss: 0.5220856973384096\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10269] Loss: 0.5221452147260236\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10270] Loss: 0.522150284949629\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10271] Loss: 0.5221511684056616\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10272] Loss: 0.5221384912790321\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10273] Loss: 0.5221170295820878\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10274] Loss: 0.522087496340226\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10275] Loss: 0.5220672891345075\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10276] Loss: 0.5220371699561454\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10277] Loss: 0.5220493526624488\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10278] Loss: 0.5220428119695011\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10279] Loss: 0.5220248336032489\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10280] Loss: 0.5220370551580782\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10281] Loss: 0.5220573890381223\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10282] Loss: 0.5220519414707573\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10283] Loss: 0.5220187716677771\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10284] Loss: 0.5220570241670704\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10285] Loss: 0.5220717660179528\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10286] Loss: 0.5221187137128288\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10287] Loss: 0.5220881771630422\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10288] Loss: 0.5220616414219501\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10289] Loss: 0.5220516526981882\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10290] Loss: 0.5220492443820206\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10291] Loss: 0.5220141360126211\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10292] Loss: 0.5219973598404473\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10293] Loss: 0.5219932488870928\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10294] Loss: 0.5220245818286915\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10295] Loss: 0.5220459497913025\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10296] Loss: 0.5220357696751303\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10297] Loss: 0.5220401938363287\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10298] Loss: 0.5220059876843623\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10299] Loss: 0.5220264527969164\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10300] Loss: 0.5220677801336526\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10301] Loss: 0.5220700154331533\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10302] Loss: 0.5220889432227933\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10303] Loss: 0.5220671650290453\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10304] Loss: 0.5221169191719391\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10305] Loss: 0.5221256673670716\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10306] Loss: 0.5221194234391824\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10307] Loss: 0.5221181645117889\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10308] Loss: 0.5221030690075549\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10309] Loss: 0.5220856027574439\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10310] Loss: 0.5221139531354515\n",
      "\n",
      "CUDA Memory Allocated: 7320371712\n",
      "[Epoch 2, Batch 10311] Loss: 0.5220897799456026\n",
      "\n",
      "*********** Saving network weights and optimizer state *********** \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-557dd032835c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*********** Saving network weights and optimizer state *********** \\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# save the weights and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             torch.save({'mini_batch': mini_batch,\n\u001b[0m\u001b[1;32m     45\u001b[0m                         \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/MAS_pt/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_weights = True\n",
    "\n",
    "print(\"Pre-Training CUDA Memory Allocation:\", torch.cuda.max_memory_allocated())\n",
    "\n",
    "if learn_weights:\n",
    "\n",
    "    # set start time for cnn training\n",
    "    start_time = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net.forward(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels.unsqueeze(-1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_sched.step()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # update mini-batch count\n",
    "        mini_batch += 1\n",
    "        epoch = mini_batch // 12204\n",
    "\n",
    "        # print every mini-batch\n",
    "        print(\"CUDA Memory Allocated:\", torch.cuda.max_memory_allocated())\n",
    "        print(f'[Epoch {epoch}, Batch {mini_batch % 12204}] Loss: {running_loss / (i+1)}\\n')\n",
    "\n",
    "        # save and outoput every 100 mini-batch\n",
    "        if i % 100 == 0:\n",
    "            print(\"*********** Saving network weights and optimizer state *********** \\n\\n\")\n",
    "            # save the weights and optimizer\n",
    "            torch.save({'mini_batch': mini_batch,\n",
    "                        'model_state_dict': net.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'lr_sched': lr_sched.state_dict()}, PATH)\n",
    "            \n",
    "        # eval every 500 mini-batch\n",
    "        if i % 500 == 0:\n",
    "            \n",
    "            print(\"******************************************************************\")\n",
    "            print(\"*********************** Performance Update ***********************\")\n",
    "            print(\"******************************************************************\\n\")\n",
    "            \n",
    "            net.eval()\n",
    "            \n",
    "            ground_truths = []\n",
    "            probs = []\n",
    "\n",
    "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "            with torch.no_grad():\n",
    "                for j, valdata in enumerate(val_loader, 0):\n",
    "                    image, label = valdata\n",
    "                    image = image.to(device)\n",
    "\n",
    "                    # save for analysis\n",
    "                    ground_truths.append(label)\n",
    "\n",
    "                    # calculate outputs by running images through the network \n",
    "                    outputs = net(image)\n",
    "                    outputs = outputs.to(\"cpu\")\n",
    "\n",
    "                    # # save for analysis\n",
    "                    probs.append(outputs)\n",
    "\n",
    "            print(\"Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs))\n",
    "            \n",
    "            net.train()\n",
    "\n",
    "            print(\"\\n******************************************************************\")\n",
    "            print(\"****************** Performance Update Complete! ******************\")\n",
    "            print(\"******************************************************************\\n\\n\")\n",
    "\n",
    "        # save unique set of weights and optimizer for validation later\n",
    "        if mini_batch % 12204 == 0:\n",
    "\n",
    "            uPATH = f'./saved_weights4/melanoma_ResNeSt_{epoch}e_{mini_batch % 12204}b.pth'\n",
    "            torch.save({'mini_batch': mini_batch,\n",
    "                        'model_state_dict': net.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'lr_sched': lr_sched.state_dict()}, uPATH)\n",
    "\n",
    "    print('*********** Finished Training this Epoch in', time.time() - start_time, 'seconds ***********')\n",
    "    \n",
    "    # save the weights and optimizer\n",
    "    torch.save({'mini_batch': mini_batch,\n",
    "                'model_state_dict': net.state_dict(), \n",
    "                'optimizer_state_dict': optimizer.state_dict(), \n",
    "                'lr_sched': lr_sched.state_dict()}, PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998de40",
   "metadata": {},
   "source": [
    "# Formally test performance on our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f63c0",
   "metadata": {},
   "source": [
    "First, let us see what the convolutional neural network thinks of a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88b172eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  Benign Benign Benign Benign\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "test_dataset = ISICDatasetImages(img_dir=os.path.join(\"train_data768x768\", \"jpgs\"), \n",
    "                            patientfile=os.path.join(\"train_data768x768\", \"val.csv\"), \n",
    "                            num_samples=8281, up_sample=False, start_ind=0, transform=val_transf)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate2d, \n",
    "                         num_workers=n_workers)\n",
    "\n",
    "\n",
    "\n",
    "testiter = iter(test_loader)\n",
    "images, labels = next(testiter)\n",
    "\n",
    "# print images\n",
    "print('GroundTruth: ', ' '.join('%5s' % label_id[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072622c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_weights | create_new_weights:\n",
    "    \n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % label_id[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9271d8",
   "metadata": {},
   "source": [
    "Fortunately, we saved weights off at different epoch/batch values. Here is the list of saved weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cc00fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['melanoma_ResNeSt_1e_0b.pth',\n",
       " 'melanoma_ResNeSt_2e_0b.pth',\n",
       " 'melanoma_ResNeSt_3e_0b.pth']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./saved_weights4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29b2cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: melanoma_ResNeSt_1e_0b.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /u/home/a/andrewma/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #0 ... Running Time 0.49071574211120605\n",
      "\t Current Testing Loss: 0.12008341401815414\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 3.4574661254882812\n",
      "\t Current Testing Loss: 0.29470906406641006\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 6.4191741943359375\n",
      "\t Current Testing Loss: 0.31715962822948185\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 9.382222414016724\n",
      "\t Current Testing Loss: 0.35811792145813665\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 12.344882249832153\n",
      "\t Current Testing Loss: 0.3504285379880812\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 15.309500694274902\n",
      "\t Current Testing Loss: 0.3451904061378217\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 18.27252221107483\n",
      "\t Current Testing Loss: 0.36091729729879096\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 21.237738847732544\n",
      "\t Current Testing Loss: 0.3595837629596952\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 24.20266056060791\n",
      "\t Current Testing Loss: 0.3510879197238404\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 27.16727042198181\n",
      "\t Current Testing Loss: 0.35265550033731774\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 30.132612943649292\n",
      "\t Current Testing Loss: 0.3571276930299136\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 33.09858775138855\n",
      "\t Current Testing Loss: 0.3599737228842469\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 36.06451773643494\n",
      "\t Current Testing Loss: 0.3623904045209412\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 39.02865028381348\n",
      "\t Current Testing Loss: 0.35988091148492946\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 43.00739145278931\n",
      "\t Current Testing Loss: 0.36070684771588507\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 45.974491119384766\n",
      "\t Current Testing Loss: 0.35949388116795494\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 48.94399690628052\n",
      "\t Current Testing Loss: 0.35751373343956394\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 52.08997464179993\n",
      "\t Current Testing Loss: 0.3543712718967806\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 55.05814838409424\n",
      "\t Current Testing Loss: 0.35206152671608476\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 58.02711033821106\n",
      "\t Current Testing Loss: 0.3508888954267452\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 60.99522376060486\n",
      "\t Current Testing Loss: 0.354840104407932\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 63.9646942615509\n",
      "\t Current Testing Loss: 0.3557618341598466\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 66.93284821510315\n",
      "\t Current Testing Loss: 0.3562348191015321\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 69.90116596221924\n",
      "\t Current Testing Loss: 0.3561594792645731\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 72.87217688560486\n",
      "\t Current Testing Loss: 0.3531891413620399\n",
      "\n",
      "\t Processing Batch #250 ... Running Time 75.84611248970032\n",
      "\t Current Testing Loss: 0.351090284576454\n",
      "\n",
      "\t Processing Batch #260 ... Running Time 78.81767296791077\n",
      "\t Current Testing Loss: 0.35017200488011957\n",
      "\n",
      "\t Processing Batch #270 ... Running Time 81.79102540016174\n",
      "\t Current Testing Loss: 0.3494758457495278\n",
      "\n",
      "\t Processing Batch #280 ... Running Time 84.75998854637146\n",
      "\t Current Testing Loss: 0.34962214455273655\n",
      "\n",
      "\t Processing Batch #290 ... Running Time 87.72983813285828\n",
      "\t Current Testing Loss: 0.35226827562879454\n",
      "\n",
      "\t Processing Batch #300 ... Running Time 90.69763255119324\n",
      "\t Current Testing Loss: 0.3520516809633008\n",
      "\n",
      "\t Processing Batch #310 ... Running Time 93.66568350791931\n",
      "\t Current Testing Loss: 0.3554594016535106\n",
      "\n",
      "\t Processing Batch #320 ... Running Time 96.63410234451294\n",
      "\t Current Testing Loss: 0.3566395596737431\n",
      "\n",
      "\t Processing Batch #330 ... Running Time 99.60323095321655\n",
      "\t Current Testing Loss: 0.35505121345336343\n",
      "\n",
      "\t Processing Batch #340 ... Running Time 102.57101321220398\n",
      "\t Current Testing Loss: 0.3578656895463068\n",
      "\n",
      "\t Processing Batch #350 ... Running Time 105.54034066200256\n",
      "\t Current Testing Loss: 0.3585749166855785\n",
      "\n",
      "\t Processing Batch #360 ... Running Time 108.50922393798828\n",
      "\t Current Testing Loss: 0.3571188761298016\n",
      "\n",
      "\t Processing Batch #370 ... Running Time 111.47737836837769\n",
      "\t Current Testing Loss: 0.3579982735839173\n",
      "\n",
      "\t Processing Batch #380 ... Running Time 114.44684171676636\n",
      "\t Current Testing Loss: 0.35721420821320665\n",
      "\n",
      "\t Processing Batch #390 ... Running Time 117.4161319732666\n",
      "\t Current Testing Loss: 0.35599206729084637\n",
      "\n",
      "\t Processing Batch #400 ... Running Time 120.38510918617249\n",
      "\t Current Testing Loss: 0.3558509152317582\n",
      "\n",
      "\t Processing Batch #410 ... Running Time 123.35291409492493\n",
      "\t Current Testing Loss: 0.35491246911368524\n",
      "\n",
      "\t Processing Batch #420 ... Running Time 126.32194781303406\n",
      "\t Current Testing Loss: 0.3558111684728688\n",
      "\n",
      "\t Processing Batch #430 ... Running Time 129.29008722305298\n",
      "\t Current Testing Loss: 0.3576426939075065\n",
      "\n",
      "\t Processing Batch #440 ... Running Time 132.25867223739624\n",
      "\t Current Testing Loss: 0.3587696742004548\n",
      "\n",
      "\t Processing Batch #450 ... Running Time 135.22819590568542\n",
      "\t Current Testing Loss: 0.3594888217102926\n",
      "\n",
      "\t Processing Batch #460 ... Running Time 138.194682598114\n",
      "\t Current Testing Loss: 0.35907658857016655\n",
      "\n",
      "\t Processing Batch #470 ... Running Time 141.16311264038086\n",
      "\t Current Testing Loss: 0.35796825973724355\n",
      "\n",
      "\t Processing Batch #480 ... Running Time 144.13132429122925\n",
      "\t Current Testing Loss: 0.35963039372790134\n",
      "\n",
      "\t Processing Batch #490 ... Running Time 147.09954833984375\n",
      "\t Current Testing Loss: 0.360263423740014\n",
      "\n",
      "\t Processing Batch #500 ... Running Time 150.06938934326172\n",
      "\t Current Testing Loss: 0.35953460312532093\n",
      "\n",
      "\t Processing Batch #510 ... Running Time 153.03794360160828\n",
      "\t Current Testing Loss: 0.36113709709998915\n",
      "\n",
      "\t Processing Batch #520 ... Running Time 156.0073218345642\n",
      "\t Current Testing Loss: 0.36040499165739986\n",
      "\n",
      "\t Processing Batch #530 ... Running Time 159.4635772705078\n",
      "\t Current Testing Loss: 0.3599156461193063\n",
      "\n",
      "\t Processing Batch #540 ... Running Time 162.4318392276764\n",
      "\t Current Testing Loss: 0.3599938626044779\n",
      "\n",
      "\t Processing Batch #550 ... Running Time 165.39979887008667\n",
      "\t Current Testing Loss: 0.35985017747714604\n",
      "\n",
      "\t Processing Batch #560 ... Running Time 168.36845588684082\n",
      "\t Current Testing Loss: 0.3623545397199199\n",
      "\n",
      "\t Processing Batch #570 ... Running Time 171.33960914611816\n",
      "\t Current Testing Loss: 0.36298587114506137\n",
      "\n",
      "\t Processing Batch #580 ... Running Time 174.3127989768982\n",
      "\t Current Testing Loss: 0.3640522507248565\n",
      "\n",
      "\t Processing Batch #590 ... Running Time 177.28531503677368\n",
      "\t Current Testing Loss: 0.36529146705938476\n",
      "\n",
      "\t Processing Batch #600 ... Running Time 180.25661611557007\n",
      "\t Current Testing Loss: 0.36562181888969486\n",
      "\n",
      "\t Processing Batch #610 ... Running Time 183.2279224395752\n",
      "\t Current Testing Loss: 0.3652168424668952\n",
      "\n",
      "\t Processing Batch #620 ... Running Time 186.19954562187195\n",
      "\t Current Testing Loss: 0.36582881259025585\n",
      "\n",
      "\t Processing Batch #630 ... Running Time 189.17176365852356\n",
      "\t Current Testing Loss: 0.3662366061434504\n",
      "\n",
      "\t Processing Batch #640 ... Running Time 192.14396691322327\n",
      "\t Current Testing Loss: 0.3658295355028184\n",
      "\n",
      "\t Processing Batch #650 ... Running Time 195.11546063423157\n",
      "\t Current Testing Loss: 0.36489046976176276\n",
      "\n",
      "\t Processing Batch #660 ... Running Time 198.08592820167542\n",
      "\t Current Testing Loss: 0.3637245325272996\n",
      "\n",
      "\t Processing Batch #670 ... Running Time 201.0582709312439\n",
      "\t Current Testing Loss: 0.36477164358966574\n",
      "\n",
      "\t Processing Batch #680 ... Running Time 204.0291211605072\n",
      "\t Current Testing Loss: 0.3653521745762461\n",
      "\n",
      "\t Processing Batch #690 ... Running Time 207.00119709968567\n",
      "\t Current Testing Loss: 0.36517082220522956\n",
      "\n",
      "\t Processing Batch #700 ... Running Time 209.97378039360046\n",
      "\t Current Testing Loss: 0.36669221821210524\n",
      "\n",
      "\t Processing Batch #710 ... Running Time 212.94627022743225\n",
      "\t Current Testing Loss: 0.36588055946446335\n",
      "\n",
      "\t Processing Batch #720 ... Running Time 215.91786932945251\n",
      "\t Current Testing Loss: 0.3660053713931786\n",
      "\n",
      "\t Processing Batch #730 ... Running Time 218.88924956321716\n",
      "\t Current Testing Loss: 0.36541747267163077\n",
      "\n",
      "\t Processing Batch #740 ... Running Time 221.86058044433594\n",
      "\t Current Testing Loss: 0.3658064599079803\n",
      "\n",
      "\t Processing Batch #750 ... Running Time 224.83127689361572\n",
      "\t Current Testing Loss: 0.36555485525596315\n",
      "\n",
      "\t Processing Batch #760 ... Running Time 227.8022210597992\n",
      "\t Current Testing Loss: 0.3673964630856464\n",
      "\n",
      "\t Processing Batch #770 ... Running Time 230.773681640625\n",
      "\t Current Testing Loss: 0.36721941256646823\n",
      "\n",
      "\t Processing Batch #780 ... Running Time 233.74370050430298\n",
      "\t Current Testing Loss: 0.3665260265887776\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #790 ... Running Time 236.71504259109497\n",
      "\t Current Testing Loss: 0.367152821070099\n",
      "\n",
      "\t Processing Batch #800 ... Running Time 239.68754816055298\n",
      "\t Current Testing Loss: 0.36747052023808163\n",
      "\n",
      "\t Processing Batch #810 ... Running Time 242.657968044281\n",
      "\t Current Testing Loss: 0.3677907403186423\n",
      "\n",
      "\t Processing Batch #820 ... Running Time 245.62878465652466\n",
      "\t Current Testing Loss: 0.3675882010181842\n",
      "\n",
      "\t Processing Batch #830 ... Running Time 248.60190296173096\n",
      "\t Current Testing Loss: 0.3676017380345312\n",
      "\n",
      "\t Processing Batch #840 ... Running Time 251.57162165641785\n",
      "\t Current Testing Loss: 0.3664025718459193\n",
      "\n",
      "\t Processing Batch #850 ... Running Time 256.02305936813354\n",
      "\t Current Testing Loss: 0.3654314595877634\n",
      "\n",
      "\t Processing Batch #860 ... Running Time 258.9892768859863\n",
      "\t Current Testing Loss: 0.3662694220914658\n",
      "\n",
      "\t Processing Batch #870 ... Running Time 261.95625829696655\n",
      "\t Current Testing Loss: 0.36671887154249316\n",
      "\n",
      "\t Processing Batch #880 ... Running Time 264.9247057437897\n",
      "\t Current Testing Loss: 0.3660978633914646\n",
      "\n",
      "\t Processing Batch #890 ... Running Time 267.89558720588684\n",
      "\t Current Testing Loss: 0.36677827925643935\n",
      "\n",
      "\t Processing Batch #900 ... Running Time 270.86729407310486\n",
      "\t Current Testing Loss: 0.3666111822803396\n",
      "\n",
      "\t Processing Batch #910 ... Running Time 273.8383231163025\n",
      "\t Current Testing Loss: 0.3662472969344301\n",
      "\n",
      "\t Processing Batch #920 ... Running Time 276.8092770576477\n",
      "\t Current Testing Loss: 0.36654022600942665\n",
      "\n",
      "\t Processing Batch #930 ... Running Time 279.7809166908264\n",
      "\t Current Testing Loss: 0.36536909069637863\n",
      "\n",
      "\t Processing Batch #940 ... Running Time 282.75233721733093\n",
      "\t Current Testing Loss: 0.3653534340229792\n",
      "\n",
      "\t Processing Batch #950 ... Running Time 285.72229981422424\n",
      "\t Current Testing Loss: 0.3659472918778376\n",
      "\n",
      "\t Processing Batch #960 ... Running Time 288.6933259963989\n",
      "\t Current Testing Loss: 0.3661202674345903\n",
      "\n",
      "\t Processing Batch #970 ... Running Time 291.66442465782166\n",
      "\t Current Testing Loss: 0.3650095004753337\n",
      "\n",
      "\t Processing Batch #980 ... Running Time 294.63458466529846\n",
      "\t Current Testing Loss: 0.36533006430659576\n",
      "\n",
      "\t Processing Batch #990 ... Running Time 297.60669231414795\n",
      "\t Current Testing Loss: 0.36556127697875956\n",
      "\n",
      "\t Processing Batch #1000 ... Running Time 300.57805490493774\n",
      "\t Current Testing Loss: 0.36487853369646733\n",
      "\n",
      "\t Processing Batch #1010 ... Running Time 303.5493383407593\n",
      "\t Current Testing Loss: 0.3648506503894881\n",
      "\n",
      "\t Processing Batch #1020 ... Running Time 306.5204074382782\n",
      "\t Current Testing Loss: 0.36580312693898404\n",
      "\n",
      "\t Processing Batch #1030 ... Running Time 309.49110651016235\n",
      "\t Current Testing Loss: 0.3662176428093538\n",
      "\n",
      "\t Processing Batch #1040 ... Running Time 312.4623646736145\n",
      "\t Current Testing Loss: 0.36628675455447574\n",
      "\n",
      "\t Processing Batch #1050 ... Running Time 315.433385848999\n",
      "\t Current Testing Loss: 0.3652018090899676\n",
      "\n",
      "\t Processing Batch #1060 ... Running Time 318.4052269458771\n",
      "\t Current Testing Loss: 0.364962596362009\n",
      "\n",
      "\t Processing Batch #1070 ... Running Time 321.37714433670044\n",
      "\t Current Testing Loss: 0.36467849430875954\n",
      "\n",
      "\t Processing Batch #1080 ... Running Time 324.3488371372223\n",
      "\t Current Testing Loss: 0.36487899235795207\n",
      "\n",
      "\t Processing Batch #1090 ... Running Time 327.3231313228607\n",
      "\t Current Testing Loss: 0.3646652842509047\n",
      "\n",
      "\t Processing Batch #1100 ... Running Time 330.2934317588806\n",
      "\t Current Testing Loss: 0.36396573438265994\n",
      "\n",
      "\t Processing Batch #1110 ... Running Time 333.26605319976807\n",
      "\t Current Testing Loss: 0.3637425349094812\n",
      "\n",
      "\t Processing Batch #1120 ... Running Time 336.23669934272766\n",
      "\t Current Testing Loss: 0.3641629392824515\n",
      "\n",
      "\t Processing Batch #1130 ... Running Time 339.2067289352417\n",
      "\t Current Testing Loss: 0.3633498673473018\n",
      "\n",
      "\t Processing Batch #1140 ... Running Time 342.1789503097534\n",
      "\t Current Testing Loss: 0.36377855531028963\n",
      "\n",
      "\t Processing Batch #1150 ... Running Time 345.14995670318604\n",
      "\t Current Testing Loss: 0.3636459005716622\n",
      "\n",
      "\t Processing Batch #1160 ... Running Time 348.1219701766968\n",
      "\t Current Testing Loss: 0.3637295957625836\n",
      "\n",
      "\t Processing Batch #1170 ... Running Time 351.09387159347534\n",
      "\t Current Testing Loss: 0.3646270382408145\n",
      "\n",
      "\t Processing Batch #1180 ... Running Time 354.06566166877747\n",
      "\t Current Testing Loss: 0.36438229776269093\n",
      "\n",
      "\t Processing Batch #1190 ... Running Time 357.03693747520447\n",
      "\t Current Testing Loss: 0.3646507335218384\n",
      "\n",
      "\t Processing Batch #1200 ... Running Time 360.01358437538147\n",
      "\t Current Testing Loss: 0.3655095005331935\n",
      "\n",
      "\t Processing Batch #1210 ... Running Time 362.9948573112488\n",
      "\t Current Testing Loss: 0.36540432115301524\n",
      "\n",
      "\t Processing Batch #1220 ... Running Time 365.96565222740173\n",
      "\t Current Testing Loss: 0.3652290500346556\n",
      "\n",
      "\t Processing Batch #1230 ... Running Time 370.42319416999817\n",
      "\t Current Testing Loss: 0.3657550635852773\n",
      "\n",
      "\t Processing Batch #1240 ... Running Time 373.3918426036835\n",
      "\t Current Testing Loss: 0.3660457403302241\n",
      "\n",
      "\t Processing Batch #1250 ... Running Time 376.36034321784973\n",
      "\t Current Testing Loss: 0.36629520344445937\n",
      "\n",
      "\t Processing Batch #1260 ... Running Time 379.32861709594727\n",
      "\t Current Testing Loss: 0.367050415092546\n",
      "\n",
      "\t Processing Batch #1270 ... Running Time 382.29622077941895\n",
      "\t Current Testing Loss: 0.3670061397638075\n",
      "\n",
      "\t Processing Batch #1280 ... Running Time 385.26308608055115\n",
      "\t Current Testing Loss: 0.3685209752321476\n",
      "\n",
      "\t Processing Batch #1290 ... Running Time 388.2333891391754\n",
      "\t Current Testing Loss: 0.36845278726944064\n",
      "\n",
      "\t Processing Batch #1300 ... Running Time 391.2034730911255\n",
      "\t Current Testing Loss: 0.36792767862520886\n",
      "\n",
      "\t Processing Batch #1310 ... Running Time 394.1748752593994\n",
      "\t Current Testing Loss: 0.3670522501076977\n",
      "\n",
      "\t Processing Batch #1320 ... Running Time 397.144091129303\n",
      "\t Current Testing Loss: 0.3668820063226111\n",
      "\n",
      "\t Processing Batch #1330 ... Running Time 400.11297941207886\n",
      "\t Current Testing Loss: 0.3666383342762848\n",
      "\n",
      "\t Processing Batch #1340 ... Running Time 403.0829246044159\n",
      "\t Current Testing Loss: 0.36651933959099803\n",
      "\n",
      "\t Processing Batch #1350 ... Running Time 407.3823049068451\n",
      "\t Current Testing Loss: 0.36627841257468846\n",
      "\n",
      "\t Processing Batch #1360 ... Running Time 410.34949111938477\n",
      "\t Current Testing Loss: 0.3666930629089639\n",
      "\n",
      "\t Processing Batch #1370 ... Running Time 413.316148519516\n",
      "\t Current Testing Loss: 0.36653208152481365\n",
      "\n",
      "\t Processing Batch #1380 ... Running Time 416.2823951244354\n",
      "\t Current Testing Loss: 0.3657289982883288\n",
      "\n",
      "\t Processing Batch #1390 ... Running Time 419.24886083602905\n",
      "\t Current Testing Loss: 0.3666060813210535\n",
      "\n",
      "\t Processing Batch #1400 ... Running Time 422.21580719947815\n",
      "\t Current Testing Loss: 0.36654978186732357\n",
      "\n",
      "\t Processing Batch #1410 ... Running Time 425.1854581832886\n",
      "\t Current Testing Loss: 0.36629696608825324\n",
      "\n",
      "\t Processing Batch #1420 ... Running Time 428.1552107334137\n",
      "\t Current Testing Loss: 0.3675680861563896\n",
      "\n",
      "\t Processing Batch #1430 ... Running Time 431.1258158683777\n",
      "\t Current Testing Loss: 0.3672990717545704\n",
      "\n",
      "\t Processing Batch #1440 ... Running Time 434.09583735466003\n",
      "\t Current Testing Loss: 0.367371460276809\n",
      "\n",
      "\t Processing Batch #1450 ... Running Time 437.06381344795227\n",
      "\t Current Testing Loss: 0.3668171146552083\n",
      "\n",
      "\t Processing Batch #1460 ... Running Time 440.0327377319336\n",
      "\t Current Testing Loss: 0.36667119368176443\n",
      "\n",
      "\t Processing Batch #1470 ... Running Time 444.4201943874359\n",
      "\t Current Testing Loss: 0.36735657958363205\n",
      "\n",
      "\t Processing Batch #1480 ... Running Time 447.3870759010315\n",
      "\t Current Testing Loss: 0.36706975935269737\n",
      "\n",
      "\t Processing Batch #1490 ... Running Time 450.3536877632141\n",
      "\t Current Testing Loss: 0.3659862911442356\n",
      "\n",
      "\t Processing Batch #1500 ... Running Time 453.3190584182739\n",
      "\t Current Testing Loss: 0.36650912458949453\n",
      "\n",
      "\t Processing Batch #1510 ... Running Time 456.2871515750885\n",
      "\t Current Testing Loss: 0.3667962995908223\n",
      "\n",
      "\t Processing Batch #1520 ... Running Time 459.2543525695801\n",
      "\t Current Testing Loss: 0.3679018009289913\n",
      "\n",
      "\t Processing Batch #1530 ... Running Time 462.22006368637085\n",
      "\t Current Testing Loss: 0.36923544571426553\n",
      "\n",
      "\t Processing Batch #1540 ... Running Time 465.1874098777771\n",
      "\t Current Testing Loss: 0.3687907406222619\n",
      "\n",
      "\t Processing Batch #1550 ... Running Time 468.1526675224304\n",
      "\t Current Testing Loss: 0.3680559882232445\n",
      "\n",
      "\t Processing Batch #1560 ... Running Time 471.1209273338318\n",
      "\t Current Testing Loss: 0.3678080943997169\n",
      "\n",
      "\t Processing Batch #1570 ... Running Time 474.0855963230133\n",
      "\t Current Testing Loss: 0.36784049474238817\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #1580 ... Running Time 477.0623049736023\n",
      "\t Current Testing Loss: 0.3678744202909743\n",
      "\n",
      "\t Processing Batch #1590 ... Running Time 480.03566789627075\n",
      "\t Current Testing Loss: 0.3679590145820233\n",
      "\n",
      "\t Processing Batch #1600 ... Running Time 483.00709676742554\n",
      "\t Current Testing Loss: 0.3677571854815306\n",
      "\n",
      "\t Processing Batch #1610 ... Running Time 485.9797215461731\n",
      "\t Current Testing Loss: 0.36727893315278765\n",
      "\n",
      "\t Processing Batch #1620 ... Running Time 488.94954013824463\n",
      "\t Current Testing Loss: 0.36707266835400454\n",
      "\n",
      "\t Processing Batch #1630 ... Running Time 491.9194977283478\n",
      "\t Current Testing Loss: 0.36750439266433826\n",
      "\n",
      "\t Processing Batch #1640 ... Running Time 494.8891189098358\n",
      "\t Current Testing Loss: 0.3684834147724598\n",
      "\n",
      "\t Processing Batch #1650 ... Running Time 497.8590290546417\n",
      "\t Current Testing Loss: 0.3682875179638832\n",
      "\n",
      "\t Processing Batch #1660 ... Running Time 500.83032178878784\n",
      "\t Current Testing Loss: 0.36817217748085107\n",
      "\n",
      "\t Processing Batch #1670 ... Running Time 503.8006103038788\n",
      "\t Current Testing Loss: 0.36801259442361295\n",
      "\n",
      "\t Processing Batch #1680 ... Running Time 506.77237248420715\n",
      "\t Current Testing Loss: 0.3681059073521614\n",
      "\n",
      "\t Processing Batch #1690 ... Running Time 509.7428226470947\n",
      "\t Current Testing Loss: 0.36821798327924726\n",
      "\n",
      "\t Processing Batch #1700 ... Running Time 512.7125849723816\n",
      "\t Current Testing Loss: 0.3679452588401739\n",
      "\n",
      "\t Processing Batch #1710 ... Running Time 515.6833915710449\n",
      "\t Current Testing Loss: 0.36796259381691315\n",
      "\n",
      "\t Processing Batch #1720 ... Running Time 518.6551463603973\n",
      "\t Current Testing Loss: 0.3679183263489668\n",
      "\n",
      "\t Processing Batch #1730 ... Running Time 521.6264245510101\n",
      "\t Current Testing Loss: 0.36789945622552067\n",
      "\n",
      "\t Processing Batch #1740 ... Running Time 524.5979278087616\n",
      "\t Current Testing Loss: 0.367913393328359\n",
      "\n",
      "\t Processing Batch #1750 ... Running Time 527.570600271225\n",
      "\t Current Testing Loss: 0.36791169677655194\n",
      "\n",
      "\t Processing Batch #1760 ... Running Time 530.5437586307526\n",
      "\t Current Testing Loss: 0.36814452300122147\n",
      "\n",
      "\t Processing Batch #1770 ... Running Time 533.5153329372406\n",
      "\t Current Testing Loss: 0.3683515320422116\n",
      "\n",
      "\t Processing Batch #1780 ... Running Time 536.487184047699\n",
      "\t Current Testing Loss: 0.3687522489847359\n",
      "\n",
      "\t Processing Batch #1790 ... Running Time 539.4587178230286\n",
      "\t Current Testing Loss: 0.3683090630059699\n",
      "\n",
      "\t Processing Batch #1800 ... Running Time 542.4307193756104\n",
      "\t Current Testing Loss: 0.3684395605813027\n",
      "\n",
      "\t Processing Batch #1810 ... Running Time 545.4025518894196\n",
      "\t Current Testing Loss: 0.3689325295701368\n",
      "\n",
      "\t Processing Batch #1820 ... Running Time 548.3734152317047\n",
      "\t Current Testing Loss: 0.36918761991353105\n",
      "\n",
      "\t Processing Batch #1830 ... Running Time 551.3442890644073\n",
      "\t Current Testing Loss: 0.3695697975245771\n",
      "\n",
      "\t Processing Batch #1840 ... Running Time 554.3151814937592\n",
      "\t Current Testing Loss: 0.3692207451907757\n",
      "\n",
      "\t Processing Batch #1850 ... Running Time 557.2953200340271\n",
      "\t Current Testing Loss: 0.36881147443982537\n",
      "\n",
      "\t Processing Batch #1860 ... Running Time 560.2663977146149\n",
      "\t Current Testing Loss: 0.3686075551892755\n",
      "\n",
      "\t Processing Batch #1870 ... Running Time 563.2374432086945\n",
      "\t Current Testing Loss: 0.3681289473103528\n",
      "\n",
      "\t Processing Batch #1880 ... Running Time 566.2087426185608\n",
      "\t Current Testing Loss: 0.36892255569339305\n",
      "\n",
      "\t Processing Batch #1890 ... Running Time 569.1798005104065\n",
      "\t Current Testing Loss: 0.36870202841951316\n",
      "\n",
      "\t Processing Batch #1900 ... Running Time 572.1504971981049\n",
      "\t Current Testing Loss: 0.36847744078847183\n",
      "\n",
      "\t Processing Batch #1910 ... Running Time 575.1211743354797\n",
      "\t Current Testing Loss: 0.36883180766073526\n",
      "\n",
      "\t Processing Batch #1920 ... Running Time 578.0909631252289\n",
      "\t Current Testing Loss: 0.3692751749089489\n",
      "\n",
      "\t Processing Batch #1930 ... Running Time 581.0619158744812\n",
      "\t Current Testing Loss: 0.3695923449989584\n",
      "\n",
      "\t Processing Batch #1940 ... Running Time 584.0328757762909\n",
      "\t Current Testing Loss: 0.3695390400894673\n",
      "\n",
      "\t Processing Batch #1950 ... Running Time 587.0042812824249\n",
      "\t Current Testing Loss: 0.36959897958769483\n",
      "\n",
      "\t Processing Batch #1960 ... Running Time 589.9752435684204\n",
      "\t Current Testing Loss: 0.36965168236903484\n",
      "\n",
      "\t Processing Batch #1970 ... Running Time 592.9468579292297\n",
      "\t Current Testing Loss: 0.36919751780543275\n",
      "\n",
      "\t Processing Batch #1980 ... Running Time 595.9191946983337\n",
      "\t Current Testing Loss: 0.36896496691309943\n",
      "\n",
      "\t Processing Batch #1990 ... Running Time 598.8915333747864\n",
      "\t Current Testing Loss: 0.3689689562042744\n",
      "\n",
      "\t Processing Batch #2000 ... Running Time 601.8637132644653\n",
      "\t Current Testing Loss: 0.36906520249894537\n",
      "\n",
      "\t Processing Batch #2010 ... Running Time 604.8354890346527\n",
      "\t Current Testing Loss: 0.3688856909382201\n",
      "\n",
      "\t Processing Batch #2020 ... Running Time 607.8064258098602\n",
      "\t Current Testing Loss: 0.36891103448702933\n",
      "\n",
      "\t Processing Batch #2030 ... Running Time 610.7789132595062\n",
      "\t Current Testing Loss: 0.3692424887381329\n",
      "\n",
      "\t Processing Batch #2040 ... Running Time 613.7507181167603\n",
      "\t Current Testing Loss: 0.3690123476073445\n",
      "\n",
      "\t Processing Batch #2050 ... Running Time 616.7221295833588\n",
      "\t Current Testing Loss: 0.36873357972790416\n",
      "\n",
      "\t Processing Batch #2060 ... Running Time 619.6935822963715\n",
      "\t Current Testing Loss: 0.3684667889218577\n",
      "\n",
      "\t Processing Batch #2070 ... Running Time 622.4531254768372\n",
      "\t Current Testing Loss: 0.36892684561504885\n",
      "\n",
      "******* Final Testing Loss: 0.36892684561504885 *******\n",
      "\n",
      "Loading: melanoma_ResNeSt_2e_0b.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /u/home/a/andrewma/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #0 ... Running Time 0.5215914249420166\n",
      "\t Current Testing Loss: 0.32885774970054626\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 3.489450454711914\n",
      "\t Current Testing Loss: 0.38734248822385614\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 6.457778215408325\n",
      "\t Current Testing Loss: 0.3789479363532293\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 9.426713466644287\n",
      "\t Current Testing Loss: 0.40477355833976497\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 12.396132469177246\n",
      "\t Current Testing Loss: 0.3955463223704478\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 15.36583161354065\n",
      "\t Current Testing Loss: 0.38641510392520945\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 18.335737705230713\n",
      "\t Current Testing Loss: 0.3971469879883235\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 21.3052761554718\n",
      "\t Current Testing Loss: 0.4060840148111464\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 24.27415633201599\n",
      "\t Current Testing Loss: 0.40756264706084755\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 27.242971897125244\n",
      "\t Current Testing Loss: 0.4111823979151118\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 30.212850332260132\n",
      "\t Current Testing Loss: 0.413941326339056\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 33.182244062423706\n",
      "\t Current Testing Loss: 0.4124356143780657\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 36.15216684341431\n",
      "\t Current Testing Loss: 0.41313155605034396\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 39.12325620651245\n",
      "\t Current Testing Loss: 0.4115111993355605\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 42.093406438827515\n",
      "\t Current Testing Loss: 0.40797626671004805\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 45.06347036361694\n",
      "\t Current Testing Loss: 0.40466824790697226\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 48.03295373916626\n",
      "\t Current Testing Loss: 0.4040517344700624\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 51.002424478530884\n",
      "\t Current Testing Loss: 0.40471462142920633\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 53.97438168525696\n",
      "\t Current Testing Loss: 0.401683792019088\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 56.94861698150635\n",
      "\t Current Testing Loss: 0.40057040290682727\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 59.92181372642517\n",
      "\t Current Testing Loss: 0.4021862277492362\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 62.8948495388031\n",
      "\t Current Testing Loss: 0.4040806176961881\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 65.86828064918518\n",
      "\t Current Testing Loss: 0.40404482017275434\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 68.84201121330261\n",
      "\t Current Testing Loss: 0.40436586328012086\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 71.81563425064087\n",
      "\t Current Testing Loss: 0.4022793374430095\n",
      "\n",
      "\t Processing Batch #250 ... Running Time 74.79031538963318\n",
      "\t Current Testing Loss: 0.3984631969931116\n",
      "\n",
      "\t Processing Batch #260 ... Running Time 77.76296973228455\n",
      "\t Current Testing Loss: 0.396971398386462\n",
      "\n",
      "\t Processing Batch #270 ... Running Time 80.73577165603638\n",
      "\t Current Testing Loss: 0.39539488424234287\n",
      "\n",
      "\t Processing Batch #280 ... Running Time 83.70933413505554\n",
      "\t Current Testing Loss: 0.3944249886528877\n",
      "\n",
      "\t Processing Batch #290 ... Running Time 86.68293952941895\n",
      "\t Current Testing Loss: 0.39775632250145126\n",
      "\n",
      "\t Processing Batch #300 ... Running Time 89.65730619430542\n",
      "\t Current Testing Loss: 0.3985454481206463\n",
      "\n",
      "\t Processing Batch #310 ... Running Time 92.63295292854309\n",
      "\t Current Testing Loss: 0.4012434296190164\n",
      "\n",
      "\t Processing Batch #320 ... Running Time 95.60815668106079\n",
      "\t Current Testing Loss: 0.4015122090823182\n",
      "\n",
      "\t Processing Batch #330 ... Running Time 98.58298945426941\n",
      "\t Current Testing Loss: 0.40015744532703273\n",
      "\n",
      "\t Processing Batch #340 ... Running Time 101.55794787406921\n",
      "\t Current Testing Loss: 0.40228784805344\n",
      "\n",
      "\t Processing Batch #350 ... Running Time 104.53304767608643\n",
      "\t Current Testing Loss: 0.4021749581791397\n",
      "\n",
      "\t Processing Batch #360 ... Running Time 107.50828409194946\n",
      "\t Current Testing Loss: 0.3985375527529835\n",
      "\n",
      "\t Processing Batch #370 ... Running Time 110.48216891288757\n",
      "\t Current Testing Loss: 0.40010404560684837\n",
      "\n",
      "\t Processing Batch #380 ... Running Time 113.45678424835205\n",
      "\t Current Testing Loss: 0.40008280305020766\n",
      "\n",
      "\t Processing Batch #390 ... Running Time 116.43085217475891\n",
      "\t Current Testing Loss: 0.39922709056102407\n",
      "\n",
      "\t Processing Batch #400 ... Running Time 119.40524339675903\n",
      "\t Current Testing Loss: 0.3984400829732269\n",
      "\n",
      "\t Processing Batch #410 ... Running Time 122.3787431716919\n",
      "\t Current Testing Loss: 0.39665621256233713\n",
      "\n",
      "\t Processing Batch #420 ... Running Time 125.35274887084961\n",
      "\t Current Testing Loss: 0.3976227940013743\n",
      "\n",
      "\t Processing Batch #430 ... Running Time 128.32571840286255\n",
      "\t Current Testing Loss: 0.39979329077730047\n",
      "\n",
      "\t Processing Batch #440 ... Running Time 131.3003911972046\n",
      "\t Current Testing Loss: 0.40023033386793266\n",
      "\n",
      "\t Processing Batch #450 ... Running Time 134.27506494522095\n",
      "\t Current Testing Loss: 0.3997814671989291\n",
      "\n",
      "\t Processing Batch #460 ... Running Time 137.24870467185974\n",
      "\t Current Testing Loss: 0.4003126600074406\n",
      "\n",
      "\t Processing Batch #470 ... Running Time 140.22203993797302\n",
      "\t Current Testing Loss: 0.40048307482540735\n",
      "\n",
      "\t Processing Batch #480 ... Running Time 143.19642448425293\n",
      "\t Current Testing Loss: 0.4016250607532424\n",
      "\n",
      "\t Processing Batch #490 ... Running Time 146.1717414855957\n",
      "\t Current Testing Loss: 0.4021345394722313\n",
      "\n",
      "\t Processing Batch #500 ... Running Time 149.14592671394348\n",
      "\t Current Testing Loss: 0.4004687522491533\n",
      "\n",
      "\t Processing Batch #510 ... Running Time 152.12221479415894\n",
      "\t Current Testing Loss: 0.4014207906364928\n",
      "\n",
      "\t Processing Batch #520 ... Running Time 155.09774661064148\n",
      "\t Current Testing Loss: 0.4016917169580304\n",
      "\n",
      "\t Processing Batch #530 ... Running Time 158.07286286354065\n",
      "\t Current Testing Loss: 0.40080662401438655\n",
      "\n",
      "\t Processing Batch #540 ... Running Time 161.0482566356659\n",
      "\t Current Testing Loss: 0.40080704179191323\n",
      "\n",
      "\t Processing Batch #550 ... Running Time 164.02388787269592\n",
      "\t Current Testing Loss: 0.4012984205476818\n",
      "\n",
      "\t Processing Batch #560 ... Running Time 166.9995014667511\n",
      "\t Current Testing Loss: 0.4033379233218132\n",
      "\n",
      "\t Processing Batch #570 ... Running Time 169.97353219985962\n",
      "\t Current Testing Loss: 0.4032129499859025\n",
      "\n",
      "\t Processing Batch #580 ... Running Time 172.94923663139343\n",
      "\t Current Testing Loss: 0.4045005046126149\n",
      "\n",
      "\t Processing Batch #590 ... Running Time 175.92547369003296\n",
      "\t Current Testing Loss: 0.4059144681959386\n",
      "\n",
      "\t Processing Batch #600 ... Running Time 178.89970779418945\n",
      "\t Current Testing Loss: 0.40592662778105\n",
      "\n",
      "\t Processing Batch #610 ... Running Time 181.8755226135254\n",
      "\t Current Testing Loss: 0.4061620138500209\n",
      "\n",
      "\t Processing Batch #620 ... Running Time 184.8495225906372\n",
      "\t Current Testing Loss: 0.4064324665784644\n",
      "\n",
      "\t Processing Batch #630 ... Running Time 187.82461190223694\n",
      "\t Current Testing Loss: 0.4070199982229201\n",
      "\n",
      "\t Processing Batch #640 ... Running Time 190.7987003326416\n",
      "\t Current Testing Loss: 0.40716686167183236\n",
      "\n",
      "\t Processing Batch #650 ... Running Time 193.77321529388428\n",
      "\t Current Testing Loss: 0.4047588887188109\n",
      "\n",
      "\t Processing Batch #660 ... Running Time 196.74945998191833\n",
      "\t Current Testing Loss: 0.40340239757533874\n",
      "\n",
      "\t Processing Batch #670 ... Running Time 199.72400975227356\n",
      "\t Current Testing Loss: 0.40383184501337754\n",
      "\n",
      "\t Processing Batch #680 ... Running Time 202.69874548912048\n",
      "\t Current Testing Loss: 0.4051652991277181\n",
      "\n",
      "\t Processing Batch #690 ... Running Time 205.67427229881287\n",
      "\t Current Testing Loss: 0.4042923218757992\n",
      "\n",
      "\t Processing Batch #700 ... Running Time 208.64957070350647\n",
      "\t Current Testing Loss: 0.4063233420167772\n",
      "\n",
      "\t Processing Batch #710 ... Running Time 211.62422442436218\n",
      "\t Current Testing Loss: 0.40542244442781816\n",
      "\n",
      "\t Processing Batch #720 ... Running Time 214.59962344169617\n",
      "\t Current Testing Loss: 0.4063865413550861\n",
      "\n",
      "\t Processing Batch #730 ... Running Time 217.5743808746338\n",
      "\t Current Testing Loss: 0.4061997600725346\n",
      "\n",
      "\t Processing Batch #740 ... Running Time 220.55018949508667\n",
      "\t Current Testing Loss: 0.4068221534714203\n",
      "\n",
      "\t Processing Batch #750 ... Running Time 223.52564907073975\n",
      "\t Current Testing Loss: 0.40595846459487783\n",
      "\n",
      "\t Processing Batch #760 ... Running Time 226.50173950195312\n",
      "\t Current Testing Loss: 0.4081089406148206\n",
      "\n",
      "\t Processing Batch #770 ... Running Time 229.4764015674591\n",
      "\t Current Testing Loss: 0.40801127094548345\n",
      "\n",
      "\t Processing Batch #780 ... Running Time 232.45208287239075\n",
      "\t Current Testing Loss: 0.40825674727692646\n",
      "\n",
      "\t Processing Batch #790 ... Running Time 235.42813086509705\n",
      "\t Current Testing Loss: 0.4080918967610816\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #800 ... Running Time 238.404066324234\n",
      "\t Current Testing Loss: 0.4080923158354527\n",
      "\n",
      "\t Processing Batch #810 ... Running Time 241.37890338897705\n",
      "\t Current Testing Loss: 0.4076392352764762\n",
      "\n",
      "\t Processing Batch #820 ... Running Time 244.3535294532776\n",
      "\t Current Testing Loss: 0.40774456445177637\n",
      "\n",
      "\t Processing Batch #830 ... Running Time 247.32856726646423\n",
      "\t Current Testing Loss: 0.4075612928785572\n",
      "\n",
      "\t Processing Batch #840 ... Running Time 250.3042230606079\n",
      "\t Current Testing Loss: 0.40693723042711494\n",
      "\n",
      "\t Processing Batch #850 ... Running Time 253.27906250953674\n",
      "\t Current Testing Loss: 0.4063817890039202\n",
      "\n",
      "\t Processing Batch #860 ... Running Time 256.2537236213684\n",
      "\t Current Testing Loss: 0.40713307017441\n",
      "\n",
      "\t Processing Batch #870 ... Running Time 259.22948956489563\n",
      "\t Current Testing Loss: 0.40757399029285574\n",
      "\n",
      "\t Processing Batch #880 ... Running Time 262.2052252292633\n",
      "\t Current Testing Loss: 0.4067604365789254\n",
      "\n",
      "\t Processing Batch #890 ... Running Time 265.1806490421295\n",
      "\t Current Testing Loss: 0.4076057530596735\n",
      "\n",
      "\t Processing Batch #900 ... Running Time 268.15505290031433\n",
      "\t Current Testing Loss: 0.40737901815729055\n",
      "\n",
      "\t Processing Batch #910 ... Running Time 271.13000440597534\n",
      "\t Current Testing Loss: 0.4068707460711345\n",
      "\n",
      "\t Processing Batch #920 ... Running Time 274.1049909591675\n",
      "\t Current Testing Loss: 0.407465513031141\n",
      "\n",
      "\t Processing Batch #930 ... Running Time 277.08081126213074\n",
      "\t Current Testing Loss: 0.40662303417280843\n",
      "\n",
      "\t Processing Batch #940 ... Running Time 280.0562582015991\n",
      "\t Current Testing Loss: 0.40608394925861635\n",
      "\n",
      "\t Processing Batch #950 ... Running Time 283.03259015083313\n",
      "\t Current Testing Loss: 0.40737412698355385\n",
      "\n",
      "\t Processing Batch #960 ... Running Time 286.0090866088867\n",
      "\t Current Testing Loss: 0.4075528746945677\n",
      "\n",
      "\t Processing Batch #970 ... Running Time 288.985018491745\n",
      "\t Current Testing Loss: 0.40664122388702956\n",
      "\n",
      "\t Processing Batch #980 ... Running Time 291.9604206085205\n",
      "\t Current Testing Loss: 0.40735737497136987\n",
      "\n",
      "\t Processing Batch #990 ... Running Time 294.935683965683\n",
      "\t Current Testing Loss: 0.408211525007105\n",
      "\n",
      "\t Processing Batch #1000 ... Running Time 297.9114787578583\n",
      "\t Current Testing Loss: 0.40771014493453756\n",
      "\n",
      "\t Processing Batch #1010 ... Running Time 300.8870265483856\n",
      "\t Current Testing Loss: 0.40778107094393284\n",
      "\n",
      "\t Processing Batch #1020 ... Running Time 303.8622086048126\n",
      "\t Current Testing Loss: 0.40901001179399266\n",
      "\n",
      "\t Processing Batch #1030 ... Running Time 306.8386080265045\n",
      "\t Current Testing Loss: 0.4100030095729287\n",
      "\n",
      "\t Processing Batch #1040 ... Running Time 309.82224678993225\n",
      "\t Current Testing Loss: 0.41028812257832453\n",
      "\n",
      "\t Processing Batch #1050 ... Running Time 312.79749035835266\n",
      "\t Current Testing Loss: 0.4093038631509192\n",
      "\n",
      "\t Processing Batch #1060 ... Running Time 315.77338314056396\n",
      "\t Current Testing Loss: 0.4102305893581729\n",
      "\n",
      "\t Processing Batch #1070 ... Running Time 318.7495765686035\n",
      "\t Current Testing Loss: 0.4099149563802836\n",
      "\n",
      "\t Processing Batch #1080 ... Running Time 321.7253179550171\n",
      "\t Current Testing Loss: 0.41010016807381694\n",
      "\n",
      "\t Processing Batch #1090 ... Running Time 324.7015309333801\n",
      "\t Current Testing Loss: 0.410528415481122\n",
      "\n",
      "\t Processing Batch #1100 ... Running Time 327.6766073703766\n",
      "\t Current Testing Loss: 0.40955974979767684\n",
      "\n",
      "\t Processing Batch #1110 ... Running Time 330.6504604816437\n",
      "\t Current Testing Loss: 0.4093488180200116\n",
      "\n",
      "\t Processing Batch #1120 ... Running Time 333.625360250473\n",
      "\t Current Testing Loss: 0.4100130631668987\n",
      "\n",
      "\t Processing Batch #1130 ... Running Time 336.6011645793915\n",
      "\t Current Testing Loss: 0.4094215220352594\n",
      "\n",
      "\t Processing Batch #1140 ... Running Time 339.5765378475189\n",
      "\t Current Testing Loss: 0.4096482785240691\n",
      "\n",
      "\t Processing Batch #1150 ... Running Time 342.5523142814636\n",
      "\t Current Testing Loss: 0.4100250109356968\n",
      "\n",
      "\t Processing Batch #1160 ... Running Time 345.5281116962433\n",
      "\t Current Testing Loss: 0.40957490193058355\n",
      "\n",
      "\t Processing Batch #1170 ... Running Time 348.50399923324585\n",
      "\t Current Testing Loss: 0.41078129553901754\n",
      "\n",
      "\t Processing Batch #1180 ... Running Time 351.47906470298767\n",
      "\t Current Testing Loss: 0.4102688588258778\n",
      "\n",
      "\t Processing Batch #1190 ... Running Time 354.4558742046356\n",
      "\t Current Testing Loss: 0.4106419110165535\n",
      "\n",
      "\t Processing Batch #1200 ... Running Time 357.4319715499878\n",
      "\t Current Testing Loss: 0.41115346715338125\n",
      "\n",
      "\t Processing Batch #1210 ... Running Time 360.40809869766235\n",
      "\t Current Testing Loss: 0.410815033910936\n",
      "\n",
      "\t Processing Batch #1220 ... Running Time 363.3836660385132\n",
      "\t Current Testing Loss: 0.4106193062384826\n",
      "\n",
      "\t Processing Batch #1230 ... Running Time 366.3593280315399\n",
      "\t Current Testing Loss: 0.4110773806221886\n",
      "\n",
      "\t Processing Batch #1240 ... Running Time 369.33440256118774\n",
      "\t Current Testing Loss: 0.41149789503776285\n",
      "\n",
      "\t Processing Batch #1250 ... Running Time 372.3109464645386\n",
      "\t Current Testing Loss: 0.4121983634422628\n",
      "\n",
      "\t Processing Batch #1260 ... Running Time 375.2877995967865\n",
      "\t Current Testing Loss: 0.41226667056449534\n",
      "\n",
      "\t Processing Batch #1270 ... Running Time 378.26535868644714\n",
      "\t Current Testing Loss: 0.4120274135721283\n",
      "\n",
      "\t Processing Batch #1280 ... Running Time 381.24066972732544\n",
      "\t Current Testing Loss: 0.4131964786410052\n",
      "\n",
      "\t Processing Batch #1290 ... Running Time 384.2229290008545\n",
      "\t Current Testing Loss: 0.4126247152582238\n",
      "\n",
      "\t Processing Batch #1300 ... Running Time 387.2105886936188\n",
      "\t Current Testing Loss: 0.41197417408434084\n",
      "\n",
      "\t Processing Batch #1310 ... Running Time 390.18667221069336\n",
      "\t Current Testing Loss: 0.41073866068043846\n",
      "\n",
      "\t Processing Batch #1320 ... Running Time 393.16306257247925\n",
      "\t Current Testing Loss: 0.4106059112867381\n",
      "\n",
      "\t Processing Batch #1330 ... Running Time 396.1406817436218\n",
      "\t Current Testing Loss: 0.410204996266014\n",
      "\n",
      "\t Processing Batch #1340 ... Running Time 399.1173198223114\n",
      "\t Current Testing Loss: 0.40971229034826706\n",
      "\n",
      "\t Processing Batch #1350 ... Running Time 402.0945248603821\n",
      "\t Current Testing Loss: 0.40954191232724685\n",
      "\n",
      "\t Processing Batch #1360 ... Running Time 405.07187247276306\n",
      "\t Current Testing Loss: 0.40951839940106494\n",
      "\n",
      "\t Processing Batch #1370 ... Running Time 408.0472950935364\n",
      "\t Current Testing Loss: 0.4092114392333792\n",
      "\n",
      "\t Processing Batch #1380 ... Running Time 411.0229752063751\n",
      "\t Current Testing Loss: 0.40842235713096564\n",
      "\n",
      "\t Processing Batch #1390 ... Running Time 413.99861097335815\n",
      "\t Current Testing Loss: 0.40834726007570904\n",
      "\n",
      "\t Processing Batch #1400 ... Running Time 416.9762363433838\n",
      "\t Current Testing Loss: 0.4081036963584506\n",
      "\n",
      "\t Processing Batch #1410 ... Running Time 419.9540100097656\n",
      "\t Current Testing Loss: 0.4076110159015926\n",
      "\n",
      "\t Processing Batch #1420 ... Running Time 422.93176794052124\n",
      "\t Current Testing Loss: 0.40877748531912683\n",
      "\n",
      "\t Processing Batch #1430 ... Running Time 425.90768361091614\n",
      "\t Current Testing Loss: 0.4085316422215131\n",
      "\n",
      "\t Processing Batch #1440 ... Running Time 428.88353395462036\n",
      "\t Current Testing Loss: 0.4088951490100765\n",
      "\n",
      "\t Processing Batch #1450 ... Running Time 431.8601369857788\n",
      "\t Current Testing Loss: 0.40847117179102277\n",
      "\n",
      "\t Processing Batch #1460 ... Running Time 434.83709621429443\n",
      "\t Current Testing Loss: 0.4082143403625423\n",
      "\n",
      "\t Processing Batch #1470 ... Running Time 437.8143882751465\n",
      "\t Current Testing Loss: 0.4084308078749337\n",
      "\n",
      "\t Processing Batch #1480 ... Running Time 440.79110836982727\n",
      "\t Current Testing Loss: 0.4079506195412545\n",
      "\n",
      "\t Processing Batch #1490 ... Running Time 443.76916313171387\n",
      "\t Current Testing Loss: 0.40702102523694333\n",
      "\n",
      "\t Processing Batch #1500 ... Running Time 446.7462706565857\n",
      "\t Current Testing Loss: 0.4077428076800944\n",
      "\n",
      "\t Processing Batch #1510 ... Running Time 449.73277401924133\n",
      "\t Current Testing Loss: 0.4077994494762427\n",
      "\n",
      "\t Processing Batch #1520 ... Running Time 452.7097964286804\n",
      "\t Current Testing Loss: 0.40888087549427476\n",
      "\n",
      "\t Processing Batch #1530 ... Running Time 455.68596148490906\n",
      "\t Current Testing Loss: 0.4099776138576474\n",
      "\n",
      "\t Processing Batch #1540 ... Running Time 458.6616771221161\n",
      "\t Current Testing Loss: 0.4101129846138509\n",
      "\n",
      "\t Processing Batch #1550 ... Running Time 461.6377627849579\n",
      "\t Current Testing Loss: 0.409282759057753\n",
      "\n",
      "\t Processing Batch #1560 ... Running Time 464.61476945877075\n",
      "\t Current Testing Loss: 0.40946651106935367\n",
      "\n",
      "\t Processing Batch #1570 ... Running Time 467.5927035808563\n",
      "\t Current Testing Loss: 0.40976092443676376\n",
      "\n",
      "\t Processing Batch #1580 ... Running Time 470.56966495513916\n",
      "\t Current Testing Loss: 0.40952283579910354\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #1590 ... Running Time 473.5456886291504\n",
      "\t Current Testing Loss: 0.4099206651188082\n",
      "\n",
      "\t Processing Batch #1600 ... Running Time 476.5240669250488\n",
      "\t Current Testing Loss: 0.40971078866537775\n",
      "\n",
      "\t Processing Batch #1610 ... Running Time 479.50212812423706\n",
      "\t Current Testing Loss: 0.40923812427667267\n",
      "\n",
      "\t Processing Batch #1620 ... Running Time 482.4786624908447\n",
      "\t Current Testing Loss: 0.409102373422663\n",
      "\n",
      "\t Processing Batch #1630 ... Running Time 485.45518493652344\n",
      "\t Current Testing Loss: 0.40947409302617493\n",
      "\n",
      "\t Processing Batch #1640 ... Running Time 488.4325158596039\n",
      "\t Current Testing Loss: 0.41032068568947544\n",
      "\n",
      "\t Processing Batch #1650 ... Running Time 491.4100515842438\n",
      "\t Current Testing Loss: 0.4101388167650825\n",
      "\n",
      "\t Processing Batch #1660 ... Running Time 494.3877332210541\n",
      "\t Current Testing Loss: 0.4098546522064499\n",
      "\n",
      "\t Processing Batch #1670 ... Running Time 497.36544394493103\n",
      "\t Current Testing Loss: 0.4096591088860638\n",
      "\n",
      "\t Processing Batch #1680 ... Running Time 500.3426146507263\n",
      "\t Current Testing Loss: 0.40952935813527813\n",
      "\n",
      "\t Processing Batch #1690 ... Running Time 503.3208272457123\n",
      "\t Current Testing Loss: 0.40917221425923983\n",
      "\n",
      "\t Processing Batch #1700 ... Running Time 506.2977159023285\n",
      "\t Current Testing Loss: 0.40911701286122915\n",
      "\n",
      "\t Processing Batch #1710 ... Running Time 509.27537059783936\n",
      "\t Current Testing Loss: 0.40921930531132367\n",
      "\n",
      "\t Processing Batch #1720 ... Running Time 512.2529585361481\n",
      "\t Current Testing Loss: 0.40911926280215755\n",
      "\n",
      "\t Processing Batch #1730 ... Running Time 515.2312240600586\n",
      "\t Current Testing Loss: 0.4092807782817135\n",
      "\n",
      "\t Processing Batch #1740 ... Running Time 518.2088952064514\n",
      "\t Current Testing Loss: 0.40923960610051596\n",
      "\n",
      "\t Processing Batch #1750 ... Running Time 521.1867256164551\n",
      "\t Current Testing Loss: 0.4090870376048702\n",
      "\n",
      "\t Processing Batch #1760 ... Running Time 524.1649811267853\n",
      "\t Current Testing Loss: 0.40916994667819856\n",
      "\n",
      "\t Processing Batch #1770 ... Running Time 527.1426672935486\n",
      "\t Current Testing Loss: 0.40921151358897173\n",
      "\n",
      "\t Processing Batch #1780 ... Running Time 530.1207172870636\n",
      "\t Current Testing Loss: 0.4099474816548061\n",
      "\n",
      "\t Processing Batch #1790 ... Running Time 533.0979228019714\n",
      "\t Current Testing Loss: 0.4097021500190532\n",
      "\n",
      "\t Processing Batch #1800 ... Running Time 536.0748887062073\n",
      "\t Current Testing Loss: 0.4098792552621182\n",
      "\n",
      "\t Processing Batch #1810 ... Running Time 539.0511698722839\n",
      "\t Current Testing Loss: 0.41041628195436736\n",
      "\n",
      "\t Processing Batch #1820 ... Running Time 542.0274739265442\n",
      "\t Current Testing Loss: 0.41102237336824454\n",
      "\n",
      "\t Processing Batch #1830 ... Running Time 545.0047266483307\n",
      "\t Current Testing Loss: 0.41126702884786903\n",
      "\n",
      "\t Processing Batch #1840 ... Running Time 547.9817078113556\n",
      "\t Current Testing Loss: 0.4113668355785233\n",
      "\n",
      "\t Processing Batch #1850 ... Running Time 550.9583630561829\n",
      "\t Current Testing Loss: 0.41111616926546485\n",
      "\n",
      "\t Processing Batch #1860 ... Running Time 553.9361221790314\n",
      "\t Current Testing Loss: 0.4109461567455213\n",
      "\n",
      "\t Processing Batch #1870 ... Running Time 556.9136166572571\n",
      "\t Current Testing Loss: 0.4106430951416301\n",
      "\n",
      "\t Processing Batch #1880 ... Running Time 559.890025138855\n",
      "\t Current Testing Loss: 0.411193955768888\n",
      "\n",
      "\t Processing Batch #1890 ... Running Time 562.8673713207245\n",
      "\t Current Testing Loss: 0.41111632123479197\n",
      "\n",
      "\t Processing Batch #1900 ... Running Time 565.8519554138184\n",
      "\t Current Testing Loss: 0.4109283832520607\n",
      "\n",
      "\t Processing Batch #1910 ... Running Time 568.8341197967529\n",
      "\t Current Testing Loss: 0.41139068149423985\n",
      "\n",
      "\t Processing Batch #1920 ... Running Time 571.8134746551514\n",
      "\t Current Testing Loss: 0.41147270823699433\n",
      "\n",
      "\t Processing Batch #1930 ... Running Time 574.793595790863\n",
      "\t Current Testing Loss: 0.4116665978623233\n",
      "\n",
      "\t Processing Batch #1940 ... Running Time 577.7714605331421\n",
      "\t Current Testing Loss: 0.4114453539401984\n",
      "\n",
      "\t Processing Batch #1950 ... Running Time 580.749564409256\n",
      "\t Current Testing Loss: 0.41130956935804663\n",
      "\n",
      "\t Processing Batch #1960 ... Running Time 583.7277698516846\n",
      "\t Current Testing Loss: 0.4117764572402551\n",
      "\n",
      "\t Processing Batch #1970 ... Running Time 586.7067289352417\n",
      "\t Current Testing Loss: 0.41112379937464366\n",
      "\n",
      "\t Processing Batch #1980 ... Running Time 589.6855771541595\n",
      "\t Current Testing Loss: 0.41082921891028024\n",
      "\n",
      "\t Processing Batch #1990 ... Running Time 592.6644389629364\n",
      "\t Current Testing Loss: 0.410701914138003\n",
      "\n",
      "\t Processing Batch #2000 ... Running Time 595.6429841518402\n",
      "\t Current Testing Loss: 0.4105912950588041\n",
      "\n",
      "\t Processing Batch #2010 ... Running Time 598.6219336986542\n",
      "\t Current Testing Loss: 0.4102159871380315\n",
      "\n",
      "\t Processing Batch #2020 ... Running Time 601.600269317627\n",
      "\t Current Testing Loss: 0.41051382415332105\n",
      "\n",
      "\t Processing Batch #2030 ... Running Time 604.5783622264862\n",
      "\t Current Testing Loss: 0.4104767165791953\n",
      "\n",
      "\t Processing Batch #2040 ... Running Time 607.5566053390503\n",
      "\t Current Testing Loss: 0.40980328348068734\n",
      "\n",
      "\t Processing Batch #2050 ... Running Time 610.5344789028168\n",
      "\t Current Testing Loss: 0.4098746949949391\n",
      "\n",
      "\t Processing Batch #2060 ... Running Time 613.5110318660736\n",
      "\t Current Testing Loss: 0.40952699293627837\n",
      "\n",
      "\t Processing Batch #2070 ... Running Time 616.275946855545\n",
      "\t Current Testing Loss: 0.41017346838349766\n",
      "\n",
      "******* Final Testing Loss: 0.41017346838349766 *******\n",
      "\n",
      "Loading: melanoma_ResNeSt_3e_0b.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /u/home/a/andrewma/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #0 ... Running Time 0.4890868663787842\n",
      "\t Current Testing Loss: 0.12941870093345642\n",
      "\n",
      "\t Processing Batch #10 ... Running Time 3.4577858448028564\n",
      "\t Current Testing Loss: 0.3359718512405049\n",
      "\n",
      "\t Processing Batch #20 ... Running Time 6.423408269882202\n",
      "\t Current Testing Loss: 0.3102944330090568\n",
      "\n",
      "\t Processing Batch #30 ... Running Time 9.389315128326416\n",
      "\t Current Testing Loss: 0.34350237827147206\n",
      "\n",
      "\t Processing Batch #40 ... Running Time 12.354933023452759\n",
      "\t Current Testing Loss: 0.3607094694928425\n",
      "\n",
      "\t Processing Batch #50 ... Running Time 15.321300268173218\n",
      "\t Current Testing Loss: 0.36237480710534486\n",
      "\n",
      "\t Processing Batch #60 ... Running Time 18.28709888458252\n",
      "\t Current Testing Loss: 0.37071150565733674\n",
      "\n",
      "\t Processing Batch #70 ... Running Time 21.25580668449402\n",
      "\t Current Testing Loss: 0.3719522944638427\n",
      "\n",
      "\t Processing Batch #80 ... Running Time 24.224215030670166\n",
      "\t Current Testing Loss: 0.37504462483856416\n",
      "\n",
      "\t Processing Batch #90 ... Running Time 27.19292378425598\n",
      "\t Current Testing Loss: 0.3756572323349806\n",
      "\n",
      "\t Processing Batch #100 ... Running Time 30.161494970321655\n",
      "\t Current Testing Loss: 0.37677535835174053\n",
      "\n",
      "\t Processing Batch #110 ... Running Time 33.13033866882324\n",
      "\t Current Testing Loss: 0.37997924066610167\n",
      "\n",
      "\t Processing Batch #120 ... Running Time 36.09936881065369\n",
      "\t Current Testing Loss: 0.3781799065056911\n",
      "\n",
      "\t Processing Batch #130 ... Running Time 39.068100690841675\n",
      "\t Current Testing Loss: 0.3811124451174081\n",
      "\n",
      "\t Processing Batch #140 ... Running Time 42.03800320625305\n",
      "\t Current Testing Loss: 0.37779147695776416\n",
      "\n",
      "\t Processing Batch #150 ... Running Time 45.00674319267273\n",
      "\t Current Testing Loss: 0.376717248647813\n",
      "\n",
      "\t Processing Batch #160 ... Running Time 47.9759795665741\n",
      "\t Current Testing Loss: 0.3722855923986583\n",
      "\n",
      "\t Processing Batch #170 ... Running Time 50.94647479057312\n",
      "\t Current Testing Loss: 0.3702182012953256\n",
      "\n",
      "\t Processing Batch #180 ... Running Time 53.91643142700195\n",
      "\t Current Testing Loss: 0.3677540157628323\n",
      "\n",
      "\t Processing Batch #190 ... Running Time 56.88921523094177\n",
      "\t Current Testing Loss: 0.36712074470925704\n",
      "\n",
      "\t Processing Batch #200 ... Running Time 59.858561277389526\n",
      "\t Current Testing Loss: 0.36929076795109467\n",
      "\n",
      "\t Processing Batch #210 ... Running Time 62.83212184906006\n",
      "\t Current Testing Loss: 0.3693012380840089\n",
      "\n",
      "\t Processing Batch #220 ... Running Time 65.80586981773376\n",
      "\t Current Testing Loss: 0.36766534457109634\n",
      "\n",
      "\t Processing Batch #230 ... Running Time 68.77979040145874\n",
      "\t Current Testing Loss: 0.3667356398972598\n",
      "\n",
      "\t Processing Batch #240 ... Running Time 71.75229668617249\n",
      "\t Current Testing Loss: 0.36490908005425543\n",
      "\n",
      "\t Processing Batch #250 ... Running Time 74.72562336921692\n",
      "\t Current Testing Loss: 0.3620309412657502\n",
      "\n",
      "\t Processing Batch #260 ... Running Time 77.70030879974365\n",
      "\t Current Testing Loss: 0.357724003607971\n",
      "\n",
      "\t Processing Batch #270 ... Running Time 80.6751880645752\n",
      "\t Current Testing Loss: 0.3593810879491352\n",
      "\n",
      "\t Processing Batch #280 ... Running Time 83.64906287193298\n",
      "\t Current Testing Loss: 0.3586411314926962\n",
      "\n",
      "\t Processing Batch #290 ... Running Time 86.62300205230713\n",
      "\t Current Testing Loss: 0.3606742771528021\n",
      "\n",
      "\t Processing Batch #300 ... Running Time 89.59695959091187\n",
      "\t Current Testing Loss: 0.3592914576051243\n",
      "\n",
      "\t Processing Batch #310 ... Running Time 92.57045531272888\n",
      "\t Current Testing Loss: 0.36189802748022354\n",
      "\n",
      "\t Processing Batch #320 ... Running Time 95.54465985298157\n",
      "\t Current Testing Loss: 0.3624930005300082\n",
      "\n",
      "\t Processing Batch #330 ... Running Time 98.51859760284424\n",
      "\t Current Testing Loss: 0.3605179819545717\n",
      "\n",
      "\t Processing Batch #340 ... Running Time 101.49228811264038\n",
      "\t Current Testing Loss: 0.364243260969864\n",
      "\n",
      "\t Processing Batch #350 ... Running Time 104.46654391288757\n",
      "\t Current Testing Loss: 0.3660677685017599\n",
      "\n",
      "\t Processing Batch #360 ... Running Time 107.4413149356842\n",
      "\t Current Testing Loss: 0.3641302048169345\n",
      "\n",
      "\t Processing Batch #370 ... Running Time 110.41480827331543\n",
      "\t Current Testing Loss: 0.3660056393824498\n",
      "\n",
      "\t Processing Batch #380 ... Running Time 113.39039301872253\n",
      "\t Current Testing Loss: 0.3667131965476384\n",
      "\n",
      "\t Processing Batch #390 ... Running Time 116.36748099327087\n",
      "\t Current Testing Loss: 0.36482218384285414\n",
      "\n",
      "\t Processing Batch #400 ... Running Time 119.34567022323608\n",
      "\t Current Testing Loss: 0.36620197762872214\n",
      "\n",
      "\t Processing Batch #410 ... Running Time 122.3197512626648\n",
      "\t Current Testing Loss: 0.36417529084821687\n",
      "\n",
      "\t Processing Batch #420 ... Running Time 125.29502749443054\n",
      "\t Current Testing Loss: 0.3650245970544226\n",
      "\n",
      "\t Processing Batch #430 ... Running Time 128.27024698257446\n",
      "\t Current Testing Loss: 0.3661637157893236\n",
      "\n",
      "\t Processing Batch #440 ... Running Time 131.24687314033508\n",
      "\t Current Testing Loss: 0.36782710500855564\n",
      "\n",
      "\t Processing Batch #450 ... Running Time 134.22109055519104\n",
      "\t Current Testing Loss: 0.36707487751930096\n",
      "\n",
      "\t Processing Batch #460 ... Running Time 137.19570565223694\n",
      "\t Current Testing Loss: 0.3665382508794553\n",
      "\n",
      "\t Processing Batch #470 ... Running Time 140.1695008277893\n",
      "\t Current Testing Loss: 0.3668839378468297\n",
      "\n",
      "\t Processing Batch #480 ... Running Time 143.14440369606018\n",
      "\t Current Testing Loss: 0.3674753909609174\n",
      "\n",
      "\t Processing Batch #490 ... Running Time 146.1182005405426\n",
      "\t Current Testing Loss: 0.36896176108153433\n",
      "\n",
      "\t Processing Batch #500 ... Running Time 149.09530186653137\n",
      "\t Current Testing Loss: 0.3668619376754095\n",
      "\n",
      "\t Processing Batch #510 ... Running Time 152.06951570510864\n",
      "\t Current Testing Loss: 0.367886391258753\n",
      "\n",
      "\t Processing Batch #520 ... Running Time 155.04431462287903\n",
      "\t Current Testing Loss: 0.36823564902262584\n",
      "\n",
      "\t Processing Batch #530 ... Running Time 158.01927161216736\n",
      "\t Current Testing Loss: 0.36803598544804167\n",
      "\n",
      "\t Processing Batch #540 ... Running Time 160.99438309669495\n",
      "\t Current Testing Loss: 0.3687809828028441\n",
      "\n",
      "\t Processing Batch #550 ... Running Time 163.96919894218445\n",
      "\t Current Testing Loss: 0.36997699999874173\n",
      "\n",
      "\t Processing Batch #560 ... Running Time 166.94464755058289\n",
      "\t Current Testing Loss: 0.37227428193185846\n",
      "\n",
      "\t Processing Batch #570 ... Running Time 169.91937637329102\n",
      "\t Current Testing Loss: 0.37237049122512234\n",
      "\n",
      "\t Processing Batch #580 ... Running Time 172.89377188682556\n",
      "\t Current Testing Loss: 0.37338691585650746\n",
      "\n",
      "\t Processing Batch #590 ... Running Time 175.86782693862915\n",
      "\t Current Testing Loss: 0.374675509540361\n",
      "\n",
      "\t Processing Batch #600 ... Running Time 178.8430848121643\n",
      "\t Current Testing Loss: 0.37538799322434946\n",
      "\n",
      "\t Processing Batch #610 ... Running Time 181.81914734840393\n",
      "\t Current Testing Loss: 0.37477433402243693\n",
      "\n",
      "\t Processing Batch #620 ... Running Time 184.79444074630737\n",
      "\t Current Testing Loss: 0.37482828350554726\n",
      "\n",
      "\t Processing Batch #630 ... Running Time 187.76971292495728\n",
      "\t Current Testing Loss: 0.37561731342751703\n",
      "\n",
      "\t Processing Batch #640 ... Running Time 190.74584555625916\n",
      "\t Current Testing Loss: 0.3752344123984276\n",
      "\n",
      "\t Processing Batch #650 ... Running Time 193.72046899795532\n",
      "\t Current Testing Loss: 0.37336523046920195\n",
      "\n",
      "\t Processing Batch #660 ... Running Time 196.69489073753357\n",
      "\t Current Testing Loss: 0.3717230379513821\n",
      "\n",
      "\t Processing Batch #670 ... Running Time 199.67125988006592\n",
      "\t Current Testing Loss: 0.372484653533038\n",
      "\n",
      "\t Processing Batch #680 ... Running Time 202.64731764793396\n",
      "\t Current Testing Loss: 0.3728012105945098\n",
      "\n",
      "\t Processing Batch #690 ... Running Time 205.6227569580078\n",
      "\t Current Testing Loss: 0.37203909487998604\n",
      "\n",
      "\t Processing Batch #700 ... Running Time 208.59855842590332\n",
      "\t Current Testing Loss: 0.3734419242440379\n",
      "\n",
      "\t Processing Batch #710 ... Running Time 211.57287311553955\n",
      "\t Current Testing Loss: 0.3728527713111181\n",
      "\n",
      "\t Processing Batch #720 ... Running Time 214.54618787765503\n",
      "\t Current Testing Loss: 0.37359252564826056\n",
      "\n",
      "\t Processing Batch #730 ... Running Time 217.52018237113953\n",
      "\t Current Testing Loss: 0.37280779713115025\n",
      "\n",
      "\t Processing Batch #740 ... Running Time 220.4939742088318\n",
      "\t Current Testing Loss: 0.37298773720800155\n",
      "\n",
      "\t Processing Batch #750 ... Running Time 223.46809792518616\n",
      "\t Current Testing Loss: 0.3720013053117992\n",
      "\n",
      "\t Processing Batch #760 ... Running Time 226.4438009262085\n",
      "\t Current Testing Loss: 0.37345176291959203\n",
      "\n",
      "\t Processing Batch #770 ... Running Time 229.4185392856598\n",
      "\t Current Testing Loss: 0.37262822831612463\n",
      "\n",
      "\t Processing Batch #780 ... Running Time 232.39378762245178\n",
      "\t Current Testing Loss: 0.3721310719847679\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #790 ... Running Time 235.36872005462646\n",
      "\t Current Testing Loss: 0.3721851298748015\n",
      "\n",
      "\t Processing Batch #800 ... Running Time 238.34473252296448\n",
      "\t Current Testing Loss: 0.37189306412706064\n",
      "\n",
      "\t Processing Batch #810 ... Running Time 241.32036876678467\n",
      "\t Current Testing Loss: 0.3715662021010783\n",
      "\n",
      "\t Processing Batch #820 ... Running Time 244.29718685150146\n",
      "\t Current Testing Loss: 0.37209144777342695\n",
      "\n",
      "\t Processing Batch #830 ... Running Time 247.27277278900146\n",
      "\t Current Testing Loss: 0.37176858142430746\n",
      "\n",
      "\t Processing Batch #840 ... Running Time 250.24899530410767\n",
      "\t Current Testing Loss: 0.3710754815927584\n",
      "\n",
      "\t Processing Batch #850 ... Running Time 253.22467827796936\n",
      "\t Current Testing Loss: 0.3700283327250447\n",
      "\n",
      "\t Processing Batch #860 ... Running Time 256.200701713562\n",
      "\t Current Testing Loss: 0.3712774192120417\n",
      "\n",
      "\t Processing Batch #870 ... Running Time 259.17695021629333\n",
      "\t Current Testing Loss: 0.3719559700135249\n",
      "\n",
      "\t Processing Batch #880 ... Running Time 262.1537070274353\n",
      "\t Current Testing Loss: 0.37173153586034474\n",
      "\n",
      "\t Processing Batch #890 ... Running Time 265.12719440460205\n",
      "\t Current Testing Loss: 0.3723762505233087\n",
      "\n",
      "\t Processing Batch #900 ... Running Time 268.1021234989166\n",
      "\t Current Testing Loss: 0.37186639469848753\n",
      "\n",
      "\t Processing Batch #910 ... Running Time 271.0784785747528\n",
      "\t Current Testing Loss: 0.3715973284205935\n",
      "\n",
      "\t Processing Batch #920 ... Running Time 274.0529959201813\n",
      "\t Current Testing Loss: 0.37206850461378677\n",
      "\n",
      "\t Processing Batch #930 ... Running Time 277.0285701751709\n",
      "\t Current Testing Loss: 0.37137573859009143\n",
      "\n",
      "\t Processing Batch #940 ... Running Time 280.0035104751587\n",
      "\t Current Testing Loss: 0.3711635405464963\n",
      "\n",
      "\t Processing Batch #950 ... Running Time 282.97998094558716\n",
      "\t Current Testing Loss: 0.3720075999054623\n",
      "\n",
      "\t Processing Batch #960 ... Running Time 285.9556522369385\n",
      "\t Current Testing Loss: 0.3720675277117636\n",
      "\n",
      "\t Processing Batch #970 ... Running Time 288.9316611289978\n",
      "\t Current Testing Loss: 0.37119855150104675\n",
      "\n",
      "\t Processing Batch #980 ... Running Time 291.9069285392761\n",
      "\t Current Testing Loss: 0.37131172385176875\n",
      "\n",
      "\t Processing Batch #990 ... Running Time 294.8832702636719\n",
      "\t Current Testing Loss: 0.37201023651014425\n",
      "\n",
      "\t Processing Batch #1000 ... Running Time 297.85991191864014\n",
      "\t Current Testing Loss: 0.37147653539414766\n",
      "\n",
      "\t Processing Batch #1010 ... Running Time 300.8365342617035\n",
      "\t Current Testing Loss: 0.3715400192794472\n",
      "\n",
      "\t Processing Batch #1020 ... Running Time 303.8125493526459\n",
      "\t Current Testing Loss: 0.3722380708125908\n",
      "\n",
      "\t Processing Batch #1030 ... Running Time 306.7885751724243\n",
      "\t Current Testing Loss: 0.3728690841041007\n",
      "\n",
      "\t Processing Batch #1040 ... Running Time 309.7624509334564\n",
      "\t Current Testing Loss: 0.37286593394638484\n",
      "\n",
      "\t Processing Batch #1050 ... Running Time 312.736935377121\n",
      "\t Current Testing Loss: 0.37184433669177946\n",
      "\n",
      "\t Processing Batch #1060 ... Running Time 315.7121067047119\n",
      "\t Current Testing Loss: 0.3729640659222223\n",
      "\n",
      "\t Processing Batch #1070 ... Running Time 318.6866281032562\n",
      "\t Current Testing Loss: 0.3724254301108288\n",
      "\n",
      "\t Processing Batch #1080 ... Running Time 321.6608865261078\n",
      "\t Current Testing Loss: 0.37241590831852645\n",
      "\n",
      "\t Processing Batch #1090 ... Running Time 324.6360993385315\n",
      "\t Current Testing Loss: 0.3726248617781673\n",
      "\n",
      "\t Processing Batch #1100 ... Running Time 327.6113109588623\n",
      "\t Current Testing Loss: 0.37179938162745507\n",
      "\n",
      "\t Processing Batch #1110 ... Running Time 330.58617329597473\n",
      "\t Current Testing Loss: 0.3722065180023708\n",
      "\n",
      "\t Processing Batch #1120 ... Running Time 333.5616843700409\n",
      "\t Current Testing Loss: 0.3728359131961662\n",
      "\n",
      "\t Processing Batch #1130 ... Running Time 336.5363748073578\n",
      "\t Current Testing Loss: 0.37195622214469415\n",
      "\n",
      "\t Processing Batch #1140 ... Running Time 339.51125049591064\n",
      "\t Current Testing Loss: 0.37196498849705895\n",
      "\n",
      "\t Processing Batch #1150 ... Running Time 342.4851930141449\n",
      "\t Current Testing Loss: 0.37222495312914966\n",
      "\n",
      "\t Processing Batch #1160 ... Running Time 345.46008801460266\n",
      "\t Current Testing Loss: 0.37192403930371737\n",
      "\n",
      "\t Processing Batch #1170 ... Running Time 348.4346125125885\n",
      "\t Current Testing Loss: 0.3727049413403571\n",
      "\n",
      "\t Processing Batch #1180 ... Running Time 351.4092364311218\n",
      "\t Current Testing Loss: 0.3724613680798406\n",
      "\n",
      "\t Processing Batch #1190 ... Running Time 354.38464617729187\n",
      "\t Current Testing Loss: 0.37297545179824787\n",
      "\n",
      "\t Processing Batch #1200 ... Running Time 357.3597893714905\n",
      "\t Current Testing Loss: 0.37364474633440287\n",
      "\n",
      "\t Processing Batch #1210 ... Running Time 360.33377289772034\n",
      "\t Current Testing Loss: 0.3731301731647636\n",
      "\n",
      "\t Processing Batch #1220 ... Running Time 363.30704712867737\n",
      "\t Current Testing Loss: 0.37297299532044337\n",
      "\n",
      "\t Processing Batch #1230 ... Running Time 366.27970266342163\n",
      "\t Current Testing Loss: 0.3731989501956563\n",
      "\n",
      "\t Processing Batch #1240 ... Running Time 369.2539494037628\n",
      "\t Current Testing Loss: 0.3737026991519257\n",
      "\n",
      "\t Processing Batch #1250 ... Running Time 372.22783279418945\n",
      "\t Current Testing Loss: 0.37448643449006036\n",
      "\n",
      "\t Processing Batch #1260 ... Running Time 375.20291352272034\n",
      "\t Current Testing Loss: 0.37460761690109046\n",
      "\n",
      "\t Processing Batch #1270 ... Running Time 378.1780445575714\n",
      "\t Current Testing Loss: 0.37433189349168405\n",
      "\n",
      "\t Processing Batch #1280 ... Running Time 381.1530520915985\n",
      "\t Current Testing Loss: 0.37545048449243723\n",
      "\n",
      "\t Processing Batch #1290 ... Running Time 384.1300301551819\n",
      "\t Current Testing Loss: 0.37529727247255795\n",
      "\n",
      "\t Processing Batch #1300 ... Running Time 387.1068060398102\n",
      "\t Current Testing Loss: 0.37482747184449117\n",
      "\n",
      "\t Processing Batch #1310 ... Running Time 390.0834810733795\n",
      "\t Current Testing Loss: 0.37387803826881766\n",
      "\n",
      "\t Processing Batch #1320 ... Running Time 393.059623003006\n",
      "\t Current Testing Loss: 0.37410800477361517\n",
      "\n",
      "\t Processing Batch #1330 ... Running Time 396.03567814826965\n",
      "\t Current Testing Loss: 0.37384324445536854\n",
      "\n",
      "\t Processing Batch #1340 ... Running Time 399.0123405456543\n",
      "\t Current Testing Loss: 0.3731071967312109\n",
      "\n",
      "\t Processing Batch #1350 ... Running Time 401.98884177207947\n",
      "\t Current Testing Loss: 0.37310409718789406\n",
      "\n",
      "\t Processing Batch #1360 ... Running Time 404.96659207344055\n",
      "\t Current Testing Loss: 0.37310241265420874\n",
      "\n",
      "\t Processing Batch #1370 ... Running Time 407.94389247894287\n",
      "\t Current Testing Loss: 0.37280537807304526\n",
      "\n",
      "\t Processing Batch #1380 ... Running Time 410.92099952697754\n",
      "\t Current Testing Loss: 0.3717958199927161\n",
      "\n",
      "\t Processing Batch #1390 ... Running Time 413.8980715274811\n",
      "\t Current Testing Loss: 0.3724908105879058\n",
      "\n",
      "\t Processing Batch #1400 ... Running Time 416.8759255409241\n",
      "\t Current Testing Loss: 0.3721264505374653\n",
      "\n",
      "\t Processing Batch #1410 ... Running Time 419.8536641597748\n",
      "\t Current Testing Loss: 0.3715716583969786\n",
      "\n",
      "\t Processing Batch #1420 ... Running Time 422.8311538696289\n",
      "\t Current Testing Loss: 0.3723063073412664\n",
      "\n",
      "\t Processing Batch #1430 ... Running Time 425.8078701496124\n",
      "\t Current Testing Loss: 0.37175076897174725\n",
      "\n",
      "\t Processing Batch #1440 ... Running Time 428.7855598926544\n",
      "\t Current Testing Loss: 0.37187729832004707\n",
      "\n",
      "\t Processing Batch #1450 ... Running Time 431.76299929618835\n",
      "\t Current Testing Loss: 0.3713710962118854\n",
      "\n",
      "\t Processing Batch #1460 ... Running Time 434.7395234107971\n",
      "\t Current Testing Loss: 0.37118953404454724\n",
      "\n",
      "\t Processing Batch #1470 ... Running Time 437.71584582328796\n",
      "\t Current Testing Loss: 0.37149718205522386\n",
      "\n",
      "\t Processing Batch #1480 ... Running Time 440.6917943954468\n",
      "\t Current Testing Loss: 0.3711716864914567\n",
      "\n",
      "\t Processing Batch #1490 ... Running Time 443.66776609420776\n",
      "\t Current Testing Loss: 0.3703581418424825\n",
      "\n",
      "\t Processing Batch #1500 ... Running Time 446.6443774700165\n",
      "\t Current Testing Loss: 0.3710577441956463\n",
      "\n",
      "\t Processing Batch #1510 ... Running Time 449.6215670108795\n",
      "\t Current Testing Loss: 0.371384152712351\n",
      "\n",
      "\t Processing Batch #1520 ... Running Time 452.5981230735779\n",
      "\t Current Testing Loss: 0.3723564570271248\n",
      "\n",
      "\t Processing Batch #1530 ... Running Time 455.57448267936707\n",
      "\t Current Testing Loss: 0.37320378604500143\n",
      "\n",
      "\t Processing Batch #1540 ... Running Time 458.55087208747864\n",
      "\t Current Testing Loss: 0.3729453991148512\n",
      "\n",
      "\t Processing Batch #1550 ... Running Time 461.5284492969513\n",
      "\t Current Testing Loss: 0.372036282967495\n",
      "\n",
      "\t Processing Batch #1560 ... Running Time 464.5063796043396\n",
      "\t Current Testing Loss: 0.3721482825276372\n",
      "\n",
      "\t Processing Batch #1570 ... Running Time 467.4826912879944\n",
      "\t Current Testing Loss: 0.3721848360027461\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Processing Batch #1580 ... Running Time 470.4597051143646\n",
      "\t Current Testing Loss: 0.3717664872366254\n",
      "\n",
      "\t Processing Batch #1590 ... Running Time 473.4365544319153\n",
      "\t Current Testing Loss: 0.371897703927598\n",
      "\n",
      "\t Processing Batch #1600 ... Running Time 476.4143190383911\n",
      "\t Current Testing Loss: 0.37165594653114165\n",
      "\n",
      "\t Processing Batch #1610 ... Running Time 479.39086723327637\n",
      "\t Current Testing Loss: 0.3713118133824816\n",
      "\n",
      "\t Processing Batch #1620 ... Running Time 482.368221282959\n",
      "\t Current Testing Loss: 0.37117500983541724\n",
      "\n",
      "\t Processing Batch #1630 ... Running Time 485.34506821632385\n",
      "\t Current Testing Loss: 0.3714984688795943\n",
      "\n",
      "\t Processing Batch #1640 ... Running Time 488.3211257457733\n",
      "\t Current Testing Loss: 0.3723030255030271\n",
      "\n",
      "\t Processing Batch #1650 ... Running Time 491.2980341911316\n",
      "\t Current Testing Loss: 0.3723121054271977\n",
      "\n",
      "\t Processing Batch #1660 ... Running Time 494.2744743824005\n",
      "\t Current Testing Loss: 0.3721381277810336\n",
      "\n",
      "\t Processing Batch #1670 ... Running Time 497.2513892650604\n",
      "\t Current Testing Loss: 0.37192869511667587\n",
      "\n",
      "\t Processing Batch #1680 ... Running Time 500.22909474372864\n",
      "\t Current Testing Loss: 0.3716900845316994\n",
      "\n",
      "\t Processing Batch #1690 ... Running Time 503.2063615322113\n",
      "\t Current Testing Loss: 0.3715805351262717\n",
      "\n",
      "\t Processing Batch #1700 ... Running Time 506.18356466293335\n",
      "\t Current Testing Loss: 0.371410381281064\n",
      "\n",
      "\t Processing Batch #1710 ... Running Time 509.15985012054443\n",
      "\t Current Testing Loss: 0.3714663986506377\n",
      "\n",
      "\t Processing Batch #1720 ... Running Time 512.1366677284241\n",
      "\t Current Testing Loss: 0.3714698877192077\n",
      "\n",
      "\t Processing Batch #1730 ... Running Time 515.1124708652496\n",
      "\t Current Testing Loss: 0.3715599893443265\n",
      "\n",
      "\t Processing Batch #1740 ... Running Time 518.0885231494904\n",
      "\t Current Testing Loss: 0.3714529810508629\n",
      "\n",
      "\t Processing Batch #1750 ... Running Time 521.0650582313538\n",
      "\t Current Testing Loss: 0.3711660630864834\n",
      "\n",
      "\t Processing Batch #1760 ... Running Time 524.0418226718903\n",
      "\t Current Testing Loss: 0.37146639156069083\n",
      "\n",
      "\t Processing Batch #1770 ... Running Time 527.0186932086945\n",
      "\t Current Testing Loss: 0.37187630739767696\n",
      "\n",
      "\t Processing Batch #1780 ... Running Time 529.9963493347168\n",
      "\t Current Testing Loss: 0.3722013544556902\n",
      "\n",
      "\t Processing Batch #1790 ... Running Time 532.9734098911285\n",
      "\t Current Testing Loss: 0.3718143444443834\n",
      "\n",
      "\t Processing Batch #1800 ... Running Time 535.950190782547\n",
      "\t Current Testing Loss: 0.3720011103676962\n",
      "\n",
      "\t Processing Batch #1810 ... Running Time 538.9280157089233\n",
      "\t Current Testing Loss: 0.37213543293795026\n",
      "\n",
      "\t Processing Batch #1820 ... Running Time 541.9037461280823\n",
      "\t Current Testing Loss: 0.3727016820173909\n",
      "\n",
      "\t Processing Batch #1830 ... Running Time 544.8807306289673\n",
      "\t Current Testing Loss: 0.3729272407323431\n",
      "\n",
      "\t Processing Batch #1840 ... Running Time 547.8578670024872\n",
      "\t Current Testing Loss: 0.3727924634784962\n",
      "\n",
      "\t Processing Batch #1850 ... Running Time 550.8349852561951\n",
      "\t Current Testing Loss: 0.3728323037550457\n",
      "\n",
      "\t Processing Batch #1860 ... Running Time 553.8119151592255\n",
      "\t Current Testing Loss: 0.3726103577196342\n",
      "\n",
      "\t Processing Batch #1870 ... Running Time 556.7888724803925\n",
      "\t Current Testing Loss: 0.372659702558829\n",
      "\n",
      "\t Processing Batch #1880 ... Running Time 559.7659537792206\n",
      "\t Current Testing Loss: 0.37358722532171\n",
      "\n",
      "\t Processing Batch #1890 ... Running Time 562.7422490119934\n",
      "\t Current Testing Loss: 0.373622636548635\n",
      "\n",
      "\t Processing Batch #1900 ... Running Time 565.7172091007233\n",
      "\t Current Testing Loss: 0.3737465891338442\n",
      "\n",
      "\t Processing Batch #1910 ... Running Time 568.6931331157684\n",
      "\t Current Testing Loss: 0.3741779140426282\n",
      "\n",
      "\t Processing Batch #1920 ... Running Time 571.6700148582458\n",
      "\t Current Testing Loss: 0.3743196232753424\n",
      "\n",
      "\t Processing Batch #1930 ... Running Time 574.647623538971\n",
      "\t Current Testing Loss: 0.37456748140508\n",
      "\n",
      "\t Processing Batch #1940 ... Running Time 577.6247544288635\n",
      "\t Current Testing Loss: 0.37423118130370614\n",
      "\n",
      "\t Processing Batch #1950 ... Running Time 580.6024391651154\n",
      "\t Current Testing Loss: 0.3741754891531306\n",
      "\n",
      "\t Processing Batch #1960 ... Running Time 583.5788819789886\n",
      "\t Current Testing Loss: 0.3744783111273756\n",
      "\n",
      "\t Processing Batch #1970 ... Running Time 586.5554807186127\n",
      "\t Current Testing Loss: 0.3741557357000222\n",
      "\n",
      "\t Processing Batch #1980 ... Running Time 589.5314910411835\n",
      "\t Current Testing Loss: 0.37389716331304956\n",
      "\n",
      "\t Processing Batch #1990 ... Running Time 592.508825302124\n",
      "\t Current Testing Loss: 0.37386013851662425\n",
      "\n",
      "\t Processing Batch #2000 ... Running Time 595.4865217208862\n",
      "\t Current Testing Loss: 0.373562035492916\n",
      "\n",
      "\t Processing Batch #2010 ... Running Time 598.4631149768829\n",
      "\t Current Testing Loss: 0.3736025588239591\n",
      "\n",
      "\t Processing Batch #2020 ... Running Time 601.4409070014954\n",
      "\t Current Testing Loss: 0.37374162572753544\n",
      "\n",
      "\t Processing Batch #2030 ... Running Time 604.4179971218109\n",
      "\t Current Testing Loss: 0.3737368876401542\n",
      "\n",
      "\t Processing Batch #2040 ... Running Time 607.3948538303375\n",
      "\t Current Testing Loss: 0.3732809736021502\n",
      "\n",
      "\t Processing Batch #2050 ... Running Time 610.3725168704987\n",
      "\t Current Testing Loss: 0.37324613371436216\n",
      "\n",
      "\t Processing Batch #2060 ... Running Time 613.3497953414917\n",
      "\t Current Testing Loss: 0.373014529728661\n",
      "\n",
      "\t Processing Batch #2070 ... Running Time 616.1145076751709\n",
      "\t Current Testing Loss: 0.37369769713236534\n",
      "\n",
      "******* Final Testing Loss: 0.37369769713236534 *******\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_fnames = os.listdir('./saved_weights4/')\n",
    "#weight_fnames.sort() # isnt perfectly sorted, but too lazy to add the code (not important)\n",
    "batch_sizes = []\n",
    "losses = []\n",
    "\n",
    "for fname in weight_fnames:\n",
    "    \n",
    "    print(f'Loading: {fname}\\n')\n",
    "\n",
    "    checkpoint = torch.load(f'./saved_weights4/{fname}', map_location=device)\n",
    "    \n",
    "    # network weights load\n",
    "    net = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True).to(device)\n",
    "    \n",
    "    \n",
    "    # for feature extraction\n",
    "    #for param in net.parameters():\n",
    "        #param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Sequential(\n",
    "               nn.Linear(num_ftrs, 1024),\n",
    "               nn.BatchNorm1d(1024),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.4),\n",
    "               nn.Linear(1024, 256),\n",
    "               nn.BatchNorm1d(256),\n",
    "               nn.ReLU(),\n",
    "               nn.Dropout(p=0.3),\n",
    "               nn.Linear(256, 1),\n",
    "               nn.Sigmoid()).to(device)\n",
    "\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])  \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # set start time for cnn training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ground_truths = []\n",
    "    probs = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i, testdata in enumerate(test_loader, 0):\n",
    "            \n",
    "            image, label = testdata\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network \n",
    "            outputs = net(image)\n",
    "            \n",
    "            loss = criterion(outputs, label.unsqueeze(-1).float())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # everything saved should be on RAM\n",
    "            outputs = outputs.to(\"cpu\")\n",
    "            label = label.to(\"cpu\")\n",
    "            \n",
    "            # save for analysis\n",
    "            ground_truths.append(label)\n",
    "            \n",
    "            # # save for analysis\n",
    "            probs += outputs.squeeze(-1).tolist()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"\\t Processing Batch #{i} ... Running Time {time.time() - start_time}\")\n",
    "                print(f'\\t Current Testing Loss: {running_loss / (i+1)}\\n')\n",
    "\n",
    "                \n",
    "    print(f'******* Final Testing Loss: {running_loss / (i+1)} *******\\n')\n",
    "\n",
    "    batch_sizes.append(checkpoint['mini_batch'])\n",
    "    losses.append(running_loss / (i+1))\n",
    "                \n",
    "    # Save ground-truths and probability results\n",
    "    res = {}\n",
    "    res[\"ground_truths\"] = ground_truths\n",
    "    res[\"probs\"] = probs\n",
    "    res[\"num_batches\"] = checkpoint['mini_batch']\n",
    "    res[\"testing_loss\"] = running_loss / (i+1)\n",
    "\n",
    "    pkl_f_name = f'./saved_results4/results_ResNeSt_{checkpoint[\"mini_batch\"]}b.pkl'\n",
    "    with open(pkl_f_name, 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "\n",
    "        \n",
    "plt.plot(batch_sizes, losses, 'o')\n",
    "plt.title(\"Testing Loss Over Time\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Overall Testing Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f86c7e",
   "metadata": {},
   "source": [
    "## Choose the results from the best performing model (training size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d8d6ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 12204] Size Area Under the ROC Curve: 0.8258807826056412 \n",
      "\n",
      "[Batch 24408] Size Area Under the ROC Curve: 0.8511823219575512 \n",
      "\n",
      "[Batch 34719] Size Area Under the ROC Curve: 0.8660548064812351 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "# in case pkl results were calculated in batch job\n",
    "# we may wont to visualize the test plot over time\n",
    "recalc_loss_plot = True\n",
    "\n",
    "if recalc_loss_plot:\n",
    "    \n",
    "    batch_sizes = []\n",
    "    losses = []\n",
    "    res_fnames = os.listdir('./saved_results4/')\n",
    "    \n",
    "    for fname in res_fnames:\n",
    "        with open(f'./saved_results4/{fname}', 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            batch_sizes.append(res[\"num_batches\"])\n",
    "            losses.append(res[\"testing_loss\"]) \n",
    "            \n",
    "            \n",
    "            gt = res[\"ground_truths\"]\n",
    "            probs = np.array(res[\"probs\"])\n",
    "\n",
    "            # match formats (shouldve done this before, forgot to check)\n",
    "            ground_truths = []\n",
    "            for i in range(len(gt)):\n",
    "                if gt[i].size() > torch.Size([1]):\n",
    "                    ground_truths += gt[i].squeeze(-1).tolist()\n",
    "                else:\n",
    "                    ground_truths.append(gt[i].squeeze(-1).tolist())\n",
    "\n",
    "            ground_truths = np.array(ground_truths)\n",
    "            print(f\"[Batch {res['num_batches']}] Size Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs), \"\\n\")\n",
    "            \n",
    "    \n",
    "    plt.plot(batch_sizes, losses, 'o')\n",
    "    plt.title(\"Testing Loss Over Time\")\n",
    "    plt.xlabel(\"Batch Number\")\n",
    "    plt.ylabel(\"Overall Testing Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_batch_size = 21350\n",
    "\n",
    "\n",
    "with open(f'./saved_results4/results_ResNeSt_{best_batch_size}b.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)\n",
    "    \n",
    "    \n",
    "gt = res[\"ground_truths\"]\n",
    "probs = np.array(res[\"probs\"])\n",
    "\n",
    "\n",
    "# match formats (shouldve done this before, forgot to check)\n",
    "ground_truths = []\n",
    "for i in range(len(gt)):\n",
    "    if gt[i].size() > torch.Size([1]):\n",
    "        ground_truths += gt[i].squeeze(-1).tolist()\n",
    "    else:\n",
    "        ground_truths.append(gt[i].squeeze(-1).tolist())\n",
    "        \n",
    "ground_truths = np.array(ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc478afa",
   "metadata": {},
   "source": [
    "## Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8fc4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(ground_truths, probs)\n",
    "recall = tpr\n",
    "\n",
    "# compute other metrics using the same thresholds\n",
    "specificity = np.zeros_like(tpr)\n",
    "precision = np.zeros_like(tpr)\n",
    "fbetascores = np.zeros_like(tpr)\n",
    "CKappas = np.zeros_like(tpr)\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    preds = probs > thresholds[i]\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(ground_truths, preds).ravel()\n",
    "    \n",
    "    specificity[i] = tn / (tn + fp)\n",
    "    precision[i] = tp / (tp + fp)\n",
    "    \n",
    "    # more attention put on recall, such as when false negatives are more important to\n",
    "    # minimize, but false positives are still important.\n",
    "    fbetascores[i] = metrics.fbeta_score(ground_truths, preds, beta = 2)\n",
    "    \n",
    "    CKappas[i] = metrics.cohen_kappa_score(ground_truths, preds,)\n",
    "    \n",
    "\n",
    "\n",
    "gmeans = np.sqrt(specificity * recall)\n",
    "\n",
    "\n",
    "print(\"Max F2-Score is:\", np.nanmax(fbetascores))\n",
    "print(\"Max G-Mean is:\", np.nanmax(gmeans))\n",
    "print(\"Max Cohen's Kappa is:\", np.nanmax(CKappas))\n",
    "\n",
    "\n",
    "print(\"Area Under the ROC Curve:\", metrics.roc_auc_score(ground_truths, probs), \"\\n\")\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr[np.nanargmax(fbetascores)], tpr[np.nanargmax(fbetascores)], 'ro')\n",
    "plt.plot(fpr[np.nanargmax(gmeans)], tpr[np.nanargmax(gmeans)], 'go')\n",
    "plt.plot(fpr[np.nanargmax(CKappas)], tpr[np.nanargmax(CKappas)], 'yo')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(['ROC Curve', 'F2-Score Optimal Coordinates', 'G-Mean Optimal Coordinates', \n",
    "            \"Kappa's Optimal Coordinates\"], loc='lower right', prop={'size': 8}, \n",
    "           frameon=True, facecolor = 'white')\n",
    "plt.show()\n",
    "\n",
    "fb_opt_thresh = thresholds[np.nanargmax(fbetascores)]\n",
    "fb_opt_preds = probs > fb_opt_thresh\n",
    "\n",
    "print('\\n********************* USING F2-SCORE OPTIMAL THRESHOLD *************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, fb_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(fbetascores)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(fbetascores)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(fbetascores)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(fbetascores)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(fbetascores)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(fbetascores)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "gm_opt_thresh = thresholds[np.nanargmax(gmeans)]\n",
    "gm_opt_preds = probs > gm_opt_thresh\n",
    "\n",
    "print('\\n********************** USING G-MEAN OPTIMAL THRESHOLD **************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, gm_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(gmeans)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(gmeans)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(gmeans)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(gmeans)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(gmeans)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(gmeans)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "\n",
    "ck_opt_thresh = thresholds[np.nanargmax(CKappas)]\n",
    "ck_opt_preds = probs > ck_opt_thresh\n",
    "\n",
    "print('\\n********************** USING KAPPA OPTIMAL THRESHOLD ***************************')\n",
    "print(\"The confusion matrix is:\\n\", metrics.confusion_matrix(ground_truths, ck_opt_preds), \"\\n\")\n",
    "print(\"Recall / Sensitivity:\",  recall[np.nanargmax(CKappas)] )\n",
    "print(\"Precision:\",  precision[np.nanargmax(CKappas)] )\n",
    "print(\"Specificity:\",  specificity[np.nanargmax(CKappas)] )\n",
    "print(\"F2-Score:\", fbetascores[np.nanargmax(CKappas)] )\n",
    "print(\"G-Mean:\", gmeans[np.nanargmax(CKappas)] )\n",
    "print(\"Cohen's Kappa:\", CKappas[np.nanargmax(CKappas)] )\n",
    "print('********************************************************************************\\n')\n",
    "\n",
    "\n",
    "accuracy_scores = []\n",
    "for thresh in thresholds:\n",
    "    accuracy_scores.append(metrics.accuracy_score(ground_truths, [m > thresh for m in probs]))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(thresholds, fbetascores, \"-r\")\n",
    "plt.plot(thresholds, gmeans, \"-g\")\n",
    "plt.plot(thresholds, CKappas, \"-y\")\n",
    "plt.title(\"F2-Score, G-Means, and Cohen's Kappa Curves\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Performance Metrics\")\n",
    "plt.legend(['F2-Score', 'G-Mean', \"Cohen's Kappa\"], loc='upper right',\n",
    "           frameon=True, facecolor = 'white')\n",
    "plt.show()    \n",
    "    \n",
    "\n",
    "plt.plot(thresholds, accuracy_scores)\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAS Pytorch CUDA",
   "language": "python",
   "name": "mas_pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
